{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7), CpuDevice(id=8), CpuDevice(id=9), CpuDevice(id=10), CpuDevice(id=11)]\n",
      "JAX default backend: cpu\n"
     ]
    }
   ],
   "source": [
    "# ==== SECTION 0: Imports & Paths ====\n",
    "import os, re, glob, csv, json, time, random, gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "# Configure matplotlib for Jupyter\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random as jrandom, jit, vmap, grad\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"JAX default backend:\", jax.default_backend())\n",
    "\n",
    "# Paths\n",
    "BRA_TS_DIR = \"../data/BraTS-2023\"\n",
    "MU_DIR     = \"../data/MU-Glioma-Post\"\n",
    "CKPT_DIR   = \"../checkpoints\"\n",
    "ART_DIR    = \"../artifacts\"\n",
    "PRED_MU    = \"../pred_mu\"\n",
    "DISTILL_DIR= \"../distill\"\n",
    "\n",
    "for d in [CKPT_DIR, ART_DIR, PRED_MU, DISTILL_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 1251 rows -> ../data/BraTS-2023/manifest.csv\n"
     ]
    }
   ],
   "source": [
    "# ==== SECTION 1: Build BraTS manifest ====\n",
    "root = BRA_TS_DIR\n",
    "cases = sorted([p for p in glob.glob(os.path.join(root, \"*\")) if os.path.isdir(p)])\n",
    "\n",
    "def pick(files, pattern):\n",
    "    rx = re.compile(pattern, re.I)\n",
    "    for f in files:\n",
    "        if rx.search(os.path.basename(f)):\n",
    "            return f\n",
    "    return \"\"\n",
    "\n",
    "rows = []\n",
    "for case in cases:\n",
    "    files = glob.glob(os.path.join(case, \"*.nii*\"))\n",
    "    rows.append({\n",
    "        \"id\":   os.path.basename(case),\n",
    "        \"t1\":   pick(files, r\"t1n|t1\\.nii\"),\n",
    "        \"t1ce\": pick(files, r\"t1c|t1ce\"),\n",
    "        \"t2\":   pick(files, r\"t2(?!f)|t2w\"),\n",
    "        \"flair\":pick(files, r\"t2f|flair\"),\n",
    "        \"mask\": pick(files, r\"seg|mask\")\n",
    "    })\n",
    "\n",
    "man_bra = os.path.join(BRA_TS_DIR, \"manifest.csv\")\n",
    "with open(man_bra, \"w\", newline=\"\") as fp:\n",
    "    w = csv.DictWriter(fp, fieldnames=[\"id\",\"t1\",\"t1ce\",\"t2\",\"flair\",\"mask\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(f\"Wrote {len(rows)} rows -> {man_bra}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root used: ../data/MU-Glioma-Post/MU-Glioma-Post\n",
      "Wrote 596 rows -> ../data/MU-Glioma-Post/manifest.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import csv\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "def pick(files: List[str], pattern: str, flags=re.IGNORECASE) -> str:\n",
    "    \"\"\"Return first file matching pattern (on basename). Returns empty string if none.\"\"\"\n",
    "    rx = re.compile(pattern, flags)\n",
    "    for f in files:\n",
    "        name = os.path.basename(f)\n",
    "        if rx.search(name):\n",
    "            return os.path.relpath(f, root)\n",
    "    return \"\"\n",
    "\n",
    "# Auto-detect correct root if dataset is nested (e.g., MU-Glioma-Post/MU-Glioma-Post/...)\n",
    "root = MU_DIR\n",
    "patients = sorted([p for p in glob.glob(os.path.join(root, \"PatientID_*\")) if os.path.isdir(p)])\n",
    "if not patients:\n",
    "    # look one level deeper for a folder that looks like the dataset (name starts with MU- or similar)\n",
    "    candidates = [d for d in glob.glob(os.path.join(MU_DIR, \"*\")) if os.path.isdir(d) and os.path.basename(d).lower().startswith(\"mu\")]\n",
    "    if candidates:\n",
    "        root = candidates[0]\n",
    "        patients = sorted([p for p in glob.glob(os.path.join(root, \"PatientID_*\")) if os.path.isdir(p)])\n",
    "\n",
    "rows = []\n",
    "for pat_dir in patients:\n",
    "    tps = sorted([tp for tp in glob.glob(os.path.join(pat_dir, \"Timepoint_*\")) if os.path.isdir(tp)])\n",
    "    for tp in tps:\n",
    "        files = glob.glob(os.path.join(tp, \"**\", \"*.nii*\"), recursive=True)\n",
    "        row = {\n",
    "            \"id\":   os.path.relpath(tp, root).replace(os.sep, \"_\"),\n",
    "            \"t1\":   pick(files, r\"(^|[^a-z0-9])t1(_n|n?1)?(\\.|_|$)\"),\n",
    "            \"t1ce\": pick(files, r\"t1c|t1ce\"),\n",
    "            \"t2\":   pick(files, r\"(^|[^a-z0-9])t2(?!f)(\\.|_|$)|t2w\"),\n",
    "            \"flair\":pick(files, r\"t2f|flair\"),\n",
    "            \"mask\": pick(files, r\"seg|mask|tumorMask|_seg\")\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "man_mu = os.path.join(MU_DIR, \"manifest.csv\")\n",
    "with open(man_mu, \"w\", newline=\"\") as fp:\n",
    "    w = csv.DictWriter(fp, fieldnames=[\"id\",\"t1\",\"t1ce\",\"t2\",\"flair\",\"mask\"])\n",
    "    w.writeheader(); w.writerows(rows)\n",
    "\n",
    "print(f\"Root used: {root}\")\n",
    "print(f\"Wrote {len(rows)} rows -> {man_mu}\")\n",
    "if len(rows) == 0:\n",
    "    print(\"Warning: no timepoints found. Check that patient and Timepoint_* directories contain .nii/.nii.gz files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 410 | Val: 102\n"
     ]
    }
   ],
   "source": [
    "# ==== SECTION 3: Train/Val split for BraTS ====\n",
    "MAX_CASES = 512\n",
    "\n",
    "rng = random.Random(42)\n",
    "with open(man_bra) as fp:\n",
    "    all_rows = [r for r in csv.DictReader(fp) if all(r.get(k) for k in [\"t1\",\"t1ce\",\"t2\",\"flair\",\"mask\"])]\n",
    "\n",
    "rng.shuffle(all_rows)\n",
    "all_rows = all_rows[:MAX_CASES]\n",
    "\n",
    "n_total = len(all_rows)\n",
    "n_val = int(0.2 * n_total)\n",
    "val_rows = all_rows[:n_val]\n",
    "train_rows = all_rows[n_val:]\n",
    "\n",
    "def write_rows(rows, path):\n",
    "    with open(path, \"w\", newline=\"\") as fp:\n",
    "        w = csv.DictWriter(fp, fieldnames=[\"id\",\"t1\",\"t1ce\",\"t2\",\"flair\",\"mask\"])\n",
    "        w.writeheader(); w.writerows(rows)\n",
    "\n",
    "train_csv = os.path.join(BRA_TS_DIR, \"train.csv\")\n",
    "val_csv   = os.path.join(BRA_TS_DIR, \"val.csv\")\n",
    "write_rows(train_rows, train_csv)\n",
    "write_rows(val_rows, val_csv)\n",
    "\n",
    "print(f\"Train: {len(train_rows)} | Val: {len(val_rows)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 4: Data Loading ====\n",
    "def load_modalities(row):\n",
    "    vols = [nib.load(row[k]).get_fdata().astype(np.float32) for k in [\"t1\",\"t1ce\",\"t2\",\"flair\"]]\n",
    "    vols = [(v - v.mean()) / (v.std() + 1e-6) for v in vols]\n",
    "    vol = np.stack(vols, axis=0)  # [C, X, Y, Z]\n",
    "    mask = nib.load(row[\"mask\"]).get_fdata().astype(np.int64)\n",
    "    mask = (mask > 0).astype(np.int64)\n",
    "    return vol, mask\n",
    "\n",
    "def create_slice_dataset(manifest_path, max_slices_per_case=8, background_drop=0.7, seed=0):\n",
    "    rng = random.Random(seed)\n",
    "    with open(manifest_path) as fp:\n",
    "        rows = list(csv.DictReader(fp))\n",
    "    \n",
    "    items = []\n",
    "    for row in rows:\n",
    "        vol, msk = load_modalities(row)\n",
    "        z = vol.shape[-1]\n",
    "        idxs = np.linspace(0, z-1, num=min(max_slices_per_case, z), dtype=int)\n",
    "        for iz in idxs:\n",
    "            x = vol[..., iz]\n",
    "            y = msk[..., iz]\n",
    "            if (y.sum() == 0) and (rng.random() < background_drop):\n",
    "                continue\n",
    "            items.append((x, y))\n",
    "    rng.shuffle(items)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array([x for x, _ in items], dtype=np.float32)\n",
    "    Y = np.array([y for _, y in items], dtype=np.int64)\n",
    "    X = np.clip(X, -5, 5)\n",
    "    return X, Y\n",
    "\n",
    "train_X, train_Y = create_slice_dataset(train_csv, seed=0)\n",
    "val_X, val_Y = create_slice_dataset(val_csv, seed=1)\n",
    "\n",
    "print(f\"Train: {train_X.shape} | Val: {val_X.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 5: UNet2D in Flax ====\n",
    "class DoubleConv(nn.Module):\n",
    "    out_ch: int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        x = nn.Conv(self.out_ch, (3, 3), padding='SAME')(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(self.out_ch, (3, 3), padding='SAME')(x)\n",
    "        x = nn.BatchNorm(use_running_average=not train)(x)\n",
    "        x = nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    n_classes: int = 2\n",
    "    base: int = 16\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, train: bool = True):\n",
    "        # Encoder\n",
    "        c1 = DoubleConv(self.base)(x, train)\n",
    "        p1 = nn.max_pool(c1, (2, 2), strides=(2, 2))\n",
    "        \n",
    "        c2 = DoubleConv(self.base * 2)(p1, train)\n",
    "        p2 = nn.max_pool(c2, (2, 2), strides=(2, 2))\n",
    "        \n",
    "        c3 = DoubleConv(self.base * 4)(p2, train)\n",
    "        \n",
    "        # Decoder\n",
    "        u2 = jax.image.resize(c3, shape=(c3.shape[0], c2.shape[1], c2.shape[2], c3.shape[3]), \n",
    "                              method='nearest')\n",
    "        u2 = nn.Conv(self.base * 2, (2, 2))(u2)\n",
    "        u2 = jnp.concatenate([u2, c2], axis=-1)\n",
    "        c2_up = DoubleConv(self.base * 2)(u2, train)\n",
    "        \n",
    "        u1 = jax.image.resize(c2_up, shape=(c2_up.shape[0], c1.shape[1], c1.shape[2], c2_up.shape[3]),\n",
    "                              method='nearest')\n",
    "        u1 = nn.Conv(self.base, (2, 2))(u1)\n",
    "        u1 = jnp.concatenate([u1, c1], axis=-1)\n",
    "        c1_up = DoubleConv(self.base)(u1, train)\n",
    "        \n",
    "        out = nn.Conv(self.n_classes, (1, 1))(c1_up)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 6: Training Setup ====\n",
    "def create_train_state(rng, learning_rate=1e-3):\n",
    "    model = UNet2D()\n",
    "    # NHWC format for JAX\n",
    "    dummy_input = jnp.ones((1, 240, 240, 4))\n",
    "    variables = model.init(rng, dummy_input, train=True)\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return train_state.TrainState.create(\n",
    "        apply_fn=model.apply,\n",
    "        params=variables['params'],\n",
    "        tx=tx\n",
    "    ), variables.get('batch_stats', {})\n",
    "\n",
    "@jit\n",
    "def compute_dice(logits, target):\n",
    "    pred = jnp.argmax(logits, axis=-1)\n",
    "    pred_bin = (pred == 1).astype(jnp.float32)\n",
    "    tgt_bin = (target == 1).astype(jnp.float32)\n",
    "    inter = jnp.sum(pred_bin * tgt_bin, axis=(1, 2))\n",
    "    union = jnp.sum(pred_bin, axis=(1, 2)) + jnp.sum(tgt_bin, axis=(1, 2))\n",
    "    return jnp.mean((2 * inter + 1e-6) / (union + 1e-6))\n",
    "\n",
    "@jit\n",
    "def train_step(state, batch_stats, batch_x, batch_y, rng):\n",
    "    def loss_fn(params):\n",
    "        variables = {'params': params, 'batch_stats': batch_stats}\n",
    "        logits, updates = state.apply_fn(\n",
    "            variables, batch_x, train=True, \n",
    "            mutable=['batch_stats']\n",
    "        )\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits, batch_y\n",
    "        ).mean()\n",
    "        return loss, (logits, updates)\n",
    "    \n",
    "    (loss, (logits, updates)), grads = jax.value_and_grad(loss_fn, has_aux=True)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    dice = compute_dice(logits, batch_y)\n",
    "    return state, updates['batch_stats'], loss, dice\n",
    "\n",
    "@jit\n",
    "def eval_step(state, batch_stats, batch_x, batch_y):\n",
    "    variables = {'params': state.params, 'batch_stats': batch_stats}\n",
    "    logits = state.apply_fn(variables, batch_x, train=False)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch_y).mean()\n",
    "    dice = compute_dice(logits, batch_y)\n",
    "    return loss, dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 7: Training Loop ====\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, init_rng = jrandom.split(rng)\n",
    "\n",
    "state, batch_stats = create_train_state(init_rng)\n",
    "\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 4\n",
    "n_train = len(train_X)\n",
    "n_val = len(val_X)\n",
    "\n",
    "history = {\"tr_loss\": [], \"tr_dice\": [], \"va_loss\": [], \"va_dice\": []}\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    # Training\n",
    "    t0 = time.time()\n",
    "    perm = np.random.permutation(n_train)\n",
    "    train_X_shuffled = train_X[perm]\n",
    "    train_Y_shuffled = train_Y[perm]\n",
    "    \n",
    "    tr_loss = tr_dice = n_batches = 0.0\n",
    "    for i in range(0, n_train, BATCH_SIZE):\n",
    "        batch_x = jnp.array(train_X_shuffled[i:i+BATCH_SIZE])\n",
    "        batch_y = jnp.array(train_Y_shuffled[i:i+BATCH_SIZE])\n",
    "        \n",
    "        # Convert from NCHW to NHWC\n",
    "        batch_x = jnp.transpose(batch_x, (0, 2, 3, 1))\n",
    "        \n",
    "        rng, step_rng = jrandom.split(rng)\n",
    "        state, batch_stats, loss, dice = train_step(state, batch_stats, batch_x, batch_y, step_rng)\n",
    "        \n",
    "        tr_loss += loss\n",
    "        tr_dice += dice\n",
    "        n_batches += 1\n",
    "    \n",
    "    tr_loss /= n_batches\n",
    "    tr_dice /= n_batches\n",
    "    \n",
    "    # Validation\n",
    "    va_loss = va_dice = n_val_batches = 0.0\n",
    "    for i in range(0, n_val, BATCH_SIZE):\n",
    "        batch_x = jnp.array(val_X[i:i+BATCH_SIZE])\n",
    "        batch_y = jnp.array(val_Y[i:i+BATCH_SIZE])\n",
    "        \n",
    "        batch_x = jnp.transpose(batch_x, (0, 2, 3, 1))\n",
    "        \n",
    "        loss, dice = eval_step(state, batch_stats, batch_x, batch_y)\n",
    "        va_loss += loss\n",
    "        va_dice += dice\n",
    "        n_val_batches += 1\n",
    "    \n",
    "    va_loss /= n_val_batches\n",
    "    va_dice /= n_val_batches\n",
    "    \n",
    "    history[\"tr_loss\"].append(float(tr_loss))\n",
    "    history[\"tr_dice\"].append(float(tr_dice))\n",
    "    history[\"va_loss\"].append(float(va_loss))\n",
    "    history[\"va_dice\"].append(float(va_dice))\n",
    "    \n",
    "    # Save checkpoint\n",
    "    ck = os.path.join(CKPT_DIR, f\"unet2d_jax_ep{ep:02d}.npz\")\n",
    "    np.savez(ck, params=state.params, batch_stats=batch_stats)\n",
    "    \n",
    "    print(f\"Epoch {ep:02d} | train loss {tr_loss:.4f} dice {tr_dice:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} dice {va_dice:.3f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Plot UNet training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(range(1, EPOCHS+1), history[\"tr_loss\"], 'o-', label=\"Train Loss\", linewidth=2, markersize=8)\n",
    "ax1.plot(range(1, EPOCHS+1), history[\"va_loss\"], 's-', label=\"Val Loss\", linewidth=2, markersize=8)\n",
    "ax1.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax1.set_ylabel(\"Cross-Entropy Loss\", fontsize=12)\n",
    "ax1.set_title(\"UNet Training & Validation Loss\", fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(range(1, EPOCHS+1))\n",
    "\n",
    "# Dice plot\n",
    "ax2.plot(range(1, EPOCHS+1), history[\"tr_dice\"], 'o-', label=\"Train Dice\", linewidth=2, markersize=8, color='green')\n",
    "ax2.plot(range(1, EPOCHS+1), history[\"va_dice\"], 's-', label=\"Val Dice\", linewidth=2, markersize=8, color='orange')\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax2.set_ylabel(\"Dice Score\", fontsize=12)\n",
    "ax2.set_title(\"UNet Training & Validation Dice\", fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(range(1, EPOCHS+1))\n",
    "ax2.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ART_DIR, \"unet_training_curves_jax.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"UNet Training Summary\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Best Val Loss:  {min(history['va_loss']):.4f} (Epoch {history['va_loss'].index(min(history['va_loss']))+1})\")\n",
    "print(f\"Best Val Dice:  {max(history['va_dice']):.4f} (Epoch {history['va_dice'].index(max(history['va_dice']))+1})\")\n",
    "print(f\"Final Val Loss: {history['va_loss'][-1]:.4f}\")\n",
    "print(f\"Final Val Dice: {history['va_dice'][-1]:.4f}\")\n",
    "print(\"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 8: Qualitative Validation ====\n",
    "batch_x = jnp.array(val_X[:4])\n",
    "batch_y = val_Y[:4]\n",
    "batch_x_hwc = jnp.transpose(batch_x, (0, 2, 3, 1))\n",
    "\n",
    "variables = {'params': state.params, 'batch_stats': batch_stats}\n",
    "logits = state.apply_fn(variables, batch_x_hwc, train=False)\n",
    "pred = jnp.argmax(logits, axis=-1)\n",
    "\n",
    "k = min(4, len(batch_x))\n",
    "fig, axes = plt.subplots(k, 3, figsize=(9, 3*k))\n",
    "for i in range(k):\n",
    "    s = np.array(batch_x[i, 3])  # FLAIR channel\n",
    "    axes[i,0].imshow(s, cmap=\"gray\")\n",
    "    axes[i,0].set_title(\"FLAIR\")\n",
    "    axes[i,1].imshow(batch_y[i], cmap=\"magma\")\n",
    "    axes[i,1].set_title(\"GT\")\n",
    "    axes[i,2].imshow(np.array(pred[i]), cmap=\"magma\")\n",
    "    axes[i,2].set_title(\"Pred\")\n",
    "    for j in range(3):\n",
    "        axes[i,j].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ART_DIR, \"val_quicklook_jax.png\"), dpi=120)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 9: Inference on MU ====\n",
    "def load_case_3d(row):\n",
    "    vols = [nib.load(row[k]).get_fdata().astype(np.float32) for k in [\"t1\",\"t1ce\",\"t2\",\"flair\"]]\n",
    "    vols = [(v - v.mean())/(v.std()+1e-6) for v in vols]\n",
    "    vol = np.stack(vols, axis=0)  # [C,X,Y,Z]\n",
    "    aff = nib.load(row[\"t1\"]).affine\n",
    "    gt = None\n",
    "    if row.get(\"mask\"):\n",
    "        try:\n",
    "            gt = nib.load(row[\"mask\"]).get_fdata().astype(np.int64)\n",
    "            gt = (gt>0).astype(np.int64)\n",
    "        except:\n",
    "            gt = None\n",
    "    return vol, aff, gt\n",
    "\n",
    "# Load best checkpoint\n",
    "last_ck = sorted(glob.glob(os.path.join(CKPT_DIR, \"unet2d_jax_ep*.npz\")))[-1]\n",
    "ckpt = np.load(last_ck, allow_pickle=True)\n",
    "state_params = ckpt['params'].item()\n",
    "state_batch_stats = ckpt['batch_stats'].item()\n",
    "\n",
    "# Read MU manifest\n",
    "with open(man_mu) as fp:\n",
    "    mu_rows = [r for r in csv.DictReader(fp) if all(r.get(k) for k in [\"t1\",\"t1ce\",\"t2\",\"flair\"])]\n",
    "\n",
    "@jit\n",
    "def predict_slice(params, batch_stats, x):\n",
    "    variables = {'params': params, 'batch_stats': batch_stats}\n",
    "    model = UNet2D()\n",
    "    logits = model.apply(variables, x, train=False)\n",
    "    return jnp.argmax(logits, axis=-1)\n",
    "\n",
    "metrics = []\n",
    "for row in mu_rows:\n",
    "    vol, aff, gt = load_case_3d(row)\n",
    "    X, Y, Z = vol.shape[1:]\n",
    "    pred_vol = np.zeros((X, Y, Z), dtype=np.uint8)\n",
    "    \n",
    "    for z in range(Z):\n",
    "        x = jnp.array(vol[..., z].copy())[None, ...]  # [1,C,H,W]\n",
    "        x = jnp.transpose(x, (0, 2, 3, 1))  # [1,H,W,C]\n",
    "        pred = predict_slice(state_params, state_batch_stats, x)\n",
    "        pred_vol[..., z] = np.array(pred[0]).astype(np.uint8)\n",
    "    \n",
    "    out_path = os.path.join(PRED_MU, f\"{row['id']}_pred_jax.nii.gz\")\n",
    "    nib.save(nib.Nifti1Image(pred_vol, aff), out_path)\n",
    "    print(\"Wrote:\", out_path)\n",
    "    \n",
    "    if gt is not None and gt.shape == pred_vol.shape:\n",
    "        inter = ((pred_vol==1)&(gt==1)).sum()\n",
    "        union = (pred_vol==1).sum() + (gt==1).sum()\n",
    "        dice = (2*inter + 1e-6)/(union + 1e-6)\n",
    "        metrics.append({\"id\": row[\"id\"], \"dice\": float(dice)})\n",
    "\n",
    "if metrics:\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(metrics)\n",
    "    csv_path = os.path.join(ART_DIR, \"mu_metrics_jax.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"Saved:\", csv_path, \" | mean dice:\", df[\"dice\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 10: Distillation Sampling ====\n",
    "os.makedirs(DISTILL_DIR, exist_ok=True)\n",
    "\n",
    "with open(train_csv) as fp:\n",
    "    tr_rows = list(csv.DictReader(fp))\n",
    "\n",
    "target_samples_per_case = 100_000\n",
    "shard_size = 1_000_000\n",
    "\n",
    "acc_x = []; acc_y = []; acc_z = []; acc_c = []\n",
    "shard_id = 0\n",
    "\n",
    "def flush_shard():\n",
    "    global acc_x, acc_y, acc_z, acc_c, shard_id\n",
    "    if not acc_x: return\n",
    "    path = os.path.join(DISTILL_DIR, f\"brats_samples_jax_{shard_id:03d}.npz\")\n",
    "    np.savez_compressed(path,\n",
    "        x=np.array(acc_x, dtype=np.float32),\n",
    "        y=np.array(acc_y, dtype=np.float32),\n",
    "        z=np.array(acc_z, dtype=np.float32),\n",
    "        c=np.array(acc_c, dtype=np.int64))\n",
    "    print(\"Saved shard:\", path, \" | n=\", len(acc_x))\n",
    "    acc_x.clear(); acc_y.clear(); acc_z.clear(); acc_c.clear()\n",
    "    shard_id += 1\n",
    "\n",
    "for i, row in enumerate(tr_rows):\n",
    "    vol, aff, _ = load_case_3d(row)\n",
    "    X, Y, Z = vol.shape[1:]\n",
    "    pred_vol = np.zeros((X, Y, Z), dtype=np.uint8)\n",
    "    \n",
    "    for z in range(Z):\n",
    "        x = jnp.array(vol[..., z].copy())[None, ...]\n",
    "        x = jnp.transpose(x, (0, 2, 3, 1))\n",
    "        pred = predict_slice(state_params, state_batch_stats, x)\n",
    "        pred_vol[..., z] = np.array(pred[0]).astype(np.uint8)\n",
    "    \n",
    "    # Stratified sampling\n",
    "    pos_idx = np.argwhere(pred_vol==1)\n",
    "    neg_idx = np.argwhere(pred_vol==0)\n",
    "    n_pos = min(len(pos_idx), target_samples_per_case//2)\n",
    "    n_neg = min(len(neg_idx), target_samples_per_case - n_pos)\n",
    "    \n",
    "    if n_pos > 0:\n",
    "        sel = pos_idx[np.random.choice(len(pos_idx), n_pos, replace=False)]\n",
    "        acc_x.extend((sel[:,0]/(X-1)).tolist())\n",
    "        acc_y.extend((sel[:,1]/(Y-1)).tolist())\n",
    "        acc_z.extend((sel[:,2]/(Z-1)).tolist())\n",
    "        acc_c.extend([1]*n_pos)\n",
    "    \n",
    "    if n_neg > 0:\n",
    "        sel = neg_idx[np.random.choice(len(neg_idx), n_neg, replace=False)]\n",
    "        acc_x.extend((sel[:,0]/(X-1)).tolist())\n",
    "        acc_y.extend((sel[:,1]/(Y-1)).tolist())\n",
    "        acc_z.extend((sel[:,2]/(Z-1)).tolist())\n",
    "        acc_c.extend([0]*n_neg)\n",
    "    \n",
    "    if len(acc_x) >= shard_size:\n",
    "        flush_shard()\n",
    "    \n",
    "    del vol, pred_vol; gc.collect()\n",
    "\n",
    "flush_shard()\n",
    "print(\"Distillation sampling done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 11: Train Tiny MLP on Distillation ====\n",
    "class TinyMLP(nn.Module):\n",
    "    hidden: tuple = (64, 64, 32)\n",
    "    out_dim: int = 2\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for h in self.hidden:\n",
    "            x = nn.Dense(h)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.out_dim)(x)\n",
    "        return x\n",
    "\n",
    "# Load NPZ files\n",
    "npz_files = sorted(glob.glob(os.path.join(DISTILL_DIR, \"brats_samples_jax_*.npz\")))\n",
    "assert npz_files, \"No distillation NPZ files found!\"\n",
    "\n",
    "# Simple 95/5 split\n",
    "n = len(npz_files)\n",
    "split = max(1, int(0.95*n))\n",
    "train_files, dev_files = npz_files[:split], npz_files[split:]\n",
    "\n",
    "# Load all data into memory (for simplicity with JAX)\n",
    "def load_npz_files(files):\n",
    "    X, Y = [], []\n",
    "    for f in files:\n",
    "        with np.load(f) as npz:\n",
    "            x = np.stack([npz[\"x\"], npz[\"y\"], npz[\"z\"]], axis=1).astype(np.float32)\n",
    "            c = npz[\"c\"].astype(np.int64)\n",
    "            X.append(x)\n",
    "            Y.append(c)\n",
    "    return np.concatenate(X), np.concatenate(Y)\n",
    "\n",
    "mlp_train_X, mlp_train_Y = load_npz_files(train_files)\n",
    "mlp_dev_X, mlp_dev_Y = load_npz_files(dev_files)\n",
    "\n",
    "print(f\"MLP Train: {mlp_train_X.shape} | Dev: {mlp_dev_X.shape}\")\n",
    "\n",
    "# Initialize MLP\n",
    "rng = jrandom.PRNGKey(42)\n",
    "rng, init_rng = jrandom.split(rng)\n",
    "\n",
    "mlp = TinyMLP()\n",
    "mlp_params = mlp.init(init_rng, jnp.ones((1, 3)))['params']\n",
    "mlp_tx = optax.adam(3e-3)\n",
    "mlp_state = train_state.TrainState.create(\n",
    "    apply_fn=mlp.apply,\n",
    "    params=mlp_params,\n",
    "    tx=mlp_tx\n",
    ")\n",
    "\n",
    "@jit\n",
    "def mlp_train_step(state, batch_x, batch_y):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch_x)\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch_y).mean()\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "@jit\n",
    "def mlp_eval(params, batch_x, batch_y):\n",
    "    logits = mlp.apply({'params': params}, batch_x)\n",
    "    pred = jnp.argmax(logits, axis=-1)\n",
    "    acc = jnp.mean(pred == batch_y)\n",
    "    return acc\n",
    "\n",
    "BATCH_SIZE = 8192\n",
    "mlp_hist = []\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Training\n",
    "    perm = np.random.permutation(len(mlp_train_X))\n",
    "    train_X_shuffled = mlp_train_X[perm]\n",
    "    train_Y_shuffled = mlp_train_Y[perm]\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    n_batches = 0\n",
    "    for i in range(0, len(train_X_shuffled), BATCH_SIZE):\n",
    "        batch_x = jnp.array(train_X_shuffled[i:i+BATCH_SIZE])\n",
    "        batch_y = jnp.array(train_Y_shuffled[i:i+BATCH_SIZE])\n",
    "        mlp_state, loss = mlp_train_step(mlp_state, batch_x, batch_y)\n",
    "        running_loss += loss\n",
    "        n_batches += 1\n",
    "    \n",
    "    # Evaluation\n",
    "    dev_acc = 0.0\n",
    "    n_dev_batches = 0\n",
    "    for i in range(0, len(mlp_dev_X), BATCH_SIZE):\n",
    "        batch_x = jnp.array(mlp_dev_X[i:i+BATCH_SIZE])\n",
    "        batch_y = jnp.array(mlp_dev_Y[i:i+BATCH_SIZE])\n",
    "        acc = mlp_eval(mlp_state.params, batch_x, batch_y)\n",
    "        dev_acc += acc\n",
    "        n_dev_batches += 1\n",
    "    \n",
    "    avg_loss = running_loss / n_batches\n",
    "    avg_acc = dev_acc / n_dev_batches\n",
    "    \n",
    "    print(f\"[MLP] Epoch {ep:02d} | loss {avg_loss:.4f} | dev acc {avg_acc:.3f} | {time.time()-t0:.1f}s\")\n",
    "    mlp_hist.append((float(avg_loss), float(avg_acc)))\n",
    "\n",
    "# Save MLP weights\n",
    "mlp_ckpt = os.path.join(CKPT_DIR, \"mlp_distill_jax.npz\")\n",
    "np.savez(mlp_ckpt, params=mlp_state.params)\n",
    "\n",
    "# Plot MLP training curves\n",
    "plt.figure()\n",
    "plt.plot([h[0] for h in mlp_hist])\n",
    "plt.title(\"MLP Loss\")\n",
    "plt.savefig(os.path.join(ART_DIR, \"mlp_loss_jax.png\"), dpi=120)\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot([h[1] for h in mlp_hist])\n",
    "plt.title(\"MLP Dev Accuracy\")\n",
    "plt.savefig(os.path.join(ART_DIR, \"mlp_acc_jax.png\"), dpi=120)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 12: Export MLP weights (NPZ and JSON) ====\n",
    "\n",
    "# Export as NPZ (efficient binary format)\n",
    "def export_mlp_npz(params, output_path):\n",
    "    \"\"\"Export MLP parameters to NPZ format\"\"\"\n",
    "    export_dict = {}\n",
    "    layer_idx = 0\n",
    "    \n",
    "    for key in sorted(params.keys()):\n",
    "        if 'Dense' in key:\n",
    "            kernel = np.array(params[key]['kernel'])  # [in, out]\n",
    "            bias = np.array(params[key]['bias'])\n",
    "            export_dict[f'layer_{layer_idx}_weight'] = kernel.T  # [out, in]\n",
    "            export_dict[f'layer_{layer_idx}_bias'] = bias\n",
    "            layer_idx += 1\n",
    "    \n",
    "    # Add metadata as simple numeric arrays\n",
    "    export_dict['n_layers'] = np.array([layer_idx], dtype=np.int32)\n",
    "    export_dict['input_dim'] = np.array([3], dtype=np.int32)\n",
    "    export_dict['output_dim'] = np.array([2], dtype=np.int32)\n",
    "    # For activation, we'll just document it's relu (or store as int code)\n",
    "    # 0=relu, 1=sigmoid, 2=tanh, etc.\n",
    "    export_dict['activation_code'] = np.array([0], dtype=np.int32)  # 0 = relu\n",
    "    \n",
    "    np.savez_compressed(output_path, **export_dict)\n",
    "    return layer_idx\n",
    "\n",
    "out_npz = os.path.join(ART_DIR, \"mlp_distill_jax.npz\")\n",
    "n_layers = export_mlp_npz(mlp_state.params, out_npz)\n",
    "print(f\"Exported MLP to NPZ: {out_npz} ({n_layers} layers)\")\n",
    "\n",
    "# Also export as JSON for compatibility (optional)\n",
    "def export_dense_layer(params, layer_name):\n",
    "    \"\"\"Extract weights from a Flax Dense layer\"\"\"\n",
    "    kernel = np.array(params[layer_name]['kernel'])  # [in, out]\n",
    "    bias = np.array(params[layer_name]['bias'])\n",
    "    return {\"W\": kernel.T.tolist(), \"b\": bias.tolist()}\n",
    "\n",
    "layers = []\n",
    "for key in sorted(mlp_state.params.keys()):\n",
    "    if 'Dense' in key:\n",
    "        layers.append(export_dense_layer(mlp_state.params, key))\n",
    "\n",
    "data = {\n",
    "    \"layers\": layers,\n",
    "    \"activation\": \"relu\",\n",
    "    \"input_dim\": 3,\n",
    "    \"output_dim\": 2\n",
    "}\n",
    "\n",
    "out_json = os.path.join(ART_DIR, \"mlp_distill_jax.json\")\n",
    "with open(out_json, \"w\") as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "print(f\"Exported MLP to JSON: {out_json}\")\n",
    "\n",
    "# Verify NPZ export\n",
    "with np.load(out_npz) as npz_data:\n",
    "    print(\"\\nNPZ Contents:\")\n",
    "    print(f\"  n_layers: {npz_data['n_layers'][0]}\")\n",
    "    print(f\"  input_dim: {npz_data['input_dim'][0]}\")\n",
    "    print(f\"  output_dim: {npz_data['output_dim'][0]}\")\n",
    "    print(f\"  activation_code: {npz_data['activation_code'][0]} (0=relu)\")\n",
    "    print(\"\\n  Layer weights:\")\n",
    "    for key in sorted(npz_data.keys()):\n",
    "        if 'weight' in key or 'bias' in key:\n",
    "            print(f\"    {key}: shape={npz_data[key].shape}, dtype={npz_data[key].dtype}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"JAX Pipeline Complete!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ UNet2D trained for {EPOCHS} epochs\")\n",
    "print(f\"✓ Final validation Dice: {history['va_dice'][-1]:.3f}\")\n",
    "print(f\"✓ MLP distillation accuracy: {mlp_hist[-1][1]:.3f}\")\n",
    "print(f\"✓ Checkpoints saved to: {CKPT_DIR}\")\n",
    "print(f\"✓ Artifacts saved to: {ART_DIR}\")\n",
    "print(f\"✓ MLP exported as NPZ and JSON\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== SECTION 13: Visualize MLP Predictions on 3D Grid ====\n",
    "\n",
    "# Load a sample case for visualization\n",
    "with open(train_csv) as fp:\n",
    "    sample_row = list(csv.DictReader(fp))[0]\n",
    "\n",
    "vol, aff, mask = load_case_3d(sample_row)\n",
    "X, Y, Z = vol.shape[1:]\n",
    "\n",
    "print(f\"Visualizing case: {sample_row['id']}\")\n",
    "print(f\"Volume shape: {vol.shape}\")\n",
    "\n",
    "# Create a 3D coordinate grid (normalized to [0,1])\n",
    "x_coords = np.linspace(0, 1, X)\n",
    "y_coords = np.linspace(0, 1, Y)\n",
    "z_coords = np.linspace(0, 1, Z)\n",
    "\n",
    "# Pick a few representative slices to visualize\n",
    "slice_indices = [Z//4, Z//2, 3*Z//4]\n",
    "\n",
    "fig, axes = plt.subplots(len(slice_indices), 4, figsize=(16, 4*len(slice_indices)))\n",
    "if len(slice_indices) == 1:\n",
    "    axes = axes.reshape(1, -1)\n",
    "\n",
    "for idx, z_slice in enumerate(slice_indices):\n",
    "    # Get FLAIR image for this slice\n",
    "    flair_slice = vol[3, :, :, z_slice]  # FLAIR is channel 3\n",
    "    \n",
    "    # Get ground truth UNet prediction for this slice\n",
    "    x_batch = jnp.array(vol[..., z_slice].copy())[None, ...]\n",
    "    x_batch = jnp.transpose(x_batch, (0, 2, 3, 1))\n",
    "    variables = {'params': state_params, 'batch_stats': state_batch_stats}\n",
    "    unet_logits = state.apply_fn(variables, x_batch, train=False)\n",
    "    unet_pred = np.array(jnp.argmax(unet_logits, axis=-1)[0])\n",
    "    \n",
    "    # Get MLP predictions for all voxels in this slice\n",
    "    mlp_pred_slice = np.zeros((X, Y), dtype=np.uint8)\n",
    "    \n",
    "    # Create coordinate grid for this slice\n",
    "    coords_2d = np.stack(np.meshgrid(x_coords, y_coords, indexing='ij'), axis=-1)\n",
    "    z_val = z_coords[z_slice]\n",
    "    \n",
    "    # Add z coordinate\n",
    "    coords_3d = np.concatenate([\n",
    "        coords_2d,\n",
    "        np.full((X, Y, 1), z_val)\n",
    "    ], axis=-1)  # [X, Y, 3]\n",
    "    \n",
    "    # Flatten for batch prediction\n",
    "    coords_flat = coords_3d.reshape(-1, 3)  # [X*Y, 3]\n",
    "    \n",
    "    # Predict in batches to avoid memory issues\n",
    "    batch_size = 8192\n",
    "    mlp_preds = []\n",
    "    for i in range(0, len(coords_flat), batch_size):\n",
    "        batch = jnp.array(coords_flat[i:i+batch_size])\n",
    "        logits = mlp.apply({'params': mlp_state.params}, batch)\n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        mlp_preds.append(np.array(preds))\n",
    "    \n",
    "    mlp_pred_slice = np.concatenate(mlp_preds).reshape(X, Y).astype(np.uint8)\n",
    "    \n",
    "    # Plot comparison\n",
    "    axes[idx, 0].imshow(flair_slice, cmap='gray')\n",
    "    axes[idx, 0].set_title(f'FLAIR (Slice {z_slice}/{Z})', fontsize=12)\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    axes[idx, 1].imshow(unet_pred, cmap='magma', vmin=0, vmax=1)\n",
    "    axes[idx, 1].set_title('UNet Prediction', fontsize=12)\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    axes[idx, 2].imshow(mlp_pred_slice, cmap='magma', vmin=0, vmax=1)\n",
    "    axes[idx, 2].set_title('MLP Prediction (Distilled)', fontsize=12)\n",
    "    axes[idx, 2].axis('off')\n",
    "    \n",
    "    # Difference map\n",
    "    diff = np.abs(unet_pred.astype(int) - mlp_pred_slice.astype(int))\n",
    "    axes[idx, 3].imshow(diff, cmap='hot', vmin=0, vmax=1)\n",
    "    axes[idx, 3].set_title(f'Difference (Err: {diff.sum()}/{X*Y})', fontsize=12)\n",
    "    axes[idx, 3].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(ART_DIR, \"mlp_vs_unet_comparison.png\"), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compute overall agreement statistics\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"MLP vs UNet Agreement Statistics\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_pixels = 0\n",
    "total_agreement = 0\n",
    "total_tumor_pixels = 0\n",
    "tumor_agreement = 0\n",
    "\n",
    "for z_slice in range(Z):\n",
    "    x_batch = jnp.array(vol[..., z_slice].copy())[None, ...]\n",
    "    x_batch = jnp.transpose(x_batch, (0, 2, 3, 1))\n",
    "    variables = {'params': state_params, 'batch_stats': state_batch_stats}\n",
    "    unet_logits = state.apply_fn(variables, x_batch, train=False)\n",
    "    unet_pred = np.array(jnp.argmax(unet_logits, axis=-1)[0])\n",
    "    \n",
    "    coords_2d = np.stack(np.meshgrid(x_coords, y_coords, indexing='ij'), axis=-1)\n",
    "    z_val = z_coords[z_slice]\n",
    "    coords_3d = np.concatenate([coords_2d, np.full((X, Y, 1), z_val)], axis=-1)\n",
    "    coords_flat = coords_3d.reshape(-1, 3)\n",
    "    \n",
    "    mlp_preds = []\n",
    "    for i in range(0, len(coords_flat), batch_size):\n",
    "        batch = jnp.array(coords_flat[i:i+batch_size])\n",
    "        logits = mlp.apply({'params': mlp_state.params}, batch)\n",
    "        preds = jnp.argmax(logits, axis=-1)\n",
    "        mlp_preds.append(np.array(preds))\n",
    "    \n",
    "    mlp_pred_slice = np.concatenate(mlp_preds).reshape(X, Y).astype(np.uint8)\n",
    "    \n",
    "    agreement = (unet_pred == mlp_pred_slice)\n",
    "    total_pixels += X * Y\n",
    "    total_agreement += agreement.sum()\n",
    "    \n",
    "    tumor_mask = (unet_pred == 1)\n",
    "    if tumor_mask.sum() > 0:\n",
    "        total_tumor_pixels += tumor_mask.sum()\n",
    "        tumor_agreement += (agreement & tumor_mask).sum()\n",
    "\n",
    "overall_acc = total_agreement / total_pixels\n",
    "tumor_acc = tumor_agreement / total_tumor_pixels if total_tumor_pixels > 0 else 0.0\n",
    "\n",
    "print(f\"Overall Pixel Accuracy:  {overall_acc:.4f} ({total_agreement}/{total_pixels})\")\n",
    "print(f\"Tumor Pixel Accuracy:    {tumor_acc:.4f} ({tumor_agreement}/{total_tumor_pixels})\")\n",
    "print(f\"Background Accuracy:     {(total_agreement - tumor_agreement)/(total_pixels - total_tumor_pixels):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis6020",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
