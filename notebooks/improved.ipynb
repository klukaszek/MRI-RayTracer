{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Research-Backed BraTS 2023 INR with Advanced Techniques\n",
    "\n",
    "\n",
    "\n",
    " Key improvements based on 2022-2025 literature:\n",
    "\n",
    " - Unified Focal Loss + TV regularization (spatial coherence)\n",
    "\n",
    " - Uncertainty-guided coordinate sampling (prevents over-segmentation)\n",
    "\n",
    " - Anisotropic Fourier features (accounts for voxel spacing)\n",
    "\n",
    " - Multi-metric evaluation (Dice, HD95, connected components)\n",
    "\n",
    " - Persistent train/val splits (reproducibility)\n",
    "\n",
    " - Two-stage training protocol (coarse → refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, time, pathlib, functools\n",
    "from typing import Tuple, Dict, Any\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import nibabel as nib\n",
    "import wandb\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import label as connected_components_3d\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "os.environ[\"JAX_PLATFORM_NAME\"] = \"cpu\"\n",
    "print('JAX devices:', jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Configuration\n",
    "# =============================================================================\n",
    "DATA_ROOT = pathlib.Path('../data/BraTS-2023')\n",
    "\n",
    "# Data splits (persistent files)\n",
    "SPLITS_DIR = pathlib.Path('../data/splits')\n",
    "SPLIT_TRAIN_FILE = SPLITS_DIR / 'train.txt'\n",
    "SPLIT_VAL_FILE = SPLITS_DIR / 'val.txt'\n",
    "\n",
    "# Sampling - Hybrid strategy (research-backed)\n",
    "GLOBAL_BATCH_SIZE = 12000  # Research recommends 12k-16k for medical segmentation\n",
    "MICRO_BATCH_SIZE = 4000\n",
    "ACCUM_STEPS = GLOBAL_BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "\n",
    "# Architecture - Anisotropic Fourier features\n",
    "FOURIER_FREQS = [1, 2, 4, 8, 16]  # Multi-scale frequencies\n",
    "FOURIER_DIM = 128  # Learnable Fourier features\n",
    "HIDDEN_DIMS = [64, 64, 64]  # Research-backed sizes\n",
    "COORD_INJECTION_LAYERS = [1, 2, 3]\n",
    "MODALITY_INJECTION_LAYERS = [2]\n",
    "DROPOUT_RATE = 0.3  # Research recommends 0.3\n",
    "\n",
    "# Training - Two-stage protocol\n",
    "STAGE1_STEPS = 3000  # Coarse learning\n",
    "STAGE2_STEPS = 0  # Boundary refinement\n",
    "TOTAL_STEPS = STAGE1_STEPS + STAGE2_STEPS\n",
    "\n",
    "LR_STAGE1 = 5e-5  # Reduced from 2e-4\n",
    "LR_STAGE2 = 1e-5  # Reduced from 5e-5\n",
    "MIN_LR = 1e-7\n",
    "WARMUP_STEPS = 1000  # Increased from 500\n",
    "WEIGHT_DECAY = 0.01\n",
    "CLIP_NORM = 0.5  # Increased clipping from 1.0\n",
    "\n",
    "# Loss weights - Research-backed Unified Focal + TV\n",
    "NUM_CLASSES = 4\n",
    "UNIFIED_FOCAL_GAMMA = 0.5  # Key hyperparameter\n",
    "UNIFIED_FOCAL_DELTA = 0.6\n",
    "UNIFIED_FOCAL_LAMBDA = 0.5\n",
    "\n",
    "# Stage-dependent loss weights\n",
    "TV_WEIGHT_STAGE1 = 0.02  # Light TV in stage 1\n",
    "TV_WEIGHT_STAGE2 = 0.05  # Stronger TV in stage 2\n",
    "BOUNDARY_WEIGHT = 0.1\n",
    "\n",
    "CLASS_WEIGHTS = [0.1, 1.5, 1.0, 2.0]  # Background, NCR, ED, ET (favor small classes)\n",
    "\n",
    "# Sampling strategy - Uncertainty-guided\n",
    "UNCERTAINTY_RATIO = 0.5  # 50% uncertainty-guided (increases in stage 2)\n",
    "BALANCED_RATIO = 0.3     # 30% balanced class sampling\n",
    "UNIFORM_RATIO = 0.2      # 20% uniform random\n",
    "\n",
    "# Stochastic preconditioning\n",
    "COORD_NOISE_SIGMA_INIT = 0.3\n",
    "COORD_NOISE_SIGMA_FINAL = 0.1\n",
    "\n",
    "# Validation & evaluation\n",
    "VAL_EVAL_STEPS = 1000  # Evaluate every 500 steps\n",
    "CHUNK_SIZE = 128  # Cases per chunk\n",
    "STEPS_PER_CHUNK = 100  # Rotate chunks every 1000 steps\n",
    "\n",
    "RNG_SEED = 42\n",
    "jax_key = jax.random.PRNGKey(RNG_SEED)\n",
    "\n",
    "# W&B\n",
    "WANDB_PROJECT = \"brats-inr-research-backed\"\n",
    "WANDB_TAGS = [\"unified-focal\", \"tv-regularization\", \"uncertainty-sampling\", \"anisotropic-fourier\"]\n",
    "WANDB_NOTES = \"Research-backed INR with spatial coherence and multi-metric evaluation\"\n",
    "\n",
    "config = {\n",
    "    \"global_batch_size\": GLOBAL_BATCH_SIZE,\n",
    "    \"fourier_dim\": FOURIER_DIM,\n",
    "    \"hidden_dims\": HIDDEN_DIMS,\n",
    "    \"total_steps\": TOTAL_STEPS,\n",
    "    \"unified_focal_gamma\": UNIFIED_FOCAL_GAMMA,\n",
    "    \"tv_weight_stage1\": TV_WEIGHT_STAGE1,\n",
    "    \"tv_weight_stage2\": TV_WEIGHT_STAGE2,\n",
    "    \"uncertainty_ratio\": UNCERTAINTY_RATIO,\n",
    "    \"coord_noise_sigma\": COORD_NOISE_SIGMA_INIT,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    project=WANDB_PROJECT,\n",
    "    config=config,\n",
    "    tags=WANDB_TAGS,\n",
    "    notes=WANDB_NOTES,\n",
    ")\n",
    "\n",
    "print(f\"W&B Run: {wandb.run.name}\")\n",
    "SAVE_PATH = pathlib.Path(f\"../artifacts/{WANDB_PROJECT}/{wandb.run.name}/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data Splitting (Persistent - Research Best Practice)\n",
    "# =============================================================================\n",
    "MODALITY_SUFFIXES = ['t1n', 't1c', 't2w', 't2f']\n",
    "SEG_SUFFIX = 'seg'\n",
    "\n",
    "def find_cases(root: pathlib.Path):\n",
    "    \"\"\"Find all case directories\"\"\"\n",
    "    cases = []\n",
    "    for p in sorted(root.iterdir()):\n",
    "        if p.is_dir():\n",
    "            if any((p / f'{p.name}-{m}.nii.gz').exists() for m in MODALITY_SUFFIXES):\n",
    "                cases.append(p)\n",
    "    return cases\n",
    "\n",
    "def load_split_file(split_file: pathlib.Path) -> list:\n",
    "    \"\"\"Load case names from split file\"\"\"\n",
    "    if not split_file.exists():\n",
    "        return None\n",
    "    with open(split_file, 'r') as f:\n",
    "        case_names = [line.strip() for line in f if line.strip()]\n",
    "    return case_names\n",
    "\n",
    "def save_split_file(split_file: pathlib.Path, case_names: list):\n",
    "    \"\"\"Save case names to split file\"\"\"\n",
    "    split_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(split_file, 'w') as f:\n",
    "        for name in case_names:\n",
    "            f.write(f\"{name}\\n\")\n",
    "    print(f\"Saved {len(case_names)} cases to {split_file}\")\n",
    "\n",
    "def get_case_name(case_path: pathlib.Path) -> str:\n",
    "    \"\"\"Extract case name from path\"\"\"\n",
    "    return case_path.name\n",
    "\n",
    "def create_splits_if_missing(all_cases: list, rng_seed: int = 42):\n",
    "    \"\"\"Create persistent train/val splits\"\"\"\n",
    "    train_names = load_split_file(SPLIT_TRAIN_FILE)\n",
    "    val_names = load_split_file(SPLIT_VAL_FILE)\n",
    "    \n",
    "    if train_names is not None and val_names is not None:\n",
    "        print(f\"✓ Loaded existing splits: {len(train_names)} train, {len(val_names)} val\")\n",
    "        return train_names, val_names\n",
    "    \n",
    "    print(f\"Creating new splits from {len(all_cases)} cases...\")\n",
    "    all_case_names = [get_case_name(c) for c in all_cases]\n",
    "    \n",
    "    rng = np.random.default_rng(rng_seed)\n",
    "    shuffled_names = list(all_case_names)\n",
    "    rng.shuffle(shuffled_names)\n",
    "    \n",
    "    split_idx = int(len(shuffled_names) * 0.8)\n",
    "    train_names = shuffled_names[:split_idx]\n",
    "    val_names = shuffled_names[split_idx:]\n",
    "    \n",
    "    save_split_file(SPLIT_TRAIN_FILE, train_names)\n",
    "    save_split_file(SPLIT_VAL_FILE, val_names)\n",
    "    \n",
    "    print(f\"Created splits: {len(train_names)} train, {len(val_names)} val\")\n",
    "    return train_names, val_names\n",
    "\n",
    "def match_cases_to_names(all_cases: list, case_names: list) -> list:\n",
    "    \"\"\"Match case paths to names\"\"\"\n",
    "    case_name_to_path = {get_case_name(cp): cp for cp in all_cases}\n",
    "    matched_cases = []\n",
    "    missing = []\n",
    "    \n",
    "    for name in case_names:\n",
    "        if name in case_name_to_path:\n",
    "            matched_cases.append(case_name_to_path[name])\n",
    "        else:\n",
    "            missing.append(name)\n",
    "    \n",
    "    if missing:\n",
    "        print(f\"⚠ Warning: {len(missing)} cases from split file not found\")\n",
    "    \n",
    "    return matched_cases\n",
    "\n",
    "# Discover and split\n",
    "print(\"Discovering cases...\")\n",
    "all_cases_full = find_cases(DATA_ROOT)\n",
    "print(f'Total discovered: {len(all_cases_full)} cases')\n",
    "\n",
    "train_names, val_names = create_splits_if_missing(all_cases_full, RNG_SEED)\n",
    "train_cases = match_cases_to_names(all_cases_full, train_names)\n",
    "val_cases = match_cases_to_names(all_cases_full, val_names)\n",
    "\n",
    "print(f'Using: Train={len(train_cases)}, Val={len(val_cases)}')\n",
    "\n",
    "# Verify no overlap\n",
    "train_names_set = set(get_case_name(c) for c in train_cases)\n",
    "val_names_set = set(get_case_name(c) for c in val_cases)\n",
    "if train_names_set & val_names_set:\n",
    "    raise ValueError(\"Train/Val overlap detected!\")\n",
    "print(\"✓ No overlap between train and validation sets\")\n",
    "\n",
    "wandb.config.update({\"train_cases\": len(train_cases), \"val_cases\": len(val_cases)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data Loading\n",
    "# =============================================================================\n",
    "def load_case(case_dir: pathlib.Path):\n",
    "    \"\"\"Load and normalize a single case\"\"\"\n",
    "    base = case_dir.name\n",
    "    mods = []\n",
    "    for suf in MODALITY_SUFFIXES:\n",
    "        fp = case_dir / f'{base}-{suf}.nii.gz'\n",
    "        img = nib.load(str(fp))\n",
    "        arr = img.get_fdata().astype(np.float32)\n",
    "        mask = arr != 0\n",
    "        if mask.any():\n",
    "            mu = arr[mask].mean()\n",
    "            sigma = arr[mask].std() + 1e-6\n",
    "            arr = (arr - mu) / sigma\n",
    "        mods.append(arr)\n",
    "    \n",
    "    seg_fp = case_dir / f'{base}-{SEG_SUFFIX}.nii.gz'\n",
    "    seg = nib.load(str(seg_fp)).get_fdata().astype(np.int16)\n",
    "    mods_arr = np.stack(mods, axis=0)\n",
    "    return mods_arr, seg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Chunked Cache\n",
    "# =============================================================================\n",
    "class ChunkedBraTSCache:\n",
    "    \"\"\"Memory-efficient chunked data loading\"\"\"\n",
    "    def __init__(self, case_paths, chunk_size=128, name=\"cache\"):\n",
    "        self.case_paths = case_paths\n",
    "        self.chunk_size = chunk_size\n",
    "        self.name = name\n",
    "        self.n_cases = len(case_paths)\n",
    "        self.n_chunks = int(np.ceil(self.n_cases / chunk_size))\n",
    "        self.current_chunk_idx = 0\n",
    "        \n",
    "        print(f'Init {name}: {self.n_cases} cases, {self.n_chunks} chunks')\n",
    "        first_mods, first_seg = load_case(case_paths[0])\n",
    "        self.vol_shape = first_mods.shape[1:]\n",
    "        self.n_modalities = first_mods.shape[0]\n",
    "        \n",
    "        # Pre-compute boundary distance transforms (for boundary loss)\n",
    "        self.boundary_dists = []\n",
    "        \n",
    "        self.cache = []\n",
    "        self.chunk_case_indices = []\n",
    "        self._load_chunk(0)\n",
    "        \n",
    "    def _load_chunk(self, chunk_idx):\n",
    "        \"\"\"Load specific chunk\"\"\"\n",
    "        start_idx = chunk_idx * self.chunk_size\n",
    "        end_idx = min(start_idx + self.chunk_size, self.n_cases)\n",
    "        chunk_paths = self.case_paths[start_idx:end_idx]\n",
    "        \n",
    "        print(f'Loading {self.name} chunk {chunk_idx+1}/{self.n_chunks}...')\n",
    "        \n",
    "        self.cache = []\n",
    "        self.boundary_dists = []\n",
    "        self.chunk_case_indices = list(range(start_idx, end_idx))\n",
    "        \n",
    "        for i, cp in enumerate(chunk_paths):\n",
    "            if i % 20 == 0 and i > 0:\n",
    "                print(f'  {i}/{len(chunk_paths)}...')\n",
    "            mods, seg = load_case(cp)\n",
    "            \n",
    "            # Compute boundary distance transform for boundary loss\n",
    "            boundary_dist = np.zeros_like(seg, dtype=np.float32)\n",
    "            for c in range(1, NUM_CLASSES):\n",
    "                mask = (seg == c).astype(np.uint8)\n",
    "                if mask.sum() > 0:\n",
    "                    dist = distance_transform_edt(1 - mask)\n",
    "                    boundary_dist = np.maximum(boundary_dist, 1.0 / (1.0 + dist))\n",
    "            \n",
    "            self.cache.append({'mods': mods, 'seg': seg})\n",
    "            self.boundary_dists.append(boundary_dist)\n",
    "        \n",
    "        bytes_per_case = self.cache[0]['mods'].nbytes + self.cache[0]['seg'].nbytes\n",
    "        chunk_gb = (bytes_per_case * len(self.cache)) / 1e9\n",
    "        print(f'{self.name} chunk loaded: {len(self.cache)} cases, {chunk_gb:.2f} GB')\n",
    "        \n",
    "        self.current_chunk_idx = chunk_idx\n",
    "    \n",
    "    def next_chunk(self):\n",
    "        \"\"\"Load next chunk\"\"\"\n",
    "        next_idx = (self.current_chunk_idx + 1) % self.n_chunks\n",
    "        self._load_chunk(next_idx)\n",
    "    \n",
    "    def get_current_chunk_indices(self):\n",
    "        \"\"\"Get current chunk case indices\"\"\"\n",
    "        return self.chunk_case_indices\n",
    "\n",
    "train_cache = ChunkedBraTSCache(train_cases, chunk_size=CHUNK_SIZE, name=\"train\")\n",
    "val_cache = ChunkedBraTSCache(val_cases, chunk_size=CHUNK_SIZE, name=\"val\") if val_cases else None\n",
    "\n",
    "H, W, D = train_cache.vol_shape\n",
    "M = train_cache.n_modalities\n",
    "\n",
    "print(f'Volume: {train_cache.vol_shape}, Modalities: {M}')\n",
    "wandb.config.update({\"volume_shape\": list(train_cache.vol_shape), \"num_modalities\": M})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Anisotropic Fourier Features (Research-Backed)\n",
    "# =============================================================================\n",
    "def init_anisotropic_fourier_features(key, fourier_dim, input_dim=3, voxel_spacing=(1.0, 1.0, 1.0)):\n",
    "    \"\"\"\n",
    "    Initialize learnable anisotropic Fourier features.\n",
    "    Accounts for different voxel spacings in medical imaging.\n",
    "    \"\"\"\n",
    "    # Random Gaussian initialization scaled by voxel spacing\n",
    "    key, subkey = jax.random.split(key)\n",
    "    B = jax.random.normal(subkey, (fourier_dim // 2, input_dim)) * 5.0  # σ = 5\n",
    "    \n",
    "    # Scale by voxel spacing for anisotropic correction\n",
    "    voxel_scale = jnp.array(voxel_spacing)\n",
    "    B = B / voxel_scale[None, :]  # Broadcast and divide\n",
    "    \n",
    "    return key, {'B': B}\n",
    "\n",
    "def apply_fourier_features(fourier_params, coords):\n",
    "    \"\"\"\n",
    "    Apply Fourier feature mapping: γ(x) = [sin(2πB·x), cos(2πB·x)]\n",
    "    coords: (N, 3) in range [-1, 1]\n",
    "    \"\"\"\n",
    "    B = fourier_params['B']\n",
    "    # Map to [0, 1] for frequency application\n",
    "    coords_01 = (coords + 1.0) / 2.0\n",
    "    \n",
    "    angles = 2 * jnp.pi * jnp.dot(coords_01, B.T)  # (N, fourier_dim//2)\n",
    "    features = jnp.concatenate([jnp.sin(angles), jnp.cos(angles)], axis=-1)  # (N, fourier_dim)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Coordinate Injection MLP with Dropout\n",
    "# =============================================================================\n",
    "def glorot(key, shape):\n",
    "    fan_in, fan_out = shape[0], shape[1]\n",
    "    limit = math.sqrt(6.0 / (fan_in + fan_out))\n",
    "    return jax.random.uniform(key, shape, minval=-limit, maxval=limit)\n",
    "\n",
    "def init_coord_injection_mlp(key, coord_dim, modality_dim, fourier_dim, hidden_dims, out_dim, dropout_rate=0.3):\n",
    "    \"\"\"Initialize MLP with Fourier features + modalities\"\"\"\n",
    "    params = []\n",
    "    \n",
    "    # Layer 0: Fourier features + modalities\n",
    "    key, k1 = jax.random.split(key)\n",
    "    W = glorot(k1, (fourier_dim + modality_dim, hidden_dims[0]))\n",
    "    b = jnp.zeros((hidden_dims[0],))\n",
    "    params.append({'W': W, 'b': b})\n",
    "    \n",
    "    # Hidden layers with coordinate/modality injection\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "        key, k1 = jax.random.split(key)\n",
    "        \n",
    "        in_dim = hidden_dims[i-1]\n",
    "        if i in COORD_INJECTION_LAYERS:\n",
    "            in_dim += coord_dim\n",
    "        if i in MODALITY_INJECTION_LAYERS:\n",
    "            in_dim += modality_dim\n",
    "        \n",
    "        W = glorot(k1, (in_dim, hidden_dims[i]))\n",
    "        b = jnp.zeros((hidden_dims[i],))\n",
    "        params.append({'W': W, 'b': b})\n",
    "    \n",
    "    # Output layer\n",
    "    key, k1 = jax.random.split(key)\n",
    "    W = glorot(k1, (hidden_dims[-1], out_dim))\n",
    "    b = jnp.zeros((out_dim,))\n",
    "    params.append({'W': W, 'b': b})\n",
    "    \n",
    "    return key, params\n",
    "\n",
    "def apply_coord_injection_mlp(params, coords, modalities, fourier_features, training=True, dropout_key=None, dropout_rate=0.3):\n",
    "    \"\"\"Apply MLP with dropout during training - simplified for JIT compatibility\"\"\"\n",
    "    # Layer 0\n",
    "    h = jnp.dot(jnp.concatenate([fourier_features, modalities], axis=-1), params[0]['W']) + params[0]['b']\n",
    "    h = jax.nn.relu(h)\n",
    "    \n",
    "    # Apply dropout conditionally using jnp.where\n",
    "    # Only apply if training and dropout_key is provided\n",
    "    if dropout_key is not None:\n",
    "        dropout_key, subkey = jax.random.split(dropout_key)\n",
    "        keep_prob = 1.0 - dropout_rate\n",
    "        mask = jax.random.bernoulli(subkey, keep_prob, h.shape)\n",
    "        h = jnp.where(training, h * mask / (keep_prob + 1e-10), h)\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(1, len(params) - 1):\n",
    "        inputs = [h]\n",
    "        \n",
    "        if i in COORD_INJECTION_LAYERS:\n",
    "            inputs.append(coords)\n",
    "        if i in MODALITY_INJECTION_LAYERS:\n",
    "            inputs.append(modalities)\n",
    "        \n",
    "        h = jnp.concatenate(inputs, axis=-1) if len(inputs) > 1 else h\n",
    "        h = jnp.dot(h, params[i]['W']) + params[i]['b']\n",
    "        h = jax.nn.relu(h)\n",
    "        \n",
    "        # Dropout\n",
    "        if dropout_key is not None:\n",
    "            dropout_key, subkey = jax.random.split(dropout_key)\n",
    "            keep_prob = 1.0 - dropout_rate\n",
    "            mask = jax.random.bernoulli(subkey, keep_prob, h.shape)\n",
    "            h = jnp.where(training, h * mask / (keep_prob + 1e-10), h)\n",
    "    \n",
    "    # Output\n",
    "    logits = jnp.dot(h, params[-1]['W']) + params[-1]['b']\n",
    "    \n",
    "    return logits\n",
    "\n",
    "# Initialize model\n",
    "jax_key, fourier_params = init_anisotropic_fourier_features(\n",
    "    jax_key, \n",
    "    FOURIER_DIM, \n",
    "    input_dim=3,\n",
    "    voxel_spacing=(1.0, 1.0, 1.0)  # BraTS is pre-resampled to 1mm isotropic\n",
    ")\n",
    "\n",
    "jax_key, mlp_params = init_coord_injection_mlp(\n",
    "    jax_key,\n",
    "    coord_dim=3,\n",
    "    modality_dim=M,\n",
    "    fourier_dim=FOURIER_DIM,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    out_dim=NUM_CLASSES,\n",
    "    dropout_rate=DROPOUT_RATE\n",
    ")\n",
    "\n",
    "# Combine all parameters\n",
    "params = {\n",
    "    'fourier': fourier_params,\n",
    "    'mlp': mlp_params\n",
    "}\n",
    "\n",
    "n_params_fourier = fourier_params['B'].size\n",
    "n_params_mlp = sum(p['W'].size + p['b'].size for p in mlp_params)\n",
    "total_params = n_params_fourier + n_params_mlp\n",
    "\n",
    "print(f'Total parameters: {total_params:,}')\n",
    "print(f'  Fourier (learnable): {n_params_fourier:,}')\n",
    "print(f'  MLP: {n_params_mlp:,}')\n",
    "\n",
    "wandb.config.update({\"total_parameters\": total_params})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Research-Backed Loss Functions\n",
    "# =============================================================================\n",
    "\n",
    "@jax.jit\n",
    "def unified_focal_loss(logits, labels, gamma=UNIFIED_FOCAL_GAMMA, delta=UNIFIED_FOCAL_DELTA, lambda_param=UNIFIED_FOCAL_LAMBDA):\n",
    "    \"\"\"\n",
    "    Unified Focal Loss from research literature.\n",
    "    Combines modified Focal Tversky Loss and modified Focal Loss.\n",
    "    \n",
    "    UFL = λ·mFTL + (1-λ)·mFL\n",
    "    \"\"\"\n",
    "    num_classes = logits.shape[-1]\n",
    "    # Clip logits for numerical stability\n",
    "    logits_clipped = jnp.clip(logits, -10.0, 10.0)\n",
    "    probs = jax.nn.softmax(logits_clipped, axis=-1)\n",
    "    probs = jnp.clip(probs, 1e-7, 1.0 - 1e-7)  # Prevent log(0)\n",
    "    targets = jax.nn.one_hot(labels, num_classes)\n",
    "    \n",
    "    # Modified Focal Tversky Loss (mFTL)\n",
    "    alpha = 0.3  # Favor recall\n",
    "    beta = 0.7\n",
    "    \n",
    "    ftl_losses = []\n",
    "    for c in range(num_classes):\n",
    "        p_c = probs[:, c]\n",
    "        t_c = targets[:, c]\n",
    "        \n",
    "        tp = (p_c * t_c).sum()\n",
    "        fp = (p_c * (1 - t_c)).sum()\n",
    "        fn = ((1 - p_c) * t_c).sum()\n",
    "        \n",
    "        tversky_index = tp / (tp + alpha * fp + beta * fn + 1e-7)\n",
    "        tversky_index = jnp.clip(tversky_index, 0.0, 1.0)\n",
    "        ftl_c = jnp.power(1.0 - tversky_index + 1e-7, 1.0 / gamma)\n",
    "        ftl_losses.append(ftl_c)\n",
    "    \n",
    "    mFTL = jnp.stack(ftl_losses).mean()\n",
    "    \n",
    "    # Modified Focal Loss (mFL)\n",
    "    ce = -jnp.sum(targets * jnp.log(probs + 1e-10), axis=-1)\n",
    "    p_t = jnp.sum(probs * targets, axis=-1)\n",
    "    p_t = jnp.clip(p_t, 1e-7, 1.0 - 1e-7)\n",
    "    focal_weight = jnp.power(1.0 - p_t + 1e-7, gamma)\n",
    "    mFL = (focal_weight * ce).mean()\n",
    "    \n",
    "    # Unified - add small epsilon to prevent NaN\n",
    "    ufl = lambda_param * mFTL + (1 - lambda_param) * mFL\n",
    "    ufl = jnp.clip(ufl, 0.0, 1e6)  # Prevent explosion\n",
    "    \n",
    "    return ufl, {'mFTL': mFTL, 'mFL': mFL}\n",
    "\n",
    "@jax.jit\n",
    "def dice_loss(logits, labels, class_weights=None):\n",
    "    \"\"\"Dice loss with class weighting\"\"\"\n",
    "    if class_weights is None:\n",
    "        class_weights = jnp.ones(NUM_CLASSES)\n",
    "    else:\n",
    "        class_weights = jnp.array(class_weights)\n",
    "    \n",
    "    logits_clipped = jnp.clip(logits, -10.0, 10.0)\n",
    "    probs = jax.nn.softmax(logits_clipped, axis=-1)\n",
    "    probs = jnp.clip(probs, 1e-7, 1.0 - 1e-7)\n",
    "    targets = jax.nn.one_hot(labels, NUM_CLASSES)\n",
    "    \n",
    "    dice_per_class = []\n",
    "    for c in range(NUM_CLASSES):\n",
    "        p_c = probs[:, c]\n",
    "        t_c = targets[:, c]\n",
    "        \n",
    "        intersection = (p_c * t_c).sum()\n",
    "        union = p_c.sum() + t_c.sum()\n",
    "        \n",
    "        dice_c = (2.0 * intersection + 1.0) / (union + 1.0)\n",
    "        dice_per_class.append(dice_c)\n",
    "    \n",
    "    dice_per_class = jnp.stack(dice_per_class)\n",
    "    weighted_dice = (dice_per_class * class_weights).sum() / (class_weights.sum() + 1e-10)\n",
    "    loss = 1.0 - weighted_dice\n",
    "    loss = jnp.clip(loss, 0.0, 10.0)\n",
    "    \n",
    "    return loss, dice_per_class\n",
    "\n",
    "def compute_tv_loss(logits, coords_grid=None):\n",
    "    \"\"\"\n",
    "    Total Variation loss for spatial coherence.\n",
    "    Penalizes abrupt changes in predictions.\n",
    "    \n",
    "    For point cloud data, approximate via nearest neighbor differences.\n",
    "    \"\"\"\n",
    "    # For now, use simple gradient penalty on logits\n",
    "    # In practice, would compute spatial gradients properly\n",
    "    # This is a simplified version for coordinate-based predictions\n",
    "    \n",
    "    # Gradient magnitude penalty\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    \n",
    "    # Approximate spatial TV via variance in batch\n",
    "    # (proper implementation would require spatial neighbors)\n",
    "    tv_approx = jnp.var(probs, axis=0).sum()\n",
    "    \n",
    "    return tv_approx\n",
    "\n",
    "def loss_fn(params, coords, modalities, labels, boundary_weights, training=True, dropout_key=None, tv_weight=0.02):\n",
    "    \"\"\"Combined loss with all research-backed components\"\"\"\n",
    "    # Forward pass\n",
    "    fourier_feats = apply_fourier_features(params['fourier'], coords)\n",
    "    logits = apply_coord_injection_mlp(\n",
    "        params['mlp'], \n",
    "        coords, \n",
    "        modalities, \n",
    "        fourier_feats,\n",
    "        training=training,\n",
    "        dropout_key=dropout_key,\n",
    "        dropout_rate=DROPOUT_RATE\n",
    "    )\n",
    "    \n",
    "    # Unified Focal Loss\n",
    "    ufl, ufl_aux = unified_focal_loss(logits, labels)\n",
    "    \n",
    "    # Dice Loss\n",
    "    dice_loss_val, dice_per_class = dice_loss(logits, labels, CLASS_WEIGHTS)\n",
    "    \n",
    "    # TV regularization\n",
    "    tv_loss_val = compute_tv_loss(logits, coords)\n",
    "    \n",
    "    # Boundary loss (weight voxels near boundaries)\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    preds = jnp.argmax(logits, axis=-1)\n",
    "    boundary_loss_val = jnp.mean(boundary_weights * jnp.abs(preds - labels))\n",
    "    \n",
    "    # Combined loss\n",
    "    loss = (\n",
    "        0.5 * ufl +\n",
    "        0.5 * dice_loss_val +\n",
    "        tv_weight * tv_loss_val +\n",
    "        BOUNDARY_WEIGHT * boundary_loss_val\n",
    "    )\n",
    "    \n",
    "    # Metrics\n",
    "    tumor_dice = dice_per_class[1:].mean()\n",
    "    \n",
    "    aux = {\n",
    "        'ufl': ufl,\n",
    "        'mFTL': ufl_aux['mFTL'],\n",
    "        'mFL': ufl_aux['mFL'],\n",
    "        'dice_loss': dice_loss_val,\n",
    "        'tv_loss': tv_loss_val,\n",
    "        'boundary_loss': boundary_loss_val,\n",
    "        'dice_per_class': dice_per_class,\n",
    "        'dice_mean_tumor': tumor_dice,\n",
    "        'accuracy': (preds == labels).mean(),\n",
    "    }\n",
    "    \n",
    "    return loss, aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Uncertainty-Guided Sampling (Research-Backed)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_uncertainty_mc_dropout(params, coords, modalities, fourier_params, n_samples=5, dropout_key=None):\n",
    "    \"\"\"\n",
    "    Compute uncertainty via MC-Dropout.\n",
    "    Returns: entropy-based uncertainty scores.\n",
    "    \"\"\"\n",
    "    all_probs = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if dropout_key is not None:\n",
    "            dropout_key, subkey = jax.random.split(dropout_key)\n",
    "        else:\n",
    "            subkey = None\n",
    "        \n",
    "        fourier_feats = apply_fourier_features(fourier_params, coords)\n",
    "        logits = apply_coord_injection_mlp(\n",
    "            params,\n",
    "            coords,\n",
    "            modalities,\n",
    "            fourier_feats,\n",
    "            training=True,  # Keep dropout on\n",
    "            dropout_key=subkey,\n",
    "            dropout_rate=DROPOUT_RATE\n",
    "        )\n",
    "        probs = jax.nn.softmax(logits, axis=-1)\n",
    "        all_probs.append(probs)\n",
    "    \n",
    "    # Average predictions\n",
    "    all_probs = jnp.stack(all_probs, axis=0)  # (n_samples, N, num_classes)\n",
    "    mean_probs = all_probs.mean(axis=0)  # (N, num_classes)\n",
    "    \n",
    "    # Entropy as uncertainty\n",
    "    entropy = -jnp.sum(mean_probs * jnp.log(mean_probs + 1e-10), axis=-1)  # (N,)\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "def sample_batch_hybrid(rng_key, batch_size, cache, params, fourier_params, \n",
    "                        uncertainty_ratio=0.5, balanced_ratio=0.3, uniform_ratio=0.2,\n",
    "                        use_uncertainty=True):\n",
    "    \"\"\"\n",
    "    Hybrid sampling: uncertainty + balanced + uniform.\n",
    "    Research shows this prevents over-segmentation.\n",
    "    \"\"\"\n",
    "    available_cases = cache.get_current_chunk_indices()\n",
    "    \n",
    "    n_uncertainty = int(batch_size * uncertainty_ratio) if use_uncertainty else 0\n",
    "    n_balanced = int(batch_size * balanced_ratio)\n",
    "    n_uniform = batch_size - n_uncertainty - n_balanced\n",
    "    \n",
    "    all_coords, all_intens, all_labels, all_boundary_weights = [], [], [], []\n",
    "    \n",
    "    # 1. Uniform random sampling\n",
    "    if n_uniform > 0:\n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        key_int = int(np.array(subkey)[0])\n",
    "        np.random.seed(key_int)\n",
    "        \n",
    "        ci = np.random.choice(available_cases, size=n_uniform)\n",
    "        xs = np.random.randint(0, H, n_uniform)\n",
    "        ys = np.random.randint(0, W, n_uniform)\n",
    "        zs = np.random.randint(0, D, n_uniform)\n",
    "        \n",
    "        coords_uniform = []\n",
    "        intens_uniform = []\n",
    "        labels_uniform = []\n",
    "        boundary_uniform = []\n",
    "        \n",
    "        for i in range(n_uniform):\n",
    "            case_idx = int(ci[i])\n",
    "            local_idx = available_cases.index(case_idx)\n",
    "            x, y, z = int(xs[i]), int(ys[i]), int(zs[i])\n",
    "            \n",
    "            coords_uniform.append([float(x), float(y), float(z)])\n",
    "            intens_uniform.append(cache.cache[local_idx]['mods'][:, x, y, z])\n",
    "            labels_uniform.append(int(cache.cache[local_idx]['seg'][x, y, z]))\n",
    "            boundary_uniform.append(float(cache.boundary_dists[local_idx][x, y, z]))\n",
    "        \n",
    "        all_coords.append(np.array(coords_uniform, dtype=np.float32))\n",
    "        all_intens.append(np.stack(intens_uniform, axis=0))\n",
    "        all_labels.append(np.array(labels_uniform, dtype=np.int32))\n",
    "        all_boundary_weights.append(np.array(boundary_uniform, dtype=np.float32))\n",
    "    \n",
    "    # 2. Balanced class sampling\n",
    "    if n_balanced > 0:\n",
    "        n_per_class = n_balanced // NUM_CLASSES\n",
    "        \n",
    "        for c in range(NUM_CLASSES):\n",
    "            rng_key, subkey = jax.random.split(rng_key)\n",
    "            key_int = int(np.array(subkey)[0])\n",
    "            np.random.seed(key_int)\n",
    "            \n",
    "            coords_class = []\n",
    "            intens_class = []\n",
    "            labels_class = []\n",
    "            boundary_class = []\n",
    "            \n",
    "            attempts = 0\n",
    "            while len(coords_class) < n_per_class and attempts < n_per_class * 100:\n",
    "                n_sample = min(1024, n_per_class * 5)\n",
    "                ci = np.random.choice(available_cases, size=n_sample)\n",
    "                xs = np.random.randint(0, H, n_sample)\n",
    "                ys = np.random.randint(0, W, n_sample)\n",
    "                zs = np.random.randint(0, D, n_sample)\n",
    "                \n",
    "                for i in range(n_sample):\n",
    "                    if len(coords_class) >= n_per_class:\n",
    "                        break\n",
    "                    \n",
    "                    case_idx = int(ci[i])\n",
    "                    local_idx = available_cases.index(case_idx)\n",
    "                    x, y, z = int(xs[i]), int(ys[i]), int(zs[i])\n",
    "                    label = int(cache.cache[local_idx]['seg'][x, y, z])\n",
    "                    \n",
    "                    if label == c:\n",
    "                        coords_class.append([float(x), float(y), float(z)])\n",
    "                        intens_class.append(cache.cache[local_idx]['mods'][:, x, y, z])\n",
    "                        labels_class.append(label)\n",
    "                        boundary_class.append(float(cache.boundary_dists[local_idx][x, y, z]))\n",
    "                \n",
    "                attempts += n_sample\n",
    "            \n",
    "            # Pad if needed\n",
    "            while len(coords_class) < n_per_class:\n",
    "                if coords_class:\n",
    "                    coords_class.append(coords_class[-1])\n",
    "                    intens_class.append(intens_class[-1])\n",
    "                    labels_class.append(labels_class[-1])\n",
    "                    boundary_class.append(boundary_class[-1])\n",
    "                else:\n",
    "                    coords_class.append([0.0, 0.0, 0.0])\n",
    "                    intens_class.append(np.zeros(M, dtype=np.float32))\n",
    "                    labels_class.append(0)\n",
    "                    boundary_class.append(0.0)\n",
    "            \n",
    "            all_coords.append(np.array(coords_class[:n_per_class], dtype=np.float32))\n",
    "            all_intens.append(np.stack(intens_class[:n_per_class], axis=0))\n",
    "            all_labels.append(np.array(labels_class[:n_per_class], dtype=np.int32))\n",
    "            all_boundary_weights.append(np.array(boundary_class[:n_per_class], dtype=np.float32))\n",
    "    \n",
    "    # 3. Uncertainty-guided sampling (if enabled)\n",
    "    if n_uncertainty > 0 and use_uncertainty:\n",
    "        # Sample candidates\n",
    "        n_candidates = min(10000, n_uncertainty * 10)\n",
    "        \n",
    "        rng_key, subkey = jax.random.split(rng_key)\n",
    "        key_int = int(np.array(subkey)[0])\n",
    "        np.random.seed(key_int)\n",
    "        \n",
    "        ci_cand = np.random.choice(available_cases, size=n_candidates)\n",
    "        xs_cand = np.random.randint(0, H, n_candidates)\n",
    "        ys_cand = np.random.randint(0, W, n_candidates)\n",
    "        zs_cand = np.random.randint(0, D, n_candidates)\n",
    "        \n",
    "        coords_cand = []\n",
    "        intens_cand = []\n",
    "        labels_cand = []\n",
    "        boundary_cand = []\n",
    "        \n",
    "        for i in range(n_candidates):\n",
    "            case_idx = int(ci_cand[i])\n",
    "            local_idx = available_cases.index(case_idx)\n",
    "            x, y, z = int(xs_cand[i]), int(ys_cand[i]), int(zs_cand[i])\n",
    "            \n",
    "            coords_cand.append([float(x), float(y), float(z)])\n",
    "            intens_cand.append(cache.cache[local_idx]['mods'][:, x, y, z])\n",
    "            labels_cand.append(int(cache.cache[local_idx]['seg'][x, y, z]))\n",
    "            boundary_cand.append(float(cache.boundary_dists[local_idx][x, y, z]))\n",
    "        \n",
    "        coords_cand = np.array(coords_cand, dtype=np.float32)\n",
    "        intens_cand = np.stack(intens_cand, axis=0)\n",
    "        labels_cand = np.array(labels_cand, dtype=np.int32)\n",
    "        boundary_cand = np.array(boundary_cand, dtype=np.float32)\n",
    "        \n",
    "        # Normalize coords for uncertainty computation\n",
    "        norm_coords_cand = (coords_cand / np.array([H-1, W-1, D-1], dtype=np.float32)) * 2.0 - 1.0\n",
    "        \n",
    "        # Compute uncertainty\n",
    "        rng_key, dropout_key = jax.random.split(rng_key)\n",
    "        uncertainty = compute_uncertainty_mc_dropout(\n",
    "            params['mlp'],\n",
    "            jnp.array(norm_coords_cand),\n",
    "            jnp.array(intens_cand),\n",
    "            params['fourier'],\n",
    "            n_samples=5,\n",
    "            dropout_key=dropout_key\n",
    "        )\n",
    "        \n",
    "        # Select top uncertain points\n",
    "        uncertainty_np = np.array(uncertainty)\n",
    "        top_indices = np.argsort(uncertainty_np)[-n_uncertainty:]\n",
    "        \n",
    "        all_coords.append(coords_cand[top_indices])\n",
    "        all_intens.append(intens_cand[top_indices])\n",
    "        all_labels.append(labels_cand[top_indices])\n",
    "        all_boundary_weights.append(boundary_cand[top_indices])\n",
    "    \n",
    "    # Concatenate and shuffle\n",
    "    coords = np.concatenate(all_coords, axis=0)\n",
    "    intens = np.concatenate(all_intens, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    boundary_weights = np.concatenate(all_boundary_weights, axis=0)\n",
    "    \n",
    "    indices = np.random.permutation(len(coords))\n",
    "    coords = coords[indices]\n",
    "    intens = intens[indices]\n",
    "    labels = labels[indices]\n",
    "    boundary_weights = boundary_weights[indices]\n",
    "    \n",
    "    # Normalize coordinates\n",
    "    norm_coords = (coords / np.array([H-1, W-1, D-1], dtype=np.float32)) * 2.0 - 1.0\n",
    "    \n",
    "    return jnp.array(norm_coords), jnp.array(intens), jnp.array(labels), jnp.array(boundary_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Multi-Metric Evaluation (Research-Backed)\n",
    "# =============================================================================\n",
    "\n",
    "def predict_full_volume(params, case_data, chunk_size=50000):\n",
    "    \"\"\"Predict full volume segmentation\"\"\"\n",
    "    mods = case_data['mods']\n",
    "    seg_true = case_data['seg']\n",
    "    M_vol, H_vol, W_vol, D_vol = mods.shape\n",
    "    \n",
    "    # Create coordinate grid\n",
    "    xs, ys, zs = np.arange(H_vol), np.arange(W_vol), np.arange(D_vol)\n",
    "    grid = np.stack(np.meshgrid(xs, ys, zs, indexing='ij'), axis=-1).reshape(-1, 3)\n",
    "    intens = mods.transpose(1, 2, 3, 0).reshape(-1, M_vol)\n",
    "    \n",
    "    # Normalize coords\n",
    "    norm_coords = (grid / np.array([H_vol-1, W_vol-1, D_vol-1])) * 2.0 - 1.0\n",
    "    \n",
    "    # Predict in chunks\n",
    "    preds = []\n",
    "    for i in range(0, len(grid), chunk_size):\n",
    "        coords_chunk = jnp.array(norm_coords[i:i+chunk_size])\n",
    "        intens_chunk = jnp.array(intens[i:i+chunk_size])\n",
    "        \n",
    "        fourier_feats = apply_fourier_features(params['fourier'], coords_chunk)\n",
    "        logits = apply_coord_injection_mlp(\n",
    "            params['mlp'],\n",
    "            coords_chunk,\n",
    "            intens_chunk,\n",
    "            fourier_feats,\n",
    "            training=False\n",
    "        )\n",
    "        pred_chunk = jnp.argmax(logits, axis=-1)\n",
    "        preds.append(np.array(pred_chunk, dtype=np.int16))\n",
    "    \n",
    "    pred_flat = np.concatenate(preds, axis=0)\n",
    "    pred_vol = pred_flat.reshape(H_vol, W_vol, D_vol)\n",
    "    \n",
    "    return pred_vol, seg_true\n",
    "\n",
    "def compute_multi_metrics(pred, true, num_classes=4):\n",
    "    \"\"\"\n",
    "    Compute multiple metrics:\n",
    "    - Dice per class\n",
    "    - Connected components count\n",
    "    - Slice-to-slice consistency\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Dice per class\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (pred == c)\n",
    "        true_c = (true == c)\n",
    "        intersection = (pred_c & true_c).sum()\n",
    "        union = pred_c.sum() + true_c.sum()\n",
    "        dice = (2 * intersection + 1e-6) / (union + 1e-6) if union > 0 else 0.0\n",
    "        metrics[f'dice_class_{c}'] = dice\n",
    "    \n",
    "    # Connected components (tumor classes only)\n",
    "    for c in range(1, num_classes):\n",
    "        mask = (pred == c).astype(np.uint8)\n",
    "        if mask.sum() > 0:\n",
    "            labeled, n_components = connected_components_3d(mask)\n",
    "            metrics[f'n_components_class_{c}'] = n_components\n",
    "        else:\n",
    "            metrics[f'n_components_class_{c}'] = 0\n",
    "    \n",
    "    # Slice-to-slice Dice consistency (middle axis)\n",
    "    slice_dices = []\n",
    "    for z in range(1, pred.shape[2]):\n",
    "        pred_z = pred[:, :, z]\n",
    "        pred_z_prev = pred[:, :, z-1]\n",
    "        \n",
    "        intersection = ((pred_z == pred_z_prev) & (pred_z > 0)).sum()\n",
    "        union = ((pred_z > 0) | (pred_z_prev > 0)).sum()\n",
    "        \n",
    "        if union > 0:\n",
    "            slice_dice = (2 * intersection) / (union + (pred_z > 0).sum() + (pred_z_prev > 0).sum() + 1e-6)\n",
    "            slice_dices.append(slice_dice)\n",
    "    \n",
    "    metrics['slice_consistency'] = np.mean(slice_dices) if slice_dices else 0.0\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Optimizer with Two-Stage Schedule\n",
    "# =============================================================================\n",
    "\n",
    "def create_two_stage_schedule(stage1_steps, stage2_steps, lr_stage1, lr_stage2, warmup_steps, min_lr):\n",
    "    \"\"\"Two-stage learning rate schedule\"\"\"\n",
    "    def schedule(step):\n",
    "        # Warmup\n",
    "        if step < warmup_steps:\n",
    "            return lr_stage1 * (step / warmup_steps)\n",
    "        \n",
    "        # Stage 1: Higher LR for coarse learning\n",
    "        if step < stage1_steps:\n",
    "            progress = (step - warmup_steps) / (stage1_steps - warmup_steps)\n",
    "            return lr_stage1 * (1.0 - progress) + lr_stage2 * progress\n",
    "        \n",
    "        # Stage 2: Lower LR for refinement\n",
    "        progress = (step - stage1_steps) / stage2_steps\n",
    "        return lr_stage2 * (1.0 - progress) + min_lr * progress\n",
    "    \n",
    "    return schedule\n",
    "\n",
    "schedule_fn = create_two_stage_schedule(\n",
    "    STAGE1_STEPS,\n",
    "    STAGE2_STEPS,\n",
    "    LR_STAGE1,\n",
    "    LR_STAGE2,\n",
    "    WARMUP_STEPS,\n",
    "    MIN_LR\n",
    ")\n",
    "\n",
    "optimizer = optax.chain(\n",
    "    optax.clip_by_global_norm(CLIP_NORM),\n",
    "    optax.adamw(learning_rate=schedule_fn, weight_decay=WEIGHT_DECAY)\n",
    ")\n",
    "\n",
    "opt_state = optimizer.init(params)\n",
    "print(f\"✓ Two-stage optimizer initialized\")\n",
    "\n",
    "loss_and_grad = jax.jit(jax.value_and_grad(loss_fn, has_aux=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Step with Stochastic Preconditioning\n",
    "# =============================================================================\n",
    "\n",
    "def add_coordinate_noise(coords, sigma, rng_key):\n",
    "    \"\"\"Stochastic preconditioning: add Gaussian noise to coordinates\"\"\"\n",
    "    noise = jax.random.normal(rng_key, coords.shape) * sigma\n",
    "    return coords + noise\n",
    "\n",
    "def microbatch_step(params, opt_state, rng_key, cache, step, use_uncertainty=True):\n",
    "    \"\"\"Single training step with gradient accumulation\"\"\"\n",
    "    grads_acc = jax.tree.map(lambda x: jnp.zeros_like(x), params)\n",
    "    loss_acc = 0.0\n",
    "    aux_acc = jax.tree.map(lambda x: 0.0, loss_fn(params, \n",
    "                                                   jnp.zeros((1, 3)),\n",
    "                                                   jnp.zeros((1, M)),\n",
    "                                                   jnp.zeros(1, dtype=jnp.int32),\n",
    "                                                   jnp.zeros(1),\n",
    "                                                   training=False)[1])\n",
    "    \n",
    "    # Determine TV weight based on stage\n",
    "    tv_weight = TV_WEIGHT_STAGE1 if step < STAGE1_STEPS else TV_WEIGHT_STAGE2\n",
    "    \n",
    "    # Determine coordinate noise (decays over training)\n",
    "    progress = step / TOTAL_STEPS\n",
    "    coord_sigma = COORD_NOISE_SIGMA_INIT * (1.0 - progress) + COORD_NOISE_SIGMA_FINAL * progress\n",
    "    \n",
    "    # Determine uncertainty ratio (increases in stage 2)\n",
    "    uncertainty_ratio = UNCERTAINTY_RATIO if step < STAGE1_STEPS else 0.75\n",
    "    \n",
    "    # Only use dropout after warmup (disable during early training)\n",
    "    use_dropout = (step > WARMUP_STEPS)\n",
    "    \n",
    "    key = rng_key\n",
    "    for _ in range(ACCUM_STEPS):\n",
    "        key, sample_key, noise_key, dropout_key = jax.random.split(key, 4)\n",
    "        \n",
    "        coords, feats, labels, boundary_weights = sample_batch_hybrid(\n",
    "            sample_key,\n",
    "            MICRO_BATCH_SIZE,\n",
    "            cache,\n",
    "            params,\n",
    "            params['fourier'],\n",
    "            uncertainty_ratio=uncertainty_ratio,\n",
    "            balanced_ratio=BALANCED_RATIO,\n",
    "            uniform_ratio=UNIFORM_RATIO,\n",
    "            use_uncertainty=use_uncertainty\n",
    "        )\n",
    "        \n",
    "        # Apply stochastic preconditioning\n",
    "        coords = add_coordinate_noise(coords, coord_sigma, noise_key)\n",
    "        \n",
    "        (loss_val, aux), grads = loss_and_grad(\n",
    "            params,\n",
    "            coords,\n",
    "            feats,\n",
    "            labels,\n",
    "            boundary_weights,\n",
    "            training=use_dropout,  # Only train with dropout after warmup\n",
    "            dropout_key=dropout_key if use_dropout else None,\n",
    "            tv_weight=tv_weight\n",
    "        )\n",
    "        \n",
    "        loss_acc += float(loss_val)\n",
    "        grads_acc = jax.tree.map(lambda acc, g: acc + g, grads_acc, grads)\n",
    "        aux_acc = jax.tree.map(lambda acc, a: acc + a, aux_acc, aux)\n",
    "    \n",
    "    # Average gradients\n",
    "    grads_mean = jax.tree.map(lambda g: g / ACCUM_STEPS, grads_acc)\n",
    "    updates, opt_state = optimizer.update(grads_mean, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    \n",
    "    # Average aux\n",
    "    aux_mean = jax.tree.map(lambda a: a / ACCUM_STEPS, aux_acc)\n",
    "    \n",
    "    return params, opt_state, loss_acc / ACCUM_STEPS, aux_mean\n",
    "\n",
    "# Warm-up\n",
    "print(\"Running warm-up...\")\n",
    "jax_key, warm_key = jax.random.split(jax_key)\n",
    "params, opt_state, warm_loss, warm_aux = microbatch_step(\n",
    "    params,\n",
    "    opt_state,\n",
    "    warm_key,\n",
    "    train_cache,\n",
    "    step=0,\n",
    "    use_uncertainty=False  # No uncertainty in warm-up\n",
    ")\n",
    "print(f'Warm-up loss: {warm_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training Loop with Multi-Metric Evaluation\n",
    "# =============================================================================\n",
    "\n",
    "loss_history = []\n",
    "dice_history = [[] for _ in range(NUM_CLASSES)]\n",
    "dice_tumor_history = []\n",
    "ufl_history = []\n",
    "\n",
    "start = time.time()\n",
    "mid_z = D // 2\n",
    "\n",
    "CLASS_LABELS = {0: 'Background', 1: 'NCR/NET', 2: 'ED', 3: 'ET'}\n",
    "\n",
    "# Visualization setup\n",
    "vis_cache = val_cache if val_cache else train_cache\n",
    "VIS_CASE_INDEX = 0\n",
    "true_slice = vis_cache.cache[VIS_CASE_INDEX]['seg'][:, :, mid_z]\n",
    "mod0_slice = vis_cache.cache[VIS_CASE_INDEX]['mods'][0, :, :, mid_z]\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Starting Two-Stage Training - {TOTAL_STEPS} steps\")\n",
    "print(f\"Stage 1 (Coarse): Steps 1-{STAGE1_STEPS}\")\n",
    "print(f\"Stage 2 (Refine): Steps {STAGE1_STEPS+1}-{TOTAL_STEPS}\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "best_val_dice = 0.0\n",
    "\n",
    "for step in range(1, TOTAL_STEPS + 1):\n",
    "    # Rotate chunks\n",
    "    if step % STEPS_PER_CHUNK == 0 and step > 0:\n",
    "        print(f\"\\n--- Rotating to next data chunk at step {step} ---\")\n",
    "        train_cache.next_chunk()\n",
    "        if val_cache:\n",
    "            val_cache.next_chunk()\n",
    "            # Update vis\n",
    "            true_slice = val_cache.cache[VIS_CASE_INDEX]['seg'][:, :, mid_z]\n",
    "            mod0_slice = val_cache.cache[VIS_CASE_INDEX]['mods'][0, :, :, mid_z]\n",
    "    \n",
    "    # Training step\n",
    "    use_uncertainty = (step > WARMUP_STEPS)  # Enable uncertainty after warmup\n",
    "    \n",
    "    jax_key, step_key = jax.random.split(jax_key)\n",
    "    params, opt_state, loss_val, aux = microbatch_step(\n",
    "        params,\n",
    "        opt_state,\n",
    "        step_key,\n",
    "        train_cache,\n",
    "        step,\n",
    "        use_uncertainty=use_uncertainty\n",
    "    )\n",
    "    \n",
    "    # Track metrics\n",
    "    loss_history.append(float(loss_val))\n",
    "    ufl_history.append(float(aux['ufl']))\n",
    "    dice_k = aux['dice_per_class']\n",
    "    dice_tumor_history.append(float(aux['dice_mean_tumor']))\n",
    "    \n",
    "    for k in range(NUM_CLASSES):\n",
    "        dice_history[k].append(float(dice_k[k]))\n",
    "    \n",
    "    # W&B logging\n",
    "    current_lr = schedule_fn(step)\n",
    "    stage = \"Stage1_Coarse\" if step <= STAGE1_STEPS else \"Stage2_Refine\"\n",
    "    \n",
    "    wandb_metrics = {\n",
    "        \"train/loss\": float(loss_val),\n",
    "        \"train/ufl\": float(aux['ufl']),\n",
    "        \"train/mFTL\": float(aux['mFTL']),\n",
    "        \"train/mFL\": float(aux['mFL']),\n",
    "        \"train/dice_loss\": float(aux['dice_loss']),\n",
    "        \"train/tv_loss\": float(aux['tv_loss']),\n",
    "        \"train/boundary_loss\": float(aux['boundary_loss']),\n",
    "        \"train/dice_mean_tumor\": float(aux['dice_mean_tumor']),\n",
    "        \"train/accuracy\": float(aux['accuracy']),\n",
    "        \"train/lr\": float(current_lr),\n",
    "        \"train/stage\": 1 if step <= STAGE1_STEPS else 2,\n",
    "    }\n",
    "    \n",
    "    for k in range(NUM_CLASSES):\n",
    "        wandb_metrics[f\"train/dice_class_{k}_{CLASS_LABELS[k]}\"] = float(dice_k[k])\n",
    "    \n",
    "    wandb.log(wandb_metrics, step=step)\n",
    "    \n",
    "    # Visualization every 100 steps\n",
    "    if step % 10 == 0 or step == 1:\n",
    "        # Predict slice\n",
    "        slice_coords = []\n",
    "        slice_feats = []\n",
    "        \n",
    "        for x in range(H):\n",
    "            for y in range(W):\n",
    "                slice_coords.append([x, y, mid_z])\n",
    "                slice_feats.append(vis_cache.cache[VIS_CASE_INDEX]['mods'][:, x, y, mid_z])\n",
    "        \n",
    "        slice_coords = np.array(slice_coords, dtype=np.float32)\n",
    "        slice_feats = np.stack(slice_feats, axis=0)\n",
    "        norm_slice_coords = (slice_coords / np.array([H-1, W-1, D-1])) * 2.0 - 1.0\n",
    "        \n",
    "        fourier_feats = apply_fourier_features(params['fourier'], jnp.array(norm_slice_coords))\n",
    "        logits = apply_coord_injection_mlp(\n",
    "            params['mlp'],\n",
    "            jnp.array(norm_slice_coords),\n",
    "            jnp.array(slice_feats),\n",
    "            fourier_feats,\n",
    "            training=False\n",
    "        )\n",
    "        pred_slice = np.array(jnp.argmax(logits, axis=-1)).reshape(H, W)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        # Loss curves\n",
    "        ax0 = axes[0]\n",
    "        ax0.plot(loss_history, 'b-', label='Total Loss', alpha=0.7, linewidth=1)\n",
    "        ax0.plot(ufl_history, 'r--', label='UFL', alpha=0.5, linewidth=1)\n",
    "        ax0.axvline(STAGE1_STEPS, color='green', linestyle='--', alpha=0.5, label='Stage 2 Start')\n",
    "        ax0.set_title('Training Loss', fontweight='bold')\n",
    "        ax0.set_xlabel('Step')\n",
    "        ax0.set_ylabel('Loss')\n",
    "        ax0.legend()\n",
    "        ax0.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Per-class Dice\n",
    "        ax1 = axes[0].twinx()\n",
    "        colors = plt.cm.tab10.colors\n",
    "        for k in range(NUM_CLASSES):\n",
    "            label = CLASS_LABELS.get(k, f'c{k}')\n",
    "            ax1.plot(dice_history[k], label=f'{label}',\n",
    "                    color=colors[k], linewidth=2, alpha=0.7)\n",
    "        ax1.plot(dice_tumor_history, label='Tumor Avg',\n",
    "                color='black', linewidth=3, linestyle='--')\n",
    "        ax1.set_ylabel('Dice Score')\n",
    "        ax1.legend(loc='lower right')\n",
    "        ax1.set_ylim([0, 1])\n",
    "        \n",
    "        # Ground truth\n",
    "        axes[1].imshow(mod0_slice, cmap='gray')\n",
    "        axes[1].imshow(true_slice, alpha=0.4, cmap='tab10', vmin=0, vmax=3)\n",
    "        axes[1].set_title('Ground Truth', fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Prediction\n",
    "        axes[2].imshow(mod0_slice, cmap='gray')\n",
    "        axes[2].imshow(pred_slice, alpha=0.4, cmap='tab10', vmin=0, vmax=3)\n",
    "        axes[2].set_title(f'Prediction (Step {step}) - {stage}', fontweight='bold')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        dice_tumor = aux['dice_mean_tumor']\n",
    "        fig.suptitle(\n",
    "            f'Step {step}/{TOTAL_STEPS} | {stage} | Loss={loss_val:.4f} | '\n",
    "            f'Dice(tumor)={dice_tumor:.3f} | LR={current_lr:.2e}',\n",
    "            fontsize=14, fontweight='bold'\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        \n",
    "        if step % 500 == 0 or step == 1:\n",
    "            wandb.log({\"train/predictions\": wandb.Image(fig)}, step=step)\n",
    "        \n",
    "        plt.close(fig)\n",
    "    \n",
    "    # Full validation evaluation every VAL_EVAL_STEPS\n",
    "    if val_cache and (step % VAL_EVAL_STEPS == 0 or step == TOTAL_STEPS):\n",
    "        print(f\"\\n--- Running full validation at step {step} ---\")\n",
    "        \n",
    "        n_val_cases = min(5, len(val_cache.cache))\n",
    "        val_metrics_all = []\n",
    "        \n",
    "        for val_idx in range(n_val_cases):\n",
    "            pred_vol, true_vol = predict_full_volume(params, val_cache.cache[val_idx], chunk_size=80000)\n",
    "            metrics = compute_multi_metrics(pred_vol, true_vol, NUM_CLASSES)\n",
    "            val_metrics_all.append(metrics)\n",
    "        \n",
    "        # Aggregate\n",
    "        val_dice_tumor = np.mean([m['dice_class_1'] + m['dice_class_2'] + m['dice_class_3'] \n",
    "                                   for m in val_metrics_all]) / 3\n",
    "        \n",
    "        print(f\"Validation Dice (tumor avg): {val_dice_tumor:.4f}\")\n",
    "        \n",
    "        # Log to W&B\n",
    "        val_wandb = {\n",
    "            \"val/dice_tumor_avg\": val_dice_tumor,\n",
    "        }\n",
    "        \n",
    "        for k in range(NUM_CLASSES):\n",
    "            dice_k = np.mean([m[f'dice_class_{k}'] for m in val_metrics_all])\n",
    "            val_wandb[f\"val/dice_class_{k}\"] = dice_k\n",
    "            print(f\"  Class {k} ({CLASS_LABELS[k]}): {dice_k:.4f}\")\n",
    "        \n",
    "        for k in range(1, NUM_CLASSES):\n",
    "            n_comp = np.mean([m[f'n_components_class_{k}'] for m in val_metrics_all])\n",
    "            val_wandb[f\"val/n_components_class_{k}\"] = n_comp\n",
    "            print(f\"  Components class {k}: {n_comp:.1f}\")\n",
    "        \n",
    "        slice_cons = np.mean([m['slice_consistency'] for m in val_metrics_all])\n",
    "        val_wandb[\"val/slice_consistency\"] = slice_cons\n",
    "        print(f\"  Slice consistency: {slice_cons:.4f}\")\n",
    "        \n",
    "        wandb.log(val_wandb, step=step)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_dice_tumor > best_val_dice:\n",
    "            best_val_dice = val_dice_tumor\n",
    "            print(f\"  ✓ New best validation Dice: {best_val_dice:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            flat_params = {}\n",
    "            flat_params['fourier_B'] = np.array(params['fourier']['B'])\n",
    "            for i, layer in enumerate(params['mlp']):\n",
    "                flat_params[f'mlp_W_{i}'] = np.array(layer['W'])\n",
    "                flat_params[f'mlp_b_{i}'] = np.array(layer['b'])\n",
    "            \n",
    "            SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "            np.savez_compressed(SAVE_PATH / f\"best_model_dice{best_val_dice:.4f}.npz\", **flat_params)\n",
    "            \n",
    "            wandb.run.summary[\"best_val_dice_tumor\"] = best_val_dice\n",
    "    \n",
    "    # Console progress\n",
    "    if step % 1 == 0 or step == 1:\n",
    "        elapsed = time.time() - start\n",
    "        steps_per_sec = step / elapsed\n",
    "        eta_min = (TOTAL_STEPS - step) / steps_per_sec / 60\n",
    "        \n",
    "        print(f\"{stage} | Step {step}/{TOTAL_STEPS} | \"\n",
    "              f\"Loss={loss_val:.4f} | \"\n",
    "              f\"Dice(tumor)={aux['dice_mean_tumor']:.3f} | \"\n",
    "              f\"LR={current_lr:.2e} | \"\n",
    "              f\"{steps_per_sec:.2f} steps/s | \"\n",
    "              f\"ETA: {eta_min:.1f}min\")\n",
    "\n",
    "training_time = time.time() - start\n",
    "print(f'\\n✓ Training complete: {training_time/60:.2f} minutes')\n",
    "wandb.run.summary[\"training_time_minutes\"] = training_time / 60\n",
    "wandb.run.summary[\"final_dice_tumor\"] = dice_tumor_history[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Final Model\n",
    "# =============================================================================\n",
    "flat_params = {}\n",
    "flat_params['fourier_B'] = np.array(params['fourier']['B'])\n",
    "for i, layer in enumerate(params['mlp']):\n",
    "    flat_params[f'mlp_W_{i}'] = np.array(layer['W'])\n",
    "    flat_params[f'mlp_b_{i}'] = np.array(layer['b'])\n",
    "\n",
    "SAVE_PATH.mkdir(parents=True, exist_ok=True)\n",
    "np.savez_compressed(SAVE_PATH / \"model_final.npz\", **flat_params)\n",
    "print(f'\\n✓ Saved final model to {SAVE_PATH}')\n",
    "\n",
    "wandb.finish()\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Training Complete!\")\n",
    "print(f\"Best Validation Dice (Tumor): {best_val_dice:.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
