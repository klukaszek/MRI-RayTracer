{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnU-Net v2: Train and Export Weights (BraTS23 example)\n",
    "\n",
    "This notebook prepares a dataset in nnU-Net format, runs planning + preprocessing, trains a model, and exports\n",
    "the trained weights to a zip. You can then use the checkpoint for INR distillation.\n",
    "\n",
    "Notes:\n",
    "- Ensure this kernel uses the project virtual environment.\n",
    "- Set the GPU/compute configuration as needed. On macOS without CUDA, nnU-Net will use MPS if available.\n",
    "- BraTS23 labels are 0,1,2,4. We remap 4→3 to make labels continuous.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BraTS-2023 naming fix\n",
    "\n",
    "Your BraTS-2023 files appear named as `-t1n`, `-t1c`, `-t2f`, `-t2w`, and `-seg` (e.g., `BraTS-GLI-XXXXX-XXX-t1n.nii.gz`).\n",
    "This cell overrides the conversion utilities to use those suffixes and re-generates `Dataset900_BraTS2023`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and path setup: set nnU-Net directories BEFORE importing nnunetv2\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent  # assumes notebook is run from 'notebooks' directory\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "RAW_DIR = DATA_DIR / 'nnUNet_raw'\n",
    "PP_DIR = DATA_DIR / 'nnUNet_preprocessed'\n",
    "RES_DIR = DATA_DIR / 'nnUNet_results'\n",
    "ARTIFACTS_DIR = PROJECT_ROOT / 'artifacts'\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ['nnUNet_raw'] = str(RAW_DIR)\n",
    "os.environ['nnUNet_preprocessed'] = str(PP_DIR)\n",
    "os.environ['nnUNet_results'] = str(RES_DIR)\n",
    "\n",
    "# Prefer local nnUNet sources if present (so docs & APIs line up)\n",
    "LOCAL_NNUNET = PROJECT_ROOT / 'nnUNet'\n",
    "if LOCAL_NNUNET.exists():\n",
    "    sys.path.insert(0, str(LOCAL_NNUNET))\n",
    "    print('Using local nnUNet source:', LOCAL_NNUNET)\n",
    "else:\n",
    "    print('Using installed nnunetv2 package')\n",
    "\n",
    "print('nnUNet_raw       =', os.environ['nnUNet_raw'])\n",
    "print('nnUNet_preprocessed =', os.environ['nnUNet_preprocessed'])\n",
    "print('nnUNet_results   =', os.environ['nnUNet_results'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert BraTS23 to nnU-Net format\n",
    "\n",
    "We search in `data/BraTS-2023` for training cases and construct an nnU-Net dataset:\n",
    "- Dataset ID/Name: `Dataset900_BraTS2023` (custom)\n",
    "- Channels: FLAIR, T1w, T1ce, T2w → `_0000.._0003`\n",
    "- Labels: remap 4 → 3 to ensure continuous labels 0..3\n",
    "- Writes `dataset.json`, `imagesTr/`, `labelsTr/`\n",
    "\n",
    "If your raw dataset is already in nnU-Net format, skip this cell and set `DATASET_ID` and `DATASET_NAME` accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# Configure dataset source and target\n",
    "BRATS23_ROOT = DATA_DIR / 'BraTS-2023'  # adjust if your location is different\n",
    "DATASET_ID = 900\n",
    "DATASET_NAME = f'Dataset{DATASET_ID:03d}_BraTS2023'\n",
    "DS_RAW = RAW_DIR / DATASET_NAME\n",
    "IMAGES_TR = DS_RAW / 'imagesTr'\n",
    "LABELS_TR = DS_RAW / 'labelsTr'\n",
    "\n",
    "IMAGES_TR.mkdir(parents=True, exist_ok=True)\n",
    "LABELS_TR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def find_cases_brats23(root: Path):\n",
    "    # Accept common patterns; adjust if your structure differs\n",
    "    case_dirs = []\n",
    "    if (root / 'BraTS-GLI-00000-000').exists():\n",
    "        # flat dir with cases\n",
    "        case_dirs = sorted([p for p in root.iterdir() if p.is_dir() and p.name.startswith('BraTS-')])\n",
    "    else:\n",
    "        # recursive\n",
    "        case_dirs = sorted([Path(p).parent for p in glob(str(root / '**/*_flair.nii*'), recursive=True)])\n",
    "    return case_dirs\n",
    "\n",
    "def has_all_modalities(case_dir: Path):\n",
    "    flair = list(case_dir.glob('*_flair.nii*'))\n",
    "    t1    = list(case_dir.glob('*_t1.nii*'))\n",
    "    t1ce  = list(case_dir.glob('*_t1ce.nii*'))\n",
    "    t2    = list(case_dir.glob('*_t2.nii*'))\n",
    "    seg   = list(case_dir.glob('*_seg.nii*'))\n",
    "    return len(flair)==1 and len(t1)==1 and len(t1ce)==1 and len(t2)==1 and len(seg)==1\n",
    "\n",
    "def case_id_from_dir(case_dir: Path):\n",
    "    # Use folder name as case_id, strip non-alnum for safety\n",
    "    return case_dir.name\n",
    "\n",
    "def remap_segmentation_to_continuous(src_seg: Path, dst_seg: Path):\n",
    "    img = nib.load(str(src_seg))\n",
    "    data = img.get_fdata().astype(np.int16)\n",
    "    # BraTS labels are typically 0,1,2,4 -> map 4->3\n",
    "    if (data==4).any():\n",
    "        data[data==4] = 3\n",
    "    # ensure int dtype\n",
    "    out = nib.Nifti1Image(data.astype(np.int16), img.affine, img.header)\n",
    "    nib.save(out, str(dst_seg))\n",
    "\n",
    "def dataset_exists(ds_raw: Path) -> bool:\n",
    "    if not ds_raw.exists():\n",
    "        return False\n",
    "    ds_json = ds_raw / 'dataset.json'\n",
    "    images_tr = ds_raw / 'imagesTr'\n",
    "    labels_tr = ds_raw / 'labelsTr'\n",
    "    if not (ds_json.exists() and images_tr.exists() and labels_tr.exists()):\n",
    "        return False\n",
    "    imgs = list(images_tr.glob('*.nii*'))\n",
    "    labs = list(labels_tr.glob('*.nii*'))\n",
    "    # Expect at least one label and roughly 4x images for 4 modalities\n",
    "    if len(labs) == 0:\n",
    "        return False\n",
    "    if len(imgs) < 4 * len(labs):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def prepare_brats23_dataset():\n",
    "    # Early exit if dataset already prepared\n",
    "    if dataset_exists(DS_RAW):\n",
    "        print('Dataset already exists at:', DS_RAW)\n",
    "        print('Skipping creation/filtering.')\n",
    "        return\n",
    "\n",
    "    cases = []\n",
    "    for cdir in find_cases_brats23(BRATS23_ROOT):\n",
    "        if not has_all_modalities(cdir):\n",
    "            continue\n",
    "        cid = case_id_from_dir(cdir)\n",
    "        cases.append((cid, cdir))\n",
    "    print(f'Found {len(cases)} cases')\n",
    "\n",
    "    for cid, cdir in cases:\n",
    "        # channel order: FLAIR, T1, T1ce, T2\n",
    "        mapping = {\n",
    "            '_flair': 0,\n",
    "            '_t1': 1,\n",
    "            '_t1ce': 2,\n",
    "            '_t2': 3,\n",
    "        }\n",
    "        for suffix, ch in mapping.items():\n",
    "            src = list(cdir.glob(f'*{suffix}.nii*'))[0]\n",
    "            dst = IMAGES_TR / f'{cid}_{ch:04d}.nii.gz'\n",
    "            if not dst.exists():\n",
    "                # Use symlink to save space; fallback to copy if needed\n",
    "                try:\n",
    "                    os.symlink(src, dst)\n",
    "                except Exception:\n",
    "                    shutil.copy2(src, dst)\n",
    "        # labels\n",
    "        src_seg = list(cdir.glob('*_seg.nii*'))[0]\n",
    "        dst_seg = LABELS_TR / f'{cid}.nii.gz'\n",
    "        if not dst_seg.exists():\n",
    "            remap_segmentation_to_continuous(src_seg, dst_seg)\n",
    "\n",
    "    # dataset.json\n",
    "    dataset = {\n",
    "        'name': 'BraTS2023',\n",
    "        'description': 'BraTS 2023 converted to nnU-Net format',\n",
    "        'reference': 'https://www.synapse.org/#!Synapse:syn51156910/wiki/',\n",
    "        'licence': 'see original dataset licence',\n",
    "        'release': '1.0',\n",
    "        'modality': {\n",
    "            '0': 'FLAIR', '1': 'T1w', '2': 'T1gd', '3': 'T2w'\n",
    "        },\n",
    "        'labels': {\n",
    "            'background': 0, 'edema': 1, 'non_enhancing': 2, 'enhancing': 3\n",
    "        },\n",
    "        'numTraining': len(list(LABELS_TR.glob('*.nii*'))),\n",
    "        'file_ending': '.nii.gz'\n",
    "    }\n",
    "    with open(DS_RAW / 'dataset.json', 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    print('Wrote', DS_RAW / 'dataset.json')\n",
    "\n",
    "prepare_brats23_dataset()\n",
    "print('Raw dataset prepared at:', DS_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82c0730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override helpers for BraTS-2023 filename scheme (-t1n, -t1c, -t2f, -t2w, -seg)\n",
    "import os, shutil, json\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "def find_cases_brats23(root: Path):\n",
    "    # Flat directories named BraTS-GLI-... exist in your tree\n",
    "    case_dirs = sorted([p for p in root.iterdir() if p.is_dir() and p.name.startswith('BraTS-')])\n",
    "    if not case_dirs:\n",
    "        # fallback to recursive search\n",
    "        case_dirs = sorted([Path(p).parent for p in glob(str(root / '**/*-t1n.nii*'), recursive=True)])\n",
    "    return case_dirs\n",
    "\n",
    "def has_all_modalities(case_dir: Path):\n",
    "    t2f = list(case_dir.glob('*-t2f.nii*'))  # FLAIR\n",
    "    t1n = list(case_dir.glob('*-t1n.nii*'))  # T1 native\n",
    "    t1c = list(case_dir.glob('*-t1c.nii*'))  # T1 contrast\n",
    "    t2w = list(case_dir.glob('*-t2w.nii*'))  # T2 weighted\n",
    "    seg = list(case_dir.glob('*-seg.nii*'))\n",
    "    return len(t2f)==1 and len(t1n)==1 and len(t1c)==1 and len(t2w)==1 and len(seg)==1\n",
    "\n",
    "def case_id_from_dir(case_dir: Path):\n",
    "    return case_dir.name\n",
    "\n",
    "def remap_segmentation_to_continuous(src_seg: Path, dst_seg: Path):\n",
    "    img = nib.load(str(src_seg))\n",
    "    data = img.get_fdata().astype(np.int16)\n",
    "    if (data==4).any():\n",
    "        data[data==4] = 3\n",
    "    out = nib.Nifti1Image(data.astype(np.int16), img.affine, img.header)\n",
    "    nib.save(out, str(dst_seg))\n",
    "\n",
    "def prepare_brats23_dataset():\n",
    "    cases = []\n",
    "    for cdir in find_cases_brats23(BRATS23_ROOT):\n",
    "        if not has_all_modalities(cdir):\n",
    "            continue\n",
    "        cid = case_id_from_dir(cdir)\n",
    "        cases.append((cid, cdir))\n",
    "    print(f'Found {len(cases)} cases')\n",
    "    if not cases:\n",
    "        # help debug\n",
    "        print('No cases found. Check BRATS23_ROOT:', BRATS23_ROOT)\n",
    "        print('Sample entries under root:')\n",
    "        for p in list(BRATS23_ROOT.iterdir())[:5]:\n",
    "            print(' ', p)\n",
    "        return\n",
    "\n",
    "    # ensure clean imagesTr/labelsTr exist\n",
    "    if IMAGES_TR.exists():\n",
    "        shutil.rmtree(IMAGES_TR)\n",
    "    if LABELS_TR.exists():\n",
    "        shutil.rmtree(LABELS_TR)\n",
    "    IMAGES_TR.mkdir(parents=True, exist_ok=True)\n",
    "    LABELS_TR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # mapping: channel index -> suffix\n",
    "    mapping = {0: '-t2f', 1: '-t1n', 2: '-t1c', 3: '-t2w'}\n",
    "\n",
    "    for cid, cdir in cases:\n",
    "        for ch, suffix in mapping.items():\n",
    "            src_list = list(cdir.glob(f'*{suffix}.nii*'))\n",
    "            assert len(src_list)==1, (cid, suffix, src_list)\n",
    "            src = src_list[0]\n",
    "            dst = IMAGES_TR / f'{cid}_{ch:04d}.nii.gz'\n",
    "            if not dst.exists():\n",
    "                try:\n",
    "                    os.symlink(src, dst)\n",
    "                except Exception:\n",
    "                    shutil.copy2(src, dst)\n",
    "        # labels\n",
    "        src_seg = list(cdir.glob('*-seg.nii*'))[0]\n",
    "        dst_seg = LABELS_TR / f'{cid}.nii.gz'\n",
    "        remap_segmentation_to_continuous(src_seg, dst_seg)\n",
    "\n",
    "    dataset = {\n",
    "        'name': 'BraTS2023',\n",
    "        'description': 'BraTS 2023 converted to nnU-Net format',\n",
    "        'reference': 'https://www.synapse.org/#!Synapse:syn51156910/wiki/',\n",
    "        'licence': 'see original dataset licence',\n",
    "        'release': '1.0',\n",
    "        'modality': {'0': 'FLAIR', '1': 'T1w', '2': 'T1gd', '3': 'T2w'},\n",
    "        'labels': {'background': 0, 'edema': 1, 'non_enhancing': 2, 'enhancing': 3},\n",
    "        'numTraining': len(list(LABELS_TR.glob('*.nii*'))),\n",
    "        'file_ending': '.nii.gz'\n",
    "    }\n",
    "    with open(DS_RAW / 'dataset.json', 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    print('Wrote', DS_RAW / 'dataset.json')\n",
    "    print('imagesTr count:', len(list(IMAGES_TR.glob('*.nii*'))))\n",
    "    print('labelsTr count:', len(list(LABELS_TR.glob('*.nii*'))))\n",
    "\n",
    "# Rebuild with corrected mapping\n",
    "prepare_brats23_dataset()\n",
    "print('Raw dataset prepared at:', DS_RAW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan and preprocess\n",
    "\n",
    "We run the standard pipeline via nnU-Net v2 APIs. You can choose configurations; for a first pass `3d_fullres`\n",
    "is typical for BraTS-sized volumes. Adjust process counts based on your CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.experiment_planning.plan_and_preprocess_api import (\n",
    "    extract_fingerprints, plan_experiments, preprocess\n",
    ")\n",
    "\n",
    "# 1) Fingerprint (optionally verify integrity)\n",
    "extract_fingerprints([DATASET_ID], check_dataset_integrity=False, clean=True, verbose=True)\n",
    "\n",
    "# 2) Plan experiments (returns plans identifier string)\n",
    "plans_identifier = plan_experiments([DATASET_ID])\n",
    "print('Using plans:', plans_identifier)\n",
    "\n",
    "# 3) Preprocess selected configurations\n",
    "configs = ('3d_fullres',)\n",
    "preprocess([DATASET_ID], plans_identifier=plans_identifier, configurations=configs, num_processes=(4,), verbose=False)\n",
    "print('Preprocessing complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Train one fold. For a quick smoke test, set `FAST_TRAINING=True` to use a short trainer variant.\n",
    "For a proper run, use the default `nnUNetTrainer` and more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.run.run_training import run_training\n",
    "\n",
    "CONFIG = '3d_fullres'\n",
    "FOLD = 0  # 0..4 or 'all'\n",
    "FAST_TRAINING = True  # set False for real training\n",
    "trainer_name = 'nnUNetTrainer_5epochs' if FAST_TRAINING else 'nnUNetTrainer'\n",
    "\n",
    "# Use string dataset identifier to avoid AttributeError in get_trainer_from_args\n",
    "dataset_arg = DATASET_NAME  # or use str(DATASET_ID)\n",
    "print('Training with dataset_arg =', dataset_arg)\n",
    "\n",
    "# Choose device: CUDA if available, else MPS (Apple), else CPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "run_training(\n",
    "    dataset_arg,\n",
    "    CONFIG,\n",
    "    FOLD,\n",
    "    trainer_class_name=trainer_name,\n",
    "    plans_identifier=plans_identifier,\n",
    "    num_gpus=1,\n",
    "    export_validation_probabilities=False,\n",
    "    continue_training=False,\n",
    "    only_run_validation=False,\n",
    "    disable_checkpointing=False,\n",
    "    val_with_best=False,\n",
    "    device=device,\n",
    ")\n",
    "print('Training complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export trained model to zip\n",
    "\n",
    "This bundles `plans.json`, folds, checkpoints (e.g., `checkpoint_final.pth`), and validation summaries.\n",
    "You can extract the checkpoint for INR distillation or downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from nnunetv2.model_sharing.model_export import export_pretrained_model\n",
    "\n",
    "export_path = ARTIFACTS_DIR / f'{DATASET_NAME}_{CONFIG}_fold{FOLD}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip'\n",
    "export_pretrained_model(\n",
    "    DATASET_ID,\n",
    "    str(export_path),\n",
    "    configurations=(CONFIG,),\n",
    "    trainer=trainer_name,\n",
    "    plans_identifier=plans_identifier,\n",
    "    folds=(FOLD,),\n",
    "    strict=False,\n",
    "    save_checkpoints=('checkpoint_final.pth',),\n",
    "    export_crossval_predictions=False,\n",
    ")\n",
    "print('Exported to:', export_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate checkpoint for INR distillation\n",
    "\n",
    "Finds the saved `checkpoint_final.pth` for the trained fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnunetv2.utilities.file_path_utilities import get_output_folder\n",
    "from os.path import join\n",
    "\n",
    "out_dir = get_output_folder(f'Dataset{DATASET_ID:03d}_BraTS2023', trainer_name, plans_identifier, CONFIG)\n",
    "ckpt_path = Path(join(out_dir, f'fold_{FOLD}', 'checkpoint_final.pth'))\n",
    "print('Checkpoint:', ckpt_path)\n",
    "print('Exists:', ckpt_path.exists())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Quick inference sanity check\n",
    "\n",
    "Run a single-case prediction to validate the pipeline. Requires raw/test data prepared similarly.\n",
    "Skip on first run if time-constrained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (disabled by default)\n",
    "# from nnunetv2.inference.predict_from_raw_data import nnUNetPredictor\n",
    "# pred = nnUNetPredictor(tile_step_size=0.5, use_gaussian=True)\n",
    "# pred.initialize_from_trained_model_folder(\n",
    "#     str(Path(out_dir) / f'fold_{FOLD}'), use_folds=(FOLD,), checkpoint_name='checkpoint_final.pth'\n",
    "# )\n",
    "# pred.predict_from_files(...)\n",
    "pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis6020",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
