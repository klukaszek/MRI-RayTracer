{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkylelukaszek\u001b[0m (\u001b[33mkylelukaszek-university-of-guelph\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX devices: [CpuDevice(id=0), CpuDevice(id=1), CpuDevice(id=2), CpuDevice(id=3), CpuDevice(id=4), CpuDevice(id=5), CpuDevice(id=6), CpuDevice(id=7), CpuDevice(id=8), CpuDevice(id=9), CpuDevice(id=10), CpuDevice(id=11)]\n",
      "JAX default backend: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/kylelukaszek/Classes/AI/Project/notebooks/wandb/run-20251114_054827-uukm4o72</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kylelukaszek-university-of-guelph/brats-inr-segmentation/runs/uukm4o72' target=\"_blank\">neumors-inr-jax</a></strong> to <a href='https://wandb.ai/kylelukaszek-university-of-guelph/brats-inr-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kylelukaszek-university-of-guelph/brats-inr-segmentation' target=\"_blank\">https://wandb.ai/kylelukaszek-university-of-guelph/brats-inr-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kylelukaszek-university-of-guelph/brats-inr-segmentation/runs/uukm4o72' target=\"_blank\">https://wandb.ai/kylelukaszek-university-of-guelph/brats-inr-segmentation/runs/uukm4o72</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== SECTION 0: Imports & Global Config ====\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import csv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "from jax import random as jr, jit\n",
    "\n",
    "import wandb\n",
    "import optax\n",
    "import os\n",
    "\n",
    "# Attempt to maximize CPU parallelism for data loading on this host.\n",
    "# These environment variables are commonly respected by BLAS/NumPy builds.\n",
    "_threads = 12\n",
    "os.environ.setdefault('OMP_NUM_THREADS', str(_threads))\n",
    "os.environ.setdefault('MKL_NUM_THREADS', str(_threads))\n",
    "os.environ.setdefault('OPENBLAS_NUM_THREADS', str(_threads))\n",
    "\n",
    "\n",
    "# ---- Top-level experiment configuration ----\n",
    "\n",
    "CONFIG: Dict[str, Any] = {\n",
    "    # Reproducibility\n",
    "    \"seed\": 42,\n",
    "\n",
    "    # Paths\n",
    "    \"data\": {\n",
    "        \"brats_dir\": \"../data/BraTS-2023\",\n",
    "        \"mu_dir\": \"../data/MU-Glioma-Post\",\n",
    "        \"ckpt_dir\": \"../checkpoints\",\n",
    "        \"artifact_dir\": \"../artifacts\",\n",
    "    },\n",
    "\n",
    "    # Dataset and splits\n",
    "    \"dataset\": {\n",
    "        \"max_cases\": 1251,\n",
    "        \"val_fraction\": 0.2,\n",
    "        \"max_slices_per_case\": 8,\n",
    "        \"background_drop\": 0.7,\n",
    "    },\n",
    "\n",
    "    # Model / INR\n",
    "    \"model\": {\n",
    "        \"hidden_dims\": [256, 256, 256],\n",
    "        \"w0\": 30.0,\n",
    "        \"num_classes\": 4,\n",
    "        \"class_names\": [\"background\", \"necrotic\", \"edema\", \"enhancing\"],\n",
    "        # class weights for Dice/CE (background, necrotic, edema, enhancing)\n",
    "        # emphasize necrotic + edema more (they are underperforming), de-emphasize enhancing\n",
    "        # Format: [background, necrotic, edema, enhancing]\n",
    "        \"class_weights\": [1.0, 1.0, 1.0, 1.0],\n",
    "    },\n",
    "\n",
    "    # Optimizer\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"muon\",\n",
    "        \"learning_rate\": 2e-4,\n",
    "    },\n",
    "\n",
    "    # Training schedule\n",
    "    \"training\": {\n",
    "        # batch size used by prefetch_train / generator (large, divisible by 12)\n",
    "        \"batch_size\": 240,\n",
    "        # epochs we want to cover over the indexed slices\n",
    "        \"epochs_target\": 3,\n",
    "        # number of points sampled from each batch of slices\n",
    "        \"points_per_slice\": 65536,\n",
    "        # evaluate lightweight val monitor every k steps (computed later)\n",
    "        \"checkpoint_every\": 50,\n",
    "    },\n",
    "\n",
    "    # Validation monitor\n",
    "    \"validation\": {\n",
    "        \"monitor_cases\": 8,\n",
    "        \"monitor_points\": 128 * 128,\n",
    "    },\n",
    "\n",
    "    # W&B configuration\n",
    "    \"wandb\": {\n",
    "        \"project\": \"brats-inr-segmentation\",\n",
    "        \"run_name\": \"neumors-inr-jax\",\n",
    "        # group, tags, and notes help steer sweeps/evals\n",
    "        \"group\": \"inr-baselines\",\n",
    "        \"tags\": [\"jax\", \"inr\", \"siren\", \"brats23\"],\n",
    "        \"notes\": \"Compact 3D SIREN INR with soft Dice loss and slice-wise sampling.\",\n",
    "    },\n",
    "\n",
    "    # Performance / resource usage (tune to use more RAM/CPU on a large-memory Mac)\n",
    "    \"performance\": {\n",
    "        # number of batches to prefetch into a queue (increase to keep device busy)\n",
    "        \"max_prefetch\": 512,\n",
    "        # If True, load all indexed slices into RAM as arrays (fast, uses lots of memory)\n",
    "        \"cache_train_in_memory\": True,\n",
    "        # Number of threads to use for any threadpool-based prefetching (not yet used heavily)\n",
    "        \"num_data_loader_threads\": 12,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def setup_dirs(cfg: Dict[str, Any]) -> Dict[str, str]:\n",
    "    paths = cfg[\"data\"]\n",
    "    for d in [paths[\"ckpt_dir\"], paths[\"artifact_dir\"]]:\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def setup_wandb(cfg: Dict[str, Any]) -> None:\n",
    "    wb_cfg = cfg[\"wandb\"].copy()\n",
    "    project = wb_cfg.pop(\"project\")\n",
    "    run_name = wb_cfg.pop(\"run_name\")\n",
    "    wandb.init(project=project, name=run_name, config=cfg, **wb_cfg)\n",
    "\n",
    "\n",
    "PATHS = setup_dirs(CONFIG)\n",
    "SEED = int(CONFIG[\"seed\"])\n",
    "RNG = jr.PRNGKey(SEED)\n",
    "\n",
    "BRA_TS_DIR = PATHS[\"brats_dir\"]\n",
    "MU_DIR = PATHS[\"mu_dir\"]\n",
    "CKPT_DIR = PATHS[\"ckpt_dir\"]\n",
    "ART_DIR = PATHS[\"artifact_dir\"]\n",
    "\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"JAX default backend:\", jax.default_backend())\n",
    "\n",
    "setup_wandb(CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BraTS manifest: wrote 1251 rows -> ../data/BraTS-2023/manifest.csv\n",
      "MU manifest root used: ../data/MU-Glioma-Post/MU-Glioma-Post\n",
      "MU manifest: wrote 596 rows -> ../data/MU-Glioma-Post/manifest.csv\n"
     ]
    }
   ],
   "source": [
    "def build_brats_manifest(root: str, manifest_path: str) -> str:\n",
    "    root_abs = root\n",
    "    cases = sorted([p for p in glob.glob(os.path.join(root_abs, \"*\")) if os.path.isdir(p)])\n",
    "\n",
    "    def _pick(files: List[str], pattern: str) -> str:\n",
    "        rx = re.compile(pattern, re.IGNORECASE)\n",
    "        for f in files:\n",
    "            if rx.search(os.path.basename(f)):\n",
    "                return f\n",
    "        return \"\"\n",
    "\n",
    "    rows: List[Dict[str, str]] = []\n",
    "    for case in cases:\n",
    "        files = glob.glob(os.path.join(case, \"*.nii*\"))\n",
    "        rows.append({\n",
    "            \"id\": os.path.basename(case),\n",
    "            \"t1\": _pick(files, r\"t1n|t1\\.nii\"),\n",
    "            \"t1ce\": _pick(files, r\"t1c|t1ce\"),\n",
    "            \"t2\": _pick(files, r\"t2(?!f)|t2w\"),\n",
    "            \"flair\": _pick(files, r\"t2f|flair\"),\n",
    "            \"mask\": _pick(files, r\"seg|mask\"),\n",
    "        })\n",
    "\n",
    "    with open(manifest_path, \"w\", newline=\"\") as fp:\n",
    "        w = csv.DictWriter(fp, fieldnames=[\"id\", \"t1\", \"t1ce\", \"t2\", \"flair\", \"mask\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"BraTS manifest: wrote {len(rows)} rows -> {manifest_path}\")\n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "def build_mu_manifest(root: str, manifest_path: str) -> str:\n",
    "    def _pick(files: List[str], pattern: str, flags=re.IGNORECASE) -> str:\n",
    "        rx = re.compile(pattern, flags)\n",
    "        for f in files:\n",
    "            name = os.path.basename(f)\n",
    "            if rx.search(name):\n",
    "                return os.path.relpath(f, root)\n",
    "        return \"\"\n",
    "\n",
    "    root_mu = root\n",
    "    patients = sorted([p for p in glob.glob(os.path.join(root_mu, \"PatientID_*\")) if os.path.isdir(p)])\n",
    "    if not patients:\n",
    "        candidates = [\n",
    "            d\n",
    "            for d in glob.glob(os.path.join(root_mu, \"*\"))\n",
    "            if os.path.isdir(d) and os.path.basename(d).lower().startswith(\"mu\")\n",
    "        ]\n",
    "        if candidates:\n",
    "            root_mu = candidates[0]\n",
    "            patients = sorted([\n",
    "                p for p in glob.glob(os.path.join(root_mu, \"PatientID_*\")) if os.path.isdir(p)\n",
    "            ])\n",
    "\n",
    "    rows: List[Dict[str, str]] = []\n",
    "    for pat_dir in patients:\n",
    "        tps = sorted([tp for tp in glob.glob(os.path.join(pat_dir, \"Timepoint_*\")) if os.path.isdir(tp)])\n",
    "        for tp in tps:\n",
    "            files = glob.glob(os.path.join(tp, \"**\", \"*.nii*\"), recursive=True)\n",
    "            row = {\n",
    "                \"id\": os.path.relpath(tp, root_mu).replace(os.sep, \"_\"),\n",
    "                \"t1\": _pick(files, r\"(^|[^a-z0-9])t1(_n|n?1)?(\\.|_|$)\"),\n",
    "                \"t1ce\": _pick(files, r\"t1c|t1ce\"),\n",
    "                \"t2\": _pick(files, r\"(^|[^a-z0-9])t2(?!f)(\\.|_|$)|t2w\"),\n",
    "                \"flair\": _pick(files, r\"t2f|flair\"),\n",
    "                \"mask\": _pick(files, r\"seg|mask|tumorMask|_seg\"),\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    with open(manifest_path, \"w\", newline=\"\") as fp:\n",
    "        w = csv.DictWriter(fp, fieldnames=[\"id\", \"t1\", \"t1ce\", \"t2\", \"flair\", \"mask\"])\n",
    "        w.writeheader()\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"MU manifest root used: {root_mu}\")\n",
    "    print(f\"MU manifest: wrote {len(rows)} rows -> {manifest_path}\")\n",
    "    if len(rows) == 0:\n",
    "        print(\"Warning: no timepoints found in MU dataset.\")\n",
    "    return manifest_path\n",
    "\n",
    "\n",
    "man_bra = build_brats_manifest(BRA_TS_DIR, os.path.join(BRA_TS_DIR, \"manifest.csv\"))\n",
    "man_mu = build_mu_manifest(MU_DIR, os.path.join(MU_DIR, \"manifest.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1001 | Val: 250 (total 1251)\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def build_train_val_splits(manifest_path: str, cfg: Dict[str, Any]) -> Tuple[str, str, list, list]:\n",
    "    max_cases = int(cfg[\"dataset\"][\"max_cases\"])\n",
    "    val_fraction = float(cfg[\"dataset\"][\"val_fraction\"])\n",
    "\n",
    "    rng_key = jr.split(RNG)[0]\n",
    "    with open(manifest_path) as fp:\n",
    "        all_rows = [\n",
    "            r\n",
    "            for r in csv.DictReader(fp)\n",
    "            if all(r.get(k) for k in [\"t1\", \"t1ce\", \"t2\", \"flair\", \"mask\"])\n",
    "        ]\n",
    "\n",
    "    perm = jr.permutation(rng_key, jnp.arange(len(all_rows)))\n",
    "    all_rows = [all_rows[int(i)] for i in perm]\n",
    "    all_rows = all_rows[:max_cases]\n",
    "\n",
    "    n_total = len(all_rows)\n",
    "    n_val = int(val_fraction * n_total)\n",
    "    val_rows = all_rows[:n_val]\n",
    "    train_rows = all_rows[n_val:]\n",
    "\n",
    "    def _write_rows(rows, path):\n",
    "        with open(path, \"w\", newline=\"\") as fp:\n",
    "            w = csv.DictWriter(fp, fieldnames=[\"id\", \"t1\", \"t1ce\", \"t2\", \"flair\", \"mask\"])\n",
    "            w.writeheader()\n",
    "            w.writerows(rows)\n",
    "\n",
    "    train_csv = os.path.join(BRA_TS_DIR, \"train.csv\")\n",
    "    val_csv = os.path.join(BRA_TS_DIR, \"val.csv\")\n",
    "    _write_rows(train_rows, train_csv)\n",
    "    _write_rows(val_rows, val_csv)\n",
    "\n",
    "    print(f\"Train: {len(train_rows)} | Val: {len(val_rows)} (total {n_total})\")\n",
    "\n",
    "    artifact = wandb.Artifact(\"dataset-splits\", type=\"dataset\")\n",
    "    artifact.add_file(train_csv)\n",
    "    artifact.add_file(val_csv)\n",
    "    wandb.log_artifact(artifact)\n",
    "\n",
    "    return train_csv, val_csv, train_rows, val_rows\n",
    "\n",
    "\n",
    "train_csv, val_csv, train_rows, val_rows = build_train_val_splits(man_bra, CONFIG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built: train=4458 | val=1130\n"
     ]
    }
   ],
   "source": [
    "# ==== SECTION 4: Data Loading (memory-optimized) ====\n",
    "def _load_nifti_slice(path, z_idx=None, dtype=np.float32):\n",
    "    \"\"\"Load either a full volume or a single Z slice using the array proxy to avoid unnecessary copies.\"\"\"\n",
    "    img = nib.load(path)\n",
    "    if z_idx is None:\n",
    "        arr = np.asarray(img.dataobj, dtype=dtype)\n",
    "    else:\n",
    "        # Use slicing on the dataobj to only read one slice into memory\n",
    "        arr = np.asarray(img.dataobj[..., int(z_idx)], dtype=dtype)\n",
    "    return arr\n",
    "\n",
    "def load_modalities(row):\n",
    "    \"Return full volume as float32 modalities and uint8 mask (preserve labels).\"\"\"\n",
    "    vols = []\n",
    "    for k in [\"t1\", \"t1ce\", \"t2\", \"flair\"]:\n",
    "        p = row[k]\n",
    "        arr = _load_nifti_slice(p, z_idx=None, dtype=np.float32)\n",
    "        # normalize per-volume (float32) -- keep small eps for stability\n",
    "        arr = (arr - arr.mean()) / (arr.std() + 1e-6)\n",
    "        vols.append(jnp.array(arr, dtype=jnp.float32))\n",
    "    vol = jnp.stack(vols, axis=0)  # [C, X, Y, Z]\n",
    "    # Load mask as uint8 and preserve segmentation labels (0,1,2,4 etc.)\n",
    "    mask_arr = _load_nifti_slice(row[\"mask\"], z_idx=None, dtype=np.uint8)\n",
    "    mask = jnp.array(mask_arr, dtype=jnp.uint8)\n",
    "    return vol, mask\n",
    "\n",
    "def build_indexed_slices(manifest_path, max_slices_per_case=8, background_drop=0.7, seed=0):\n",
    "    \"Build a lightweight index of (row, z, z_norm) pairs. Slices are loaded on-demand at batch time.\"\"\"\n",
    "    rng = np.random.RandomState(int(SEED) + int(seed))\n",
    "    with open(manifest_path) as fp:\n",
    "        rows = list(csv.DictReader(fp))\n",
    "    index_list = []\n",
    "    for row in rows:\n",
    "        # probe z from one modality (assumes modalities share spatial dims)\n",
    "        p = row['t1']\n",
    "        img = nib.load(p)\n",
    "        z = img.shape[-1]\n",
    "        idxs = np.linspace(0, z - 1, num=min(max_slices_per_case, z), dtype=int)\n",
    "        for iz in idxs:\n",
    "            # check background for this slice by reading only the mask slice\n",
    "            mask_slice = np.asarray(nib.load(row['mask']).dataobj[..., int(iz)], dtype=np.uint8)\n",
    "            is_bg = (mask_slice.sum() == 0)\n",
    "            # optionally drop some background slices\n",
    "            if is_bg and rng.rand() < background_drop:\n",
    "                continue\n",
    "            # normalized z in [-1,1] (handle degenerate single-slice volumes)\n",
    "            if img.shape[-1] > 1:\n",
    "                z_norm = float( (float(iz) / float(img.shape[-1] - 1)) * 2.0 - 1.0 )\n",
    "            else:\n",
    "                z_norm = 0.0\n",
    "            index_list.append({'row': row, 'z': int(iz), 'z_norm': z_norm})\n",
    "    return index_list\n",
    "\n",
    "def batch_generator(index_list, batch_size=8, shuffle=True, loop_forever=False):\n",
    "    \"Yield batches of (X, Y, Zs) where X=[B, C, H, W], Y=[B, H, W] and Zs=[B] (normalized z floats). Loads data per-slice on demand.\"\"\"\n",
    "    n = len(index_list)\n",
    "    order = np.arange(n)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(order)\n",
    "        for start in range(0, n, batch_size):\n",
    "            batch_idx = order[start:start + batch_size]\n",
    "            Xb = []\n",
    "            Yb = []\n",
    "            Zb = []\n",
    "            for i in batch_idx:\n",
    "                entry = index_list[int(i)]\n",
    "                row = entry['row']\n",
    "                z = entry['z']\n",
    "                z_norm = entry.get('z_norm', 0.0)\n",
    "                # load 2D slice for each modality (float32)\n",
    "                slices = []\n",
    "                for k in [\"t1\", \"t1ce\", \"t2\", \"flair\"]:\n",
    "                    sl = _load_nifti_slice(row[k], z_idx=z, dtype=np.float32)\n",
    "                    # per-slice normalization (keeps contrast comparable across slices)\n",
    "                    sl = (sl - sl.mean()) / (sl.std() + 1e-6)\n",
    "                    slices.append(sl)\n",
    "                X = np.stack(slices, axis=0)  # [C, H, W]\n",
    "                mask_sl = np.asarray(nib.load(row['mask']).dataobj[..., z], dtype=np.uint8)\n",
    "                Xb.append(X)\n",
    "                Yb.append(mask_sl)\n",
    "                Zb.append(z_norm)\n",
    "            # convert to JAX arrays and put on device when consumed\n",
    "            Xb = jnp.array(np.stack(Xb, axis=0), dtype=jnp.float32)\n",
    "            Yb = jnp.array(np.stack(Yb, axis=0), dtype=jnp.uint8)\n",
    "            Zb = jnp.array(Zb, dtype=jnp.float32)\n",
    "            yield Xb, Yb, Zb\n",
    "        if not loop_forever:\n",
    "            break\n",
    "\n",
    "\n",
    "def build_in_memory_dataset(index_list):\n",
    "    \"\"\"Load all indexed slices into memory arrays. Returns (X_all, Y_all, Z_all).\n",
    "\n",
    "    WARNING: This loads all selected slices into RAM and may require a lot of memory.\n",
    "    Set `CONFIG['performance']['cache_train_in_memory'] = True` only if you have\n",
    "    sufficient RAM (Mac unified memory recommended >= 32GB for large indices).\n",
    "    \"\"\"\n",
    "    n = len(index_list)\n",
    "    if n == 0:\n",
    "        return None, None, None\n",
    "    # probe first entry for shape\n",
    "    e0 = index_list[0]\n",
    "    row0 = e0['row']\n",
    "    # load one slice to get H,W\n",
    "    tmp = _load_nifti_slice(row0['t1'], z_idx=e0['z'], dtype=np.float32)\n",
    "    H, W = tmp.shape\n",
    "    X_all = np.zeros((n, 4, H, W), dtype=np.float32)\n",
    "    Y_all = np.zeros((n, H, W), dtype=np.uint8)\n",
    "    Z_all = np.zeros((n,), dtype=np.float32)\n",
    "    for i, entry in enumerate(index_list):\n",
    "        row = entry['row']\n",
    "        z = entry['z']\n",
    "        z_norm = entry.get('z_norm', 0.0)\n",
    "        slices = []\n",
    "        for k in [\"t1\", \"t1ce\", \"t2\", \"flair\"]:\n",
    "            sl = _load_nifti_slice(row[k], z_idx=z, dtype=np.float32)\n",
    "            sl = (sl - sl.mean()) / (sl.std() + 1e-6)\n",
    "            slices.append(sl)\n",
    "        X_all[i] = np.stack(slices, axis=0)\n",
    "        Y_all[i] = np.asarray(nib.load(row['mask']).dataobj[..., z], dtype=np.uint8)\n",
    "        Z_all[i] = float(z_norm)\n",
    "    # Convert to jax arrays for downstream usage\n",
    "    return jnp.array(X_all, dtype=jnp.float32), jnp.array(Y_all, dtype=jnp.uint8), jnp.array(Z_all, dtype=jnp.float32)\n",
    "\n",
    "\n",
    "def batch_generator_cached(X_all, Y_all, Z_all, batch_size=8, shuffle=True, loop_forever=False):\n",
    "    \"\"\"Yield batches from in-memory arrays X_all/Y_all/Z_all.\n",
    "\n",
    "    X_all: [N, C, H, W], Y_all: [N, H, W], Z_all: [N]\n",
    "    \"\"\"\n",
    "    n = X_all.shape[0]\n",
    "    order = np.arange(n)\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(order)\n",
    "        for start in range(0, n, batch_size):\n",
    "            batch_idx = order[start:start + batch_size]\n",
    "            Xb = X_all[batch_idx]\n",
    "            Yb = Y_all[batch_idx]\n",
    "            Zb = Z_all[batch_idx]\n",
    "            # convert to jax arrays (if not already)\n",
    "            yield jnp.array(Xb, dtype=jnp.float32), jnp.array(Yb, dtype=jnp.uint8), jnp.array(Zb, dtype=jnp.float32)\n",
    "        if not loop_forever:\n",
    "            break\n",
    "\n",
    "# Build lightweight index (no full-volume allocations) and expose a batch generator\n",
    "train_index = build_indexed_slices(\n",
    "    train_csv,\n",
    "    max_slices_per_case=int(CONFIG[\"dataset\"][\"max_slices_per_case\"]),\n",
    "    background_drop=float(CONFIG[\"dataset\"][\"background_drop\"]),\n",
    "    seed=0,\n",
    ")\n",
    "val_index = build_indexed_slices(\n",
    "    val_csv,\n",
    "    max_slices_per_case=int(CONFIG[\"dataset\"][\"max_slices_per_case\"]),\n",
    "    background_drop=float(CONFIG[\"dataset\"][\"background_drop\"]),\n",
    "    seed=1,\n",
    ")\n",
    "\n",
    "print(f\"Index built: train={len(train_index)} | val={len(val_index)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATA LOADING / SCHEDULE SUMMARY\n",
      "============================================================\n",
      "Train index size: 4458 | Val index size: 1130\n",
      "Batch size: 240\n",
      "Batches per epoch (covering remainders): 19\n",
      "Target epochs: 3\n",
      "Computed max_steps: 57\n",
      "Approx. epochs of coverage: 3.07\n",
      "Validation monitor every 9 steps (approx 2x/epoch)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "batch_size = int(CONFIG[\"training\"][\"batch_size\"])\n",
    "epochs_target = int(CONFIG[\"training\"][\"epochs_target\"])\n",
    "\n",
    "# Number of full batches per epoch; we also account for the remainder so that\n",
    "# every indexed slice is seen.\n",
    "n_train = len(train_index)\n",
    "num_batches_per_epoch = max(1, (n_train + batch_size - 1) // batch_size)\n",
    "\n",
    "# Total steps chosen so that we cover the train index approximately\n",
    "# `epochs_target` times, including any remainder.\n",
    "max_steps = num_batches_per_epoch * epochs_target\n",
    "\n",
    "# Validation monitor every ~half epoch\n",
    "val_every = max(1, num_batches_per_epoch // 2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADING / SCHEDULE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train index size: {len(train_index)} | Val index size: {len(val_index)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Batches per epoch (covering remainders): {num_batches_per_epoch}\")\n",
    "print(f\"Target epochs: {epochs_target}\")\n",
    "print(f\"Computed max_steps: {max_steps}\")\n",
    "if len(train_index) > 0:\n",
    "    coverage = (max_steps * batch_size) / len(train_index)\n",
    "    print(f\"Approx. epochs of coverage: {coverage:.2f}\")\n",
    "else:\n",
    "    print(\"⚠️  No training samples; index is empty.\")\n",
    "print(f\"Validation monitor every {val_every} steps (approx 2x/epoch)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "# (Optional) interactive visualization widgets existed in the original notebook.\n",
    "# They are intentionally removed here to keep this file focused on\n",
    "# train/eval. If you want them back, consider a separate\n",
    "# `neumors_inr_vis.ipynb` that imports the shared helpers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching training index in memory (this may use a lot of RAM)...\n",
      "Prefetch batch shapes: (240, 4, 240, 240) (240, 240, 240) (240,) float32 uint8\n"
     ]
    }
   ],
   "source": [
    "# ==== SECTION 5: Prefetch + device-transfer helper ====\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "import jax\n",
    "\n",
    "def prefetch_generator(generator, max_prefetch=8, device=None):\n",
    "    \"\"\"Wrap a Python generator with a background thread that prefetches items into a queue.\n",
    "\n",
    "    - `generator` yields (X, Y, Zs) where X/Y are JAX arrays or numpy arrays and Zs is per-sample normalized z.\n",
    "    - `max_prefetch` controls queue depth (useful for keeping the accelerator busy).\n",
    "    - `device` (optional) is a `jax.Device` to transfer batches to (e.g., `jax.devices()[0]`).\n",
    "\n",
    "    Notes:\n",
    "    - Uses a daemon thread so the notebook can exit cleanly.\n",
    "    - For reproducible multi-worker setups consider using multiprocessing-aware designs.\n",
    "    \"\"\"\n",
    "    q = Queue(max_prefetch)\n",
    "    sentinel = object()\n",
    "\n",
    "    def _worker():\n",
    "        try:\n",
    "            for item in generator:\n",
    "                q.put(item)\n",
    "        finally:\n",
    "            q.put(sentinel)\n",
    "\n",
    "    t = Thread(target=_worker, daemon=True)\n",
    "    t.start()\n",
    "\n",
    "    while True:\n",
    "        item = q.get()\n",
    "        if item is sentinel:\n",
    "            break\n",
    "        X, Y, Zs = item\n",
    "        if device is not None:\n",
    "            X = jax.device_put(X, device=device)\n",
    "            Y = jax.device_put(Y, device=device)\n",
    "            Zs = jax.device_put(Zs, device=device)\n",
    "        yield X, Y, Zs\n",
    "\n",
    "# Example: create a prefetched training generator using available resources\n",
    "# Optionally cache training slices in memory for higher throughput\n",
    "perf_cfg = CONFIG.get(\"performance\", {})\n",
    "max_prefetch = int(perf_cfg.get(\"max_prefetch\", 12))\n",
    "cache_train = bool(perf_cfg.get(\"cache_train_in_memory\", False))\n",
    "\n",
    "if cache_train:\n",
    "    print(\"Caching training index in memory (this may use a lot of RAM)...\")\n",
    "    X_all, Y_all, Z_all = build_in_memory_dataset(train_index)\n",
    "    train_gen = batch_generator_cached(\n",
    "        X_all, Y_all, Z_all,\n",
    "        batch_size=int(CONFIG[\"training\"][\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        loop_forever=True,\n",
    "    )\n",
    "else:\n",
    "    train_gen = batch_generator(\n",
    "        train_index,\n",
    "        batch_size=int(CONFIG[\"training\"][\"batch_size\"]),\n",
    "        shuffle=True,\n",
    "        loop_forever=True,\n",
    "    )\n",
    "\n",
    "prefetch_train = prefetch_generator(train_gen, max_prefetch=max_prefetch, device=jax.devices()[0])\n",
    "\n",
    "# Pull a single batch to sanity-check shapes\n",
    "Xb, Yb, Zb = next(prefetch_train)\n",
    "print('Prefetch batch shapes:', Xb.shape, Yb.shape, Zb.shape, Xb.dtype, Yb.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared sharded batch: (12, 20, 4, 240, 240) (12, 20, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "# ==== SECTION 6: JIT & PMAP-friendly helpers ====\n",
    "import math\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "\n",
    "n_devices = jax.device_count()\n",
    "\n",
    "@jit\n",
    "def preprocess_batch_jit(X, Y):\n",
    "    \"\"\"JIT-able preprocessing for a batch.\n",
    "\n",
    "    - X: [B, C, H, W] (numeric)\n",
    "    - Y: [B, H, W] (labels)\n",
    "\n",
    "    Normalizes X per-sample and per-channel, clamps values, and ensures dtypes.\n",
    "    Returns jnp arrays with dtypes (float32, uint8).\n",
    "    \"\"\"\n",
    "    X = jnp.asarray(X, dtype=jnp.float32)\n",
    "    # per-sample, per-channel normalization across spatial dims\n",
    "    mean = jnp.mean(X, axis=(2, 3), keepdims=True)\n",
    "    std = jnp.std(X, axis=(2, 3), keepdims=True) + 1e-6\n",
    "    X = (X - mean) / std\n",
    "    X = jnp.clip(X, -5.0, 5.0)\n",
    "    Y = jnp.asarray(Y, dtype=jnp.uint8)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def shard_batch(X, Y, n_dev=None):\n",
    "    \"\"\"Split a batch into `n_dev` shards for `pmap` / device_put_sharded.\n",
    "\n",
    "    - X: [B, C, H, W]\n",
    "    - Y: [B, H, W]\n",
    "\n",
    "    Returns two lists of length `n_dev` with per-device arrays.\n",
    "    \"\"\"\n",
    "    if n_dev is None:\n",
    "        n_dev = jax.device_count()\n",
    "    B = X.shape[0]\n",
    "    if B % n_dev != 0:\n",
    "        raise ValueError(f\"Batch size {B} not divisible by n_devices {n_dev}\")\n",
    "    per = B // n_dev\n",
    "    # reshape into (n_dev, per, ...)\n",
    "    Xs = jnp.reshape(jnp.asarray(X), (n_dev, per) + X.shape[1:])\n",
    "    Ys = jnp.reshape(jnp.asarray(Y), (n_dev, per) + Y.shape[1:])\n",
    "    # convert to Python list of arrays for device_put_sharded\n",
    "    X_list = [Xs[i] for i in range(n_dev)]\n",
    "    Y_list = [Ys[i] for i in range(n_dev)]\n",
    "    return X_list, Y_list\n",
    "\n",
    "\n",
    "def device_put_sharded_batch(X, Y, devices=None):\n",
    "    \"\"\"Transfer a batch to devices as sharded arrays suitable for `pmap`.\n",
    "\n",
    "    - devices: list of jax.Device or None (defaults to all local devices)\n",
    "    - Returns: (X_sharded, Y_sharded) where each is a ShardedDeviceArray\n",
    "    \"\"\"\n",
    "    if devices is None:\n",
    "        devices = jax.devices()\n",
    "    n_dev = len(devices)\n",
    "    X_list, Y_list = shard_batch(X, Y, n_dev=n_dev)\n",
    "    X_sharded = jax.device_put_sharded(X_list, devices)\n",
    "    Y_sharded = jax.device_put_sharded(Y_list, devices)\n",
    "    return X_sharded, Y_sharded\n",
    "\n",
    "\n",
    "# Example jittable single-device training step (skeleton)\n",
    "def train_step_single(fn_apply, params, X, Y):\n",
    "    \"\"\"Example: single-device jitted training step stub.\n",
    "\n",
    "    - fn_apply: model forward (params, X) -> logits\n",
    "    - params: model parameters\n",
    "    - X: [B, C, H, W] float32\n",
    "    - Y: [B, H, W] uint8\n",
    "\n",
    "    This function is intentionally minimal — replace with your optimizer/update logic.\n",
    "    \"\"\"\n",
    "    logits = fn_apply(params, X)\n",
    "    # Example loss (mean squared) — replace with appropriate segmentation loss\n",
    "    loss = jnp.mean((logits - Y.astype(jnp.float32)) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Wrap with jit and mark fn_apply static (it's a Python callable)\n",
    "train_step_single = jit(train_step_single, static_argnames=('fn_apply',))\n",
    "\n",
    "\n",
    "# pmap-ready wrapper: make sure the single-device fn is pmapped across devices\n",
    "p_train_step = jax.pmap(train_step_single, axis_name='batch')\n",
    "\n",
    "# Small usage example (sanity-check). This will only run if a prefetched batch exists.\n",
    "try:\n",
    "    Xb, Yb, Zb = next(prefetch_train)\n",
    "    # Preprocess (runs on CPU until jitted is called)\n",
    "    Xb_proc, Yb_proc = preprocess_batch_jit(Xb, Yb)\n",
    "    # Make sharded arrays and transfer to devices\n",
    "    X_sharded, Y_sharded = device_put_sharded_batch(Xb_proc, Yb_proc)\n",
    "    print('Prepared sharded batch:', X_sharded.shape, Y_sharded.shape)\n",
    "except Exception as e:\n",
    "    print('Sanity-check skipped (no batch or small mismatch):', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: build_coords_modalities, mask mapping, dice, prediction, and train_step\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, value_and_grad\n",
    "import optax\n",
    "\n",
    "\n",
    "def build_coords_modalities(X, z_indices=None):\n",
    "    \"\"\"Build per-pixel coordinate+modality inputs for SIREN/INR.\n",
    "\n",
    "    Args:\n",
    "      X: jnp array shaped [S, C, H, W] (C==4: t1,t1ce,t2,flair)\n",
    "      z_indices: optional 1D array-like of length S (normalized z in [-1,1])\n",
    "\n",
    "    Returns:\n",
    "      inputs: jnp array shaped [S*H*W, 7] with columns [x, y, z_norm, t1, t1ce, t2, flair]\n",
    "    \"\"\"\n",
    "    X = jnp.asarray(X)\n",
    "    if X.ndim != 4:\n",
    "        raise ValueError(f\"Expected X with 4 dims [S,C,H,W], got {X.shape}\")\n",
    "    S, C, H, W = X.shape\n",
    "    if C != 4:\n",
    "        raise ValueError(f\"Expected 4 modalities (C==4), got C={C}\")\n",
    "\n",
    "    # normalized x,y in [-1,1]\n",
    "    xs = jnp.linspace(-1.0, 1.0, W, dtype=jnp.float32)\n",
    "    ys = jnp.linspace(-1.0, 1.0, H, dtype=jnp.float32)\n",
    "    gx, gy = jnp.meshgrid(xs, ys)  # shapes [H, W]\n",
    "    gx_f = gx.reshape(-1)\n",
    "    gy_f = gy.reshape(-1)\n",
    "\n",
    "    # modalities: reshape each slice to [S, H*W, C]\n",
    "    X_resh = jnp.transpose(X, (0, 2, 3, 1))  # [S, H, W, C]\n",
    "    X_flat = X_resh.reshape(S, H * W, C)     # [S, H*W, C]\n",
    "\n",
    "    # z_indices handling\n",
    "    if z_indices is None:\n",
    "        z_arr = jnp.zeros((S,), dtype=jnp.float32)\n",
    "    else:\n",
    "        z_arr = jnp.asarray(z_indices, dtype=jnp.float32)\n",
    "        if z_arr.ndim == 0:\n",
    "            z_arr = jnp.full((S,), float(z_arr), dtype=jnp.float32)\n",
    "        elif z_arr.shape[0] != S:\n",
    "            raise ValueError(f\"z_indices length {z_arr.shape[0]} != S {S}\")\n",
    "\n",
    "    coords_xy = jnp.stack([gx_f, gy_f], axis=-1)  # [H*W, 2]\n",
    "    coords_xy_b = jnp.broadcast_to(coords_xy[None, ...], (S, coords_xy.shape[0], 2))\n",
    "    z_col = z_arr[:, None]\n",
    "    z_b = jnp.broadcast_to(z_col[:, None, :], (S, coords_xy.shape[0], 1))  # [S, H*W, 1]\n",
    "\n",
    "    # Concatenate coords and modalities -> [S, H*W, 3+4]\n",
    "    inputs_per = jnp.concatenate([coords_xy_b, z_b, X_flat], axis=-1)  # [S, H*W, 7]\n",
    "    inputs = inputs_per.reshape((-1, inputs_per.shape[-1]))\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def map_mask_to_class(mask):\n",
    "    \"\"\"Map mask labels to contiguous class ids 0..3.\n",
    "\n",
    "    Expects mask in {0,1,2,4} (common BraTS labels). Maps 4 -> 3.\n",
    "    Accepts mask shaped [S,H,W] or [H,W].\n",
    "    \"\"\"\n",
    "    m = jnp.asarray(mask)\n",
    "    m_mapped = jnp.where(m == 4, 3, m)\n",
    "    return m_mapped\n",
    "\n",
    "\n",
    "def dice_per_class(pred_labels, true_labels, num_classes=4, eps=1e-6):\n",
    "    \"\"\"Compute Dice per class. Inputs are 1D or flattened  arrays of labels.\"\"\"\n",
    "    pred = jnp.asarray(pred_labels).ravel()\n",
    "    true = jnp.asarray(true_labels).ravel()\n",
    "    dices = []\n",
    "    for c in range(num_classes):\n",
    "        p_c = (pred == c).astype(jnp.float32)\n",
    "        t_c = (true == c).astype(jnp.float32)\n",
    "        inter = jnp.sum(p_c * t_c)\n",
    "        denom = jnp.sum(p_c) + jnp.sum(t_c)\n",
    "        dice = (2.0 * inter + eps) / (denom + eps)\n",
    "        dices.append(dice)\n",
    "    return jnp.stack(dices)\n",
    "\n",
    "\n",
    "def binary_dice(pred_labels, true_labels, pos_classes=(1,2,3), eps=1e-6):\n",
    "    \"\"\"Binary Dice treating any non-background as positive.\"\"\"\n",
    "    pred = jnp.asarray(pred_labels).ravel()\n",
    "    true = jnp.asarray(true_labels).ravel()\n",
    "    pos = jnp.array(pos_classes)\n",
    "    pred_pos = jnp.isin(pred, pos).astype(jnp.float32)\n",
    "    true_pos = jnp.isin(true, pos).astype(jnp.float32)\n",
    "    inter = jnp.sum(pred_pos * true_pos)\n",
    "    denom = jnp.sum(pred_pos) + jnp.sum(true_pos)\n",
    "    return (2.0 * inter + eps) / (denom + eps)\n",
    "\n",
    "\n",
    "@jit\n",
    "def predict_mask(params, inputs):\n",
    "    \"\"\"Return softmax probabilities for inputs: inputs shape [N, in_dim].\"\"\"\n",
    "    logits = siren_apply(params, inputs)\n",
    "    return jax.nn.softmax(logits)\n",
    "\n",
    "\n",
    "def soft_dice_from_probs(probs, targets, num_classes=4, eps=1e-6, class_weights=None):\n",
    "        \"\"\"Compute soft Dice per class and mean from probabilities.\n",
    "\n",
    "        Args:\n",
    "            probs: [N, C] softmax probabilities.\n",
    "            targets: [N] integer labels.\n",
    "            class_weights: optional array of shape [C] to weight per-class Dice.\n",
    "        Returns:\n",
    "            (dice_per_class, mean_dice) as JAX arrays.\n",
    "        \"\"\"\n",
    "        one_hot = jax.nn.one_hot(targets, num_classes)\n",
    "        probs_f = probs.reshape(-1, num_classes)\n",
    "        oh_f = one_hot.reshape(-1, num_classes)\n",
    "        inter = jnp.sum(probs_f * oh_f, axis=0)\n",
    "        denom = jnp.sum(probs_f + oh_f, axis=0)\n",
    "        dice_per_c = (2.0 * inter + eps) / (denom + eps)\n",
    "        if class_weights is not None:\n",
    "                w = jnp.asarray(class_weights, dtype=jnp.float32)\n",
    "                w = w / jnp.sum(w)\n",
    "                mean_dice = jnp.sum(w * dice_per_c)\n",
    "        else:\n",
    "                mean_dice = jnp.mean(dice_per_c)\n",
    "        return dice_per_c, mean_dice\n",
    "\n",
    "\n",
    "def soft_dice_loss(logits, targets, num_classes=4, eps=1e-6):\n",
    "    \"\"\"Soft Dice loss over all classes.\n",
    "\n",
    "    Args:\n",
    "      logits: [N, C] unnormalized scores.\n",
    "      targets: [N] integer labels.\n",
    "    Returns:\n",
    "      scalar soft Dice loss = 1 - mean Dice over classes.\n",
    "    \"\"\"\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    _, mean_dice = soft_dice_from_probs(probs, targets, num_classes=num_classes, eps=eps)\n",
    "    return 1.0 - mean_dice\n",
    "\n",
    "\n",
    "def soft_dice_ce_loss(\n",
    "    logits,\n",
    "    targets,\n",
    "    num_classes=4,\n",
    "    eps=1e-6,\n",
    "    ce_weight=0.6,\n",
    "    l2_weight=1e-6,\n",
    "    class_weights=None,\n",
    "    label_smoothing=0.15,\n",
    "    entropy_weight=0.02,\n",
    "):\n",
    "    \"\"\"Hybrid soft Dice + cross-entropy loss with extra regularizers to reduce overconfidence.\n",
    "\n",
    "    Additions:\n",
    "    - `label_smoothing`: smooth one-hot targets for CE to reduce overconfident peaks.\n",
    "    - `entropy_weight`: encourages higher predictive entropy (confidence penalty) to\n",
    "       prevent the model from becoming overconfident on a single class (e.g., enhancing).\n",
    "\n",
    "    The final loss is: dice_loss + ce_weight * ce_loss + l2_weight * l2_term - entropy_weight * mean_entropy\n",
    "    where mean_entropy is the mean entropy over predictions (higher entropy reduces loss).\n",
    "    \"\"\"\n",
    "    # Dynamic per-batch class weights (helps very imbalanced classes like necrotic/edema)\n",
    "    if class_weights is None:\n",
    "        counts = jnp.bincount(targets, minlength=num_classes).astype(jnp.float32)\n",
    "        # avoid division by zero for absent classes\n",
    "        inv = 1.0 / (counts + 1e-6)\n",
    "        weights = inv / (jnp.sum(inv) + 1e-12) * float(num_classes)\n",
    "    else:\n",
    "        w = jnp.asarray(class_weights, dtype=jnp.float32)\n",
    "        weights = w / (jnp.sum(w) + 1e-12) * float(num_classes)\n",
    "\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    # Weighted soft-dice component\n",
    "    _, mean_dice = soft_dice_from_probs(\n",
    "        probs, targets, num_classes=num_classes, eps=eps, class_weights=weights\n",
    "    )\n",
    "    dice_loss = 1.0 - mean_dice\n",
    "\n",
    "    # Cross-entropy with label smoothing\n",
    "    n = num_classes\n",
    "    smooth = float(label_smoothing)\n",
    "    one_hot = jax.nn.one_hot(targets, num_classes)\n",
    "    if smooth > 0.0:\n",
    "        one_hot = one_hot * (1.0 - smooth) + (smooth / float(n))\n",
    "\n",
    "    # Use focal CE (alpha derived from class weights) to focus gradients on hard examples\n",
    "    # and reduce influence of over-predicted easy negatives.\n",
    "    model_cfg = CONFIG.get(\"model\", {})\n",
    "    cfg_class_weights = model_cfg.get(\"class_weights\", None)\n",
    "    # convert normalized weights into alpha factor (sum to 1)\n",
    "    if cfg_class_weights is not None:\n",
    "        alpha = jnp.asarray(cfg_class_weights, dtype=jnp.float32)\n",
    "        alpha = alpha / (jnp.sum(alpha) + 1e-12)\n",
    "    else:\n",
    "        alpha = None\n",
    "    ce_loss = focal_cross_entropy(logits, targets, num_classes=num_classes, gamma=2.0, alpha=alpha, label_smoothing=smooth)\n",
    "\n",
    "    # Entropy regularization (mean entropy across samples)\n",
    "    # compute log-probabilities for entropy term\n",
    "    logp = jax.nn.log_softmax(logits, axis=-1)\n",
    "    entropy_per = -jnp.sum(probs * logp, axis=-1)\n",
    "    mean_entropy = jnp.mean(entropy_per)\n",
    "\n",
    "    # Mild L2 regularization on logits\n",
    "    l2_term = jnp.mean(logits ** 2)\n",
    "\n",
    "    return dice_loss + ce_weight * ce_loss + l2_weight * l2_term - entropy_weight * mean_entropy\n",
    "\n",
    "\n",
    "def focal_cross_entropy(logits, targets, num_classes=4, gamma=2.0, alpha=None, label_smoothing=0.0):\n",
    "    \"\"\"Focal-style cross-entropy operating on logits and (optionally) smoothed targets.\n",
    "\n",
    "    - `alpha` can be a per-class weight array or None.\n",
    "    - Uses smoothed one-hot targets if `label_smoothing` > 0.\n",
    "    \"\"\"\n",
    "    probs = jax.nn.softmax(logits, axis=-1)\n",
    "    one_hot = jax.nn.one_hot(targets, num_classes)\n",
    "    if label_smoothing > 0.0:\n",
    "        one_hot = one_hot * (1.0 - label_smoothing) + (label_smoothing / float(num_classes))\n",
    "    # probability mass assigned to the (possibly smoothed) target distribution\n",
    "    p_t = jnp.sum(probs * one_hot, axis=-1)\n",
    "    # avoid log(0)\n",
    "    log_p_t = jnp.log(p_t + 1e-12)\n",
    "    if alpha is not None:\n",
    "        a = jnp.asarray(alpha, dtype=jnp.float32)\n",
    "        alpha_t = jnp.sum(one_hot * a[None, :], axis=-1)\n",
    "    else:\n",
    "        alpha_t = 1.0\n",
    "    loss = -alpha_t * ((1.0 - p_t) ** float(gamma)) * log_p_t\n",
    "    return jnp.mean(loss)\n",
    "\n",
    "\n",
    "# jitted train step using optax; mark `opt` as static (it's a Python object)\n",
    "def _train_step_impl(params, opt_state, inputs, targets, num_classes):\n",
    "    def loss_fn(p, x, y):\n",
    "        logits = siren_apply(p, x)\n",
    "        model_cfg = CONFIG.get(\"model\", {})\n",
    "        class_weights = model_cfg.get(\"class_weights\", None)\n",
    "        return soft_dice_ce_loss(logits, y, num_classes=num_classes, class_weights=class_weights)\n",
    "    loss, grads = value_and_grad(loss_fn)(params, inputs, targets)\n",
    "    return loss, grads\n",
    "\n",
    "\n",
    "def train_step(params, opt_state, inputs, targets, opt):\n",
    "    # opt must be static when jitted below; num_classes captured from notebook globals\n",
    "    num_classes = globals().get('num_classes', 4)\n",
    "    loss, grads = _train_step_impl(params, opt_state, inputs, targets, num_classes)\n",
    "    updates, opt_state = opt.update(grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "\n",
    "# JIT the outer train_step and mark `opt` static\n",
    "train_step = jit(train_step, static_argnames=('opt',))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_per_slice = int(CONFIG[\"training\"][\"points_per_slice\"])\n",
    "checkpoint_every = int(CONFIG[\"training\"][\"checkpoint_every\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized SIREN INR with hidden dims: [256, 256, 256]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304c8243181d48a6b32813a3d9d9694f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/57 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==== SECTION 7: Compact 3D SIREN INR + Optax Muon training loop ====\n",
    "# Inputs to the INR: (x, y, z_norm) normalized coordinates + 4-modality values -> predict multi-class mask\n",
    "# This cell defines a lightweight 3D INR SIREN, initializes it, and runs the training loop.\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import random, jit, value_and_grad\n",
    "import optax\n",
    "import jax\n",
    "\n",
    "def siren_init(key, in_dim, hidden_dims, out_dim, w0=30.0):\n",
    "    \"\"\"Initialize a compact SIREN MLP for 3D INR.\"\"\"\n",
    "    keys = random.split(key, len(hidden_dims) + 1)\n",
    "    params = {}\n",
    "    dims = [in_dim] + hidden_dims + [out_dim]\n",
    "    for i in range(len(dims) - 1):\n",
    "        k = keys[i]\n",
    "        r = jnp.sqrt(6.0 / dims[i]) / (w0 if i == 0 else 1.0)\n",
    "        w = random.uniform(k, (dims[i], dims[i + 1]), minval=-r, maxval=r)\n",
    "        b = jnp.zeros((dims[i + 1],), dtype=jnp.float32)\n",
    "        params[f\"l{i}\"] = {\"w\": w, \"b\": b}\n",
    "    return params\n",
    "\n",
    "\n",
    "@jit\n",
    "def siren_apply(params, x, w0=30.0):\n",
    "    \"\"\"Apply SIREN: x is [N, in_dim] with (x,y,z_norm,modalities...).\"\"\"\n",
    "    h = x\n",
    "    for i in range(len(params) - 1):\n",
    "        w = params[f\"l{i}\"][\"w\"]\n",
    "        b = params[f\"l{i}\"][\"b\"]\n",
    "        if i == 0:\n",
    "            h = jnp.sin(w0 * (h @ w) + b)\n",
    "        else:\n",
    "            h = jnp.sin((h @ w) + b)\n",
    "    last = params[f\"l{len(params) - 1}\"]\n",
    "    out = (h @ last[\"w\"]) + last[\"b\"]\n",
    "    return out\n",
    "\n",
    "\n",
    "def init_model_and_opt(cfg: Dict[str, Any]):\n",
    "    model_cfg = cfg[\"model\"]\n",
    "    opt_cfg = cfg[\"optimizer\"]\n",
    "    key = jr.PRNGKey(int(cfg[\"seed\"]))\n",
    "    input_dim = 3 + 4\n",
    "    hidden = list(model_cfg[\"hidden_dims\"])\n",
    "    num_classes = int(model_cfg[\"num_classes\"])\n",
    "    w0 = float(model_cfg[\"w0\"])\n",
    "    siren_key, _ = jr.split(key)\n",
    "    params = siren_init(siren_key, input_dim, hidden, num_classes, w0=w0)\n",
    "\n",
    "    if opt_cfg[\"type\"] == \"muon\":\n",
    "        opt = optax.contrib.muon(learning_rate=float(opt_cfg[\"learning_rate\"]))\n",
    "    else:\n",
    "        opt = optax.adam(learning_rate=float(opt_cfg[\"learning_rate\"]))\n",
    "\n",
    "    opt_state = opt.init(params)\n",
    "    print(\"Initialized SIREN INR with hidden dims:\", hidden)\n",
    "    return params, opt_state, opt, num_classes, model_cfg[\"class_names\"], w0\n",
    "\n",
    "params, opt_state, opt, num_classes, class_names, w0 = init_model_and_opt(CONFIG)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_dice_history = []\n",
    "\n",
    "gen = prefetch_train\n",
    "sample_key = random.PRNGKey(int(SEED) + 1234)\n",
    "\n",
    "import numpy as onp\n",
    "\n",
    "# Training loop\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "pbar = tqdm(range(max_steps), desc='Training', unit='step')\n",
    "# Visualization history buffers\n",
    "loss_history = []\n",
    "dice_history = [[] for _ in range(num_classes)]\n",
    "soft_dice_history = [[] for _ in range(num_classes)]\n",
    "soft_dice_mean_history = []\n",
    "# we keep CE history only for reference/monitoring if needed\n",
    "ce_history = [[] for _ in range(num_classes)]\n",
    "binary_dice_history = []  # train binary dice per step\n",
    "# validation per-step histories (small held-out monitor)\n",
    "val_dice_history = [[] for _ in range(num_classes)]\n",
    "val_soft_dice_history = [[] for _ in range(num_classes)]\n",
    "val_soft_dice_mean_history = []\n",
    "val_ce_history = [[] for _ in range(num_classes)]\n",
    "val_binary_dice_history = []  # val binary dice per step\n",
    "start = time.time()\n",
    "# Choose a case to visualize (validation preferred)\n",
    "vis_rows = val_rows if len(val_rows) > 0 else train_rows\n",
    "vis_row = vis_rows[0] if len(vis_rows) > 0 else None\n",
    "if vis_row is not None:\n",
    "    img_probe = nib.load(vis_row['t1'])\n",
    "    # nib shape is typically (H, W, D)\n",
    "    vol_shape = img_probe.shape\n",
    "    H, W, D = int(vol_shape[0]), int(vol_shape[1]), int(vol_shape[2])\n",
    "    mid_z = D // 2\n",
    "else:\n",
    "    H = W = D = None\n",
    "    mid_z = 0\n",
    "displayed_fig = None\n",
    "\n",
    "val_monitor_cases = int(CONFIG[\"validation\"][\"monitor_cases\"])\n",
    "val_monitor_points = int(CONFIG[\"validation\"][\"monitor_points\"])\n",
    "val_inputs = None\n",
    "val_targets = None\n",
    "if len(val_index) > 0:\n",
    "    # collect up to `val_monitor_cases` distinct slices from the validation index\n",
    "    vm_entries = val_index[:min(len(val_index), val_monitor_cases)]\n",
    "    Xs = []\n",
    "    Ys = []\n",
    "    Zs = []\n",
    "    for e in vm_entries:\n",
    "        row = e['row']\n",
    "        z = int(e.get('z', mid_z))\n",
    "        slices = []\n",
    "        for kmod in ['t1', 't1ce', 't2', 'flair']:\n",
    "            sl = _load_nifti_slice(row[kmod], z_idx=z, dtype=np.float32)\n",
    "            sl = (sl - sl.mean()) / (sl.std() + 1e-6)\n",
    "            slices.append(sl)\n",
    "        Xs.append(np.stack(slices, axis=0))\n",
    "        Ys.append(np.asarray(nib.load(row['mask']).dataobj[..., z], dtype=np.uint8))\n",
    "        Zs.append(float(e.get('z_norm', 0.0)))\n",
    "    if len(Xs) > 0:\n",
    "        X_val_case = jnp.array(np.stack(Xs, axis=0), dtype=jnp.float32)  # [S, C, H, W]\n",
    "        Y_val_case = jnp.array(np.stack(Ys, axis=0), dtype=jnp.uint8)     # [S, H, W]\n",
    "        Z_val_case = jnp.array(np.array(Zs, dtype=np.float32), dtype=jnp.float32)\n",
    "        # build full inputs/targets once and sample from them each step\n",
    "        val_inputs = build_coords_modalities(X_val_case, z_indices=Z_val_case)\n",
    "        val_targets = jnp.ravel(map_mask_to_class(Y_val_case)).astype(jnp.int32)\n",
    "        val_total = int(val_inputs.shape[0])\n",
    "    else:\n",
    "        val_inputs = None\n",
    "        val_targets = None\n",
    "        val_total = 0\n",
    "else:\n",
    "    val_inputs = None\n",
    "    val_targets = None\n",
    "    val_total = 0\n",
    "\n",
    "# helper to predict a single slice for visualization\n",
    "\n",
    "def predict_slice(params, case_row=vis_row, z=mid_z):\n",
    "    if case_row is None:\n",
    "        return None\n",
    "    # load slice intensities for each modality and normalize\n",
    "    slices = []\n",
    "    for kmod in ['t1', 't1ce', 't2', 'flair']:\n",
    "        sl = _load_nifti_slice(case_row[kmod], z_idx=z, dtype=np.float32)\n",
    "        sl = (sl - sl.mean()) / (sl.std() + 1e-6)\n",
    "        slices.append(sl)\n",
    "    X_slice = np.stack(slices, axis=0)  # [C,H,W]\n",
    "    # Build inputs and predict using existing helpers\n",
    "    Xj = jnp.array(X_slice[None, ...], dtype=jnp.float32)  # [1,C,H,W]\n",
    "    if D and D > 1:\n",
    "        z_norm = float((z / float(D - 1)) * 2.0 - 1.0)\n",
    "    else:\n",
    "        z_norm = 0.0\n",
    "    inputs = build_coords_modalities(Xj, z_indices=jnp.array([z_norm], dtype=jnp.float32))\n",
    "    probs = predict_mask(params, inputs)\n",
    "    labels = jnp.argmax(probs, axis=-1).astype(jnp.int32)\n",
    "    return np.array(labels).reshape(H, W)\n",
    "\n",
    "# counter to control how often we log plots to W&B\n",
    "plot_log_counter = 0\n",
    "\n",
    "try:\n",
    "    for step in pbar:\n",
    "        # training batch\n",
    "        try:\n",
    "            Xb, Yb, Zb = next(gen)\n",
    "        except Exception as e_batch:\n",
    "            pbar.write(f'Batch fetch failed: {e_batch}')\n",
    "            break\n",
    "        # prepare inputs & targets\n",
    "        inputs = build_coords_modalities(Xb, z_indices=Zb)\n",
    "        classes = map_mask_to_class(Yb)\n",
    "        targets = jnp.ravel(classes).astype(jnp.int32)\n",
    "        total = int(inputs.shape[0])\n",
    "\n",
    "        # Positive-biased sampling: ensure each step sees a healthy fraction\n",
    "        # of tumor pixels instead of purely uniform sampling.\n",
    "        nsel = min(points_per_slice, total)\n",
    "        targets_flat = targets\n",
    "\n",
    "        # Per-class sampling: ensure each tumor sub-region gets representation.\n",
    "        # Classes: 0=background, 1=necrotic, 2=edema, 3=enhancing\n",
    "        idx_bg = jnp.where(targets_flat == 0)[0]\n",
    "        idx_ncr = jnp.where(targets_flat == 1)[0]\n",
    "        idx_edema = jnp.where(targets_flat == 2)[0]\n",
    "        idx_enh = jnp.where(targets_flat == 3)[0]\n",
    "\n",
    "        # Desired fractions (sum <= 1.0). Remaining goes to background.\n",
    "        # Increase tumor sampling fractions to present the model with more positive\n",
    "        # examples per step which helps escape plateaus when rare classes are underlearned.\n",
    "        frac_ncr = 0.12\n",
    "        frac_edema = 0.34\n",
    "        frac_enh = 0.34\n",
    "\n",
    "        sample_key, k_ncr = random.split(sample_key)\n",
    "        sample_key, k_edema = random.split(sample_key)\n",
    "        sample_key, k_enh = random.split(sample_key)\n",
    "        sample_key, k_bg = random.split(sample_key)\n",
    "\n",
    "        def _sample_indices(key, pool, k_target):\n",
    "            k_target = int(k_target)\n",
    "            if pool.shape[0] == 0 or k_target <= 0:\n",
    "                return jnp.array([], dtype=jnp.int32)\n",
    "            # If the available pool is smaller than the desired target, sample with\n",
    "            # replacement to ensure we still present the model with the intended\n",
    "            # number of positive examples per class. This helps when some classes are\n",
    "            # extremely rare in a given batch/slice.\n",
    "            if pool.shape[0] >= k_target:\n",
    "                sel = random.choice(key, pool.shape[0], (int(k_target),), replace=False)\n",
    "            else:\n",
    "                sel = random.choice(key, pool.shape[0], (int(k_target),), replace=True)\n",
    "            return pool[sel]\n",
    "\n",
    "        n_ncr = int(frac_ncr * nsel)\n",
    "        n_edema = int(frac_edema * nsel)\n",
    "        n_enh = int(frac_enh * nsel)\n",
    "\n",
    "        chosen_ncr = _sample_indices(k_ncr, idx_ncr, n_ncr)\n",
    "        chosen_edema = _sample_indices(k_edema, idx_edema, n_edema)\n",
    "        chosen_enh = _sample_indices(k_enh, idx_enh, n_enh)\n",
    "\n",
    "        used = int(chosen_ncr.shape[0] + chosen_edema.shape[0] + chosen_enh.shape[0])\n",
    "        remaining = max(0, nsel - used)\n",
    "        chosen_bg = _sample_indices(k_bg, idx_bg, remaining)\n",
    "\n",
    "        sel = jnp.concatenate([chosen_ncr, chosen_edema, chosen_enh, chosen_bg], axis=0)\n",
    "        inputs_s = inputs[sel]\n",
    "        targets_s = targets_flat[sel]\n",
    "        params, opt_state, loss_val = train_step(params, opt_state, inputs_s, targets_s, opt)\n",
    "        loss_val_f = float(loss_val)\n",
    "        train_losses.append(loss_val_f)\n",
    "        loss_history.append(loss_val_f)\n",
    "\n",
    "        # Compute quick per-class Dice + soft Dice + Binary Dice on the sampled mini-batch for monitoring\n",
    "        try:\n",
    "            probs_s = predict_mask(params, inputs_s)\n",
    "            pred_labels_s = jnp.argmax(probs_s, axis=-1).astype(jnp.int32)\n",
    "            dice_vals = dice_per_class(pred_labels_s, targets_s, num_classes)\n",
    "            bin_dice_train = binary_dice(pred_labels_s, targets_s)\n",
    "            soft_dice_c, soft_dice_mean = soft_dice_from_probs(probs_s, targets_s, num_classes=num_classes)\n",
    "            for c in range(num_classes):\n",
    "                dice_history[c].append(float(dice_vals[c]))\n",
    "                soft_dice_history[c].append(float(soft_dice_c[c]))\n",
    "            soft_dice_mean_history.append(float(soft_dice_mean))\n",
    "            # (Optional) cross-entropy per-sample then aggregated per-class, kept for reference\n",
    "            logp = jax.nn.log_softmax(siren_apply(params, inputs_s))\n",
    "            ce_per_sample = -jnp.sum(jax.nn.one_hot(targets_s, num_classes) * logp, axis=-1)\n",
    "            for c in range(num_classes):\n",
    "                mask_c = (targets_s == c)\n",
    "                if jnp.sum(mask_c) > 0:\n",
    "                    ce_c = jnp.mean(ce_per_sample[mask_c])\n",
    "                else:\n",
    "                    ce_c = jnp.array(jnp.nan)\n",
    "                ce_history[c].append(float(ce_c))\n",
    "            binary_dice_history.append(float(bin_dice_train))\n",
    "        except Exception as e_mon:\n",
    "            pbar.write(f'Monitor metrics failed: {e_mon}')\n",
    "\n",
    "        # Prepare W&B metrics for this step (train + val if available)\n",
    "        try:\n",
    "            # use soft-dice loss as primary loss\n",
    "            wandb_metrics = {\n",
    "                'train/loss_soft_dice': float(loss_val),\n",
    "                'train/step': int(step + 1),\n",
    "            }\n",
    "            # add latest train per-class metrics (hard and soft Dice)\n",
    "            for c in range(num_classes):\n",
    "                if len(dice_history[c]) > 0:\n",
    "                    wandb_metrics[f'train/dice_class_{c}'] = float(dice_history[c][-1])\n",
    "                if len(soft_dice_history[c]) > 0:\n",
    "                    wandb_metrics[f'train/soft_dice_class_{c}'] = float(soft_dice_history[c][-1])\n",
    "                if len(ce_history[c]) > 0:\n",
    "                    wandb_metrics[f'train/ce_class_{c}'] = float(ce_history[c][-1])\n",
    "            wandb_metrics['train/dice_mean'] = float(onp.nanmean(onp.array([d[-1] if len(d)>0 else onp.nan for d in dice_history])))\n",
    "            wandb_metrics['train/soft_dice_mean'] = float(onp.nanmean(onp.array(soft_dice_mean_history))) if len(soft_dice_mean_history) > 0 else onp.nan\n",
    "            wandb_metrics['train/ce_mean'] = float(onp.nanmean(onp.array([c[-1] if len(c)>0 else onp.nan for c in ce_history])))\n",
    "            if len(binary_dice_history) > 0:\n",
    "                wandb_metrics['train/binary_dice'] = float(binary_dice_history[-1])\n",
    "            # --- Validation monitor: sample from prebuilt val_inputs each step (cheap) ---\n",
    "            if val_inputs is not None and val_total > 0:\n",
    "                nsel_val = min(val_monitor_points, val_total)\n",
    "                # use numpy sampling for the selection index to avoid JAX randomness issues in Python loop\n",
    "                sel_val = onp.random.choice(val_total, nsel_val, replace=False)\n",
    "                vi = val_inputs[sel_val]\n",
    "                vt = val_targets[sel_val]\n",
    "                probs_v = predict_mask(params, vi)\n",
    "                pred_v = jnp.argmax(probs_v, axis=-1).astype(jnp.int32)\n",
    "                dice_v = dice_per_class(pred_v, vt, num_classes)\n",
    "                bin_dice_val = binary_dice(pred_v, vt)\n",
    "                soft_dice_c_v, soft_dice_mean_v = soft_dice_from_probs(probs_v, vt, num_classes=num_classes)\n",
    "                for c in range(num_classes):\n",
    "                    val_dice_history[c].append(float(dice_v[c]))\n",
    "                    val_soft_dice_history[c].append(float(soft_dice_c_v[c]))\n",
    "                val_soft_dice_mean_history.append(float(soft_dice_mean_v))\n",
    "                # CE on validation sampled points (optional, for reference)\n",
    "                logp_v = jax.nn.log_softmax(siren_apply(params, vi))\n",
    "                ce_per_sample_v = -jnp.sum(jax.nn.one_hot(vt, num_classes) * logp_v, axis=-1)\n",
    "                for c in range(num_classes):\n",
    "                    mask_c_v = (vt == c)\n",
    "                    if jnp.sum(mask_c_v) > 0:\n",
    "                        ce_c_v = float(jnp.mean(ce_per_sample_v[mask_c_v]))\n",
    "                    else:\n",
    "                        ce_c_v = float(onp.nan)\n",
    "                    val_ce_history[c].append(ce_c_v)\n",
    "                val_binary_dice_history.append(float(bin_dice_val))\n",
    "                wandb_metrics['val/dice_mean'] = float(onp.nanmean(onp.array([d[-1] if len(d)>0 else onp.nan for d in val_dice_history])))\n",
    "                wandb_metrics['val/soft_dice_mean'] = float(onp.nanmean(onp.array(val_soft_dice_mean_history))) if len(val_soft_dice_mean_history) > 0 else onp.nan\n",
    "                wandb_metrics['val/ce_mean'] = float(onp.nanmean(onp.array([c[-1] if len(c)>0 else onp.nan for c in val_ce_history])))\n",
    "                wandb_metrics['val/binary_dice'] = float(bin_dice_val)\n",
    "                for c in range(num_classes):\n",
    "                    if len(val_dice_history[c]) > 0:\n",
    "                        wandb_metrics[f'val/dice_class_{c}'] = float(val_dice_history[c][-1])\n",
    "                    if len(val_soft_dice_history[c]) > 0:\n",
    "                        wandb_metrics[f'val/soft_dice_class_{c}'] = float(val_soft_dice_history[c][-1])\n",
    "                    if len(val_ce_history[c]) > 0:\n",
    "                        wandb_metrics[f'val/ce_class_{c}'] = float(val_ce_history[c][-1])\n",
    "            wandb.log(wandb_metrics, step=int(step + 1))\n",
    "        except Exception as e_wlog:\n",
    "            pbar.write(f'W&B log failed: {e_wlog}')\n",
    "\n",
    "        # Visualize every step (update training plot each iteration)\n",
    "        try:\n",
    "            pred_slice = predict_slice(params, case_row=vis_row, z=mid_z) if vis_row is not None else None\n",
    "            # load ground truth & modality for visualization\n",
    "            if vis_row is not None:\n",
    "                true_slice = _load_nifti_slice(vis_row['mask'], z_idx=mid_z, dtype=np.uint8)\n",
    "                mod0_slice = _load_nifti_slice(vis_row['t1'], z_idx=mid_z, dtype=np.float32)\n",
    "                mod0_slice = (mod0_slice - mod0_slice.mean()) / (mod0_slice.std() + 1e-6)\n",
    "            clear_output(wait=True)\n",
    "            import matplotlib.pyplot as plt\n",
    "            # Create a 3x3 grid: loss, Dice curves, binary Dice | GT, pred, (empty)\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 8))\n",
    "            # Top-left: loss over time\n",
    "            ax_loss = axes[0, 0]\n",
    "            ax_loss.plot(loss_history, color='tab:blue', linewidth=2, label='soft Dice loss')\n",
    "            ax_loss.set_title('Train Soft Dice Loss', fontsize=12, fontweight='bold')\n",
    "            ax_loss.set_xlabel('Step')\n",
    "            ax_loss.set_ylabel('Soft Dice Loss')\n",
    "            ax_loss.grid(True, alpha=0.3)\n",
    "            # Top-middle: train per-class Dice (hard and soft)\n",
    "            ax_dice_train = axes[0, 1]\n",
    "            colors = plt.cm.tab10.colors\n",
    "            for c in range(num_classes):\n",
    "                ax_dice_train.plot(dice_history[c], label=f'hard c{c}: {class_names[c]}', color=colors[c % len(colors)], linewidth=1.5, alpha=0.6)\n",
    "                ax_dice_train.plot(soft_dice_history[c], linestyle='--', label=f'soft c{c}: {class_names[c]}', color=colors[c % len(colors)], linewidth=1.5)\n",
    "            ax_dice_train.set_title('Train Per-Class Dice (hard vs soft)', fontsize=12, fontweight='bold')\n",
    "            ax_dice_train.set_xlabel('Step')\n",
    "            ax_dice_train.set_ylabel('Dice')\n",
    "            ax_dice_train.set_ylim(-0.05, 1.05)\n",
    "            ax_dice_train.legend(fontsize=7, loc='lower right')\n",
    "            ax_dice_train.grid(True, alpha=0.3)\n",
    "            # Top-right: val per-class Dice (hard and soft)\n",
    "            ax_dice_val = axes[0, 2]\n",
    "            if len(val_dice_history[0]) > 0:\n",
    "                for c in range(num_classes):\n",
    "                    ax_dice_val.plot(val_dice_history[c], label=f'hard c{c}: {class_names[c]}', color=colors[c % len(colors)], linewidth=1.5, alpha=0.6)\n",
    "                    ax_dice_val.plot(val_soft_dice_history[c], linestyle='--', label=f'soft c{c}: {class_names[c]}', color=colors[c % len(colors)], linewidth=1.5)\n",
    "                ax_dice_val.set_ylim(-0.05, 1.05)\n",
    "                ax_dice_val.legend(fontsize=7, loc='lower right')\n",
    "            ax_dice_val.set_title('Val Per-Class Dice (hard vs soft)', fontsize=12, fontweight='bold')\n",
    "            ax_dice_val.set_xlabel('Step')\n",
    "            ax_dice_val.set_ylabel('Dice')\n",
    "            ax_dice_val.grid(True, alpha=0.3)\n",
    "            # Bottom-left: ground truth slice\n",
    "            ax_gt = axes[1, 0]\n",
    "            if vis_row is not None:\n",
    "                ax_gt.imshow(mod0_slice, cmap='gray')\n",
    "                ax_gt.imshow(true_slice, alpha=0.35, cmap='tab10')\n",
    "                ax_gt.set_title('Ground Truth Slice', fontsize=12, fontweight='bold')\n",
    "            ax_gt.axis('off')\n",
    "            # Bottom-middle: prediction overlay\n",
    "            ax_pred = axes[1, 1]\n",
    "            if vis_row is not None:\n",
    "                ax_pred.imshow(mod0_slice, cmap='gray')\n",
    "                if pred_slice is not None:\n",
    "                    ax_pred.imshow(pred_slice, alpha=0.35, cmap='tab10')\n",
    "                ax_pred.set_title(f'Pred Slice Step {int(step+1)}', fontsize=12, fontweight='bold')\n",
    "            ax_pred.axis('off')\n",
    "            # Bottom-right: Binary Dice (train + val)\n",
    "            ax_binary = axes[1, 2]\n",
    "            if len(binary_dice_history) > 0:\n",
    "                ax_binary.plot(binary_dice_history, label='Train Binary Dice', color='tab:green', linewidth=2)\n",
    "            if len(val_binary_dice_history) > 0:\n",
    "                ax_binary.plot(val_binary_dice_history, label='Val Binary Dice', color='tab:red', linestyle='--', linewidth=2)\n",
    "            ax_binary.set_title('Binary Dice (Tumor Overlap)', fontsize=12, fontweight='bold')\n",
    "            ax_binary.set_xlabel('Step')\n",
    "            ax_binary.set_ylabel('Binary Dice')\n",
    "            ax_binary.set_ylim(-0.05, 1.05)\n",
    "            ax_binary.legend(fontsize=9, loc='lower right')\n",
    "            ax_binary.grid(True, alpha=0.3)\n",
    "            fig.suptitle(f'Step {int(step+1)}/{max_steps} SoftDiceLoss={loss_val_f:.4f}')\n",
    "            plt.tight_layout()\n",
    "            display(fig)\n",
    "\n",
    "            # Increment plot counter and only log image to W&B every 10 plots\n",
    "            plot_log_counter += 1\n",
    "            if plot_log_counter % 10 == 0:\n",
    "                try:\n",
    "                    wandb.log({'train/predictions': wandb.Image(fig)}, step=int(step+1))\n",
    "                except Exception as e_wimg:\n",
    "                    pbar.write(f'W&B image log failed: {e_wimg}')\n",
    "\n",
    "            plt.close(fig)\n",
    "        except Exception as e_vis:\n",
    "            pbar.write(f'Visualization failed: {e_vis}')\n",
    "\n",
    "        # Checkpoint occasionally\n",
    "        if checkpoint_every and (step + 1) % checkpoint_every == 0:\n",
    "            try:\n",
    "                ckpt_path = os.path.join(CKPT_DIR, f'siren_params_step{step+1}.npz')\n",
    "                flat = jax.tree_util.tree_map(lambda x: onp.array(x), params)\n",
    "                if isinstance(flat, dict):\n",
    "                    onp.savez_compressed(ckpt_path, **{f: flat[f] for f in flat})\n",
    "                else:\n",
    "                    leaves, _ = jax.tree_util.tree_flatten(flat)\n",
    "                    onp.savez_compressed(ckpt_path, *leaves)\n",
    "                pbar.write(f'Wrote checkpoint {ckpt_path}')\n",
    "            except Exception as e_ck:\n",
    "                pbar.write(f'Checkpoint failed: {e_ck}')\n",
    "\n",
    "        # update tqdm postfix\n",
    "        postfix = {'soft_dice_loss': f'{loss_val_f:.6f}'}\n",
    "        pbar.set_postfix(postfix)\n",
    "        pbar.refresh()\n",
    "\n",
    "except Exception as e:\n",
    "    pbar.write(f'Training loop aborted/skipped due to error: {e}')\n",
    "finally:\n",
    "    pbar.close()\n",
    "\n",
    "# --- After training: plot and log loss curve and final metrics ---\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute final and best metrics\n",
    "final_step = int(step + 1) if 'step' in locals() else 0\n",
    "final_loss = float(train_losses[-1]) if len(train_losses) > 0 else float('nan')\n",
    "best_loss = float(min(train_losses)) if len(train_losses) > 0 else float('nan')\n",
    "best_loss_step = int(train_losses.index(min(train_losses)) + 1) if len(train_losses) > 0 else 0\n",
    "\n",
    "# Final and best metrics summary\n",
    "final_metrics = {\n",
    "    'final_step': final_step,\n",
    "    'final_soft_dice_loss': final_loss,\n",
    "    'best_soft_dice_loss': best_loss,\n",
    "    'best_loss_at_step': best_loss_step,\n",
    "}\n",
    "\n",
    "# Add final per-class metrics\n",
    "for c in range(num_classes):\n",
    "    if len(dice_history[c]) > 0:\n",
    "        final_metrics[f'final_train_dice_{class_names[c]}'] = float(dice_history[c][-1])\n",
    "        final_metrics[f'best_train_dice_{class_names[c]}'] = float(max(dice_history[c]))\n",
    "    if len(val_dice_history[c]) > 0:\n",
    "        final_metrics[f'final_val_dice_{class_names[c]}'] = float(val_dice_history[c][-1])\n",
    "        final_metrics[f'best_val_dice_{class_names[c]}'] = float(max(val_dice_history[c]))\n",
    "\n",
    "if len(binary_dice_history) > 0:\n",
    "    final_metrics['final_train_binary_dice'] = float(binary_dice_history[-1])\n",
    "    final_metrics['best_train_binary_dice'] = float(max(binary_dice_history))\n",
    "\n",
    "if len(val_binary_dice_history) > 0:\n",
    "    final_metrics['final_val_binary_dice'] = float(val_binary_dice_history[-1])\n",
    "    final_metrics['best_val_binary_dice'] = float(max(val_binary_dice_history))\n",
    "\n",
    "# Log final metrics to W&B\n",
    "try:\n",
    "    wandb.log(final_metrics, step=final_step)\n",
    "except Exception as e:\n",
    "    print(f'Failed to log final metrics: {e}')\n",
    "\n",
    "# Plot and log loss curve (soft Dice loss)\n",
    "if len(train_losses) > 0:\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(train_losses, label='train soft Dice loss')\n",
    "    plt.xlabel('step')\n",
    "    plt.ylabel('soft Dice loss')\n",
    "    plt.legend()\n",
    "    plt.title('SIREN training soft-Dice loss (multi-class)')\n",
    "    plt.grid(True)\n",
    "    fig_loss = plt.gcf()\n",
    "    try:\n",
    "        wandb.log({'media/loss_curve_soft_dice': wandb.Image(fig_loss)}, step=final_step)\n",
    "    except Exception as e_w:\n",
    "        print('W&B loss logging failed:', e_w)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('No training steps recorded; skipping final loss plot/log.')\n",
    "\n",
    "# Print summary\n",
    "print('\\n' + '='*60)\n",
    "print('TRAINING COMPLETE')\n",
    "print('='*60)\n",
    "print(f'Final step: {final_step}')\n",
    "print(f'Final soft Dice loss: {final_loss:.6f}')\n",
    "print(f'Best soft Dice loss: {best_loss:.6f} (at step {best_loss_step})')\n",
    "print('\\nFinal Train Metrics:')\n",
    "for c in range(num_classes):\n",
    "    if len(dice_history[c]) > 0:\n",
    "        print(f'  {class_names[c]}: {float(dice_history[c][-1]):.4f}')\n",
    "if len(binary_dice_history) > 0:\n",
    "    print(f'  Binary Dice: {float(binary_dice_history[-1]):.4f}')\n",
    "if len(val_dice_history[0]) > 0:\n",
    "    print('\\nFinal Val Metrics:')\n",
    "    for c in range(num_classes):\n",
    "        if len(val_dice_history[c]) > 0:\n",
    "            print(f'  {class_names[c]}: {float(val_dice_history[c][-1]):.4f}')\n",
    "    if len(val_binary_dice_history) > 0:\n",
    "        print(f'  Binary Dice: {float(val_binary_dice_history[-1]):.4f}')\n",
    "print('='*60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## End wandb run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis6020",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
